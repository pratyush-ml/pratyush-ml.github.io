[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! Welcome to my blog. I am Pratyush Sinha from India. I have been working in Tech for the last 10 years. First as a Trainee Decision Scientist(Very Glorified SAS programmer) and now as a Decision Science Consultant(Not glorified enough Machine Learning Engineer). In the career spanning over a decade, I have had a chance to work SAS, Python, C++, and a fair bit of VBA. I do not claim to be an expert in any of these languages but I think I can reason through most codebases thrown at me. I have worked with Forecasting algorithms a lot, NLP and more recently LLMs.\nI currently spend a lot of my day in meetings trying to understand and build solutions our clients have need.\nAside from work, I mostly spend my time with my family and reading. Whatever remains of the day is consumed by Liverpool Football Club.\nThe idea behind this blog is very simple - Share a lot of my learnings and hopefully meet people who have similar interests."
  },
  {
    "objectID": "posts/Movie Review -  Nayak The Hero by Satyajeet Ray.html",
    "href": "posts/Movie Review -  Nayak The Hero by Satyajeet Ray.html",
    "title": "Movie Review: Nayak - the Real hero by Satyajeet Ray",
    "section": "",
    "text": "I am a swucker for movies. I don’t like these rowdy movies - Saiyaara, Houseful 5. These are the kind of movies. I don’t think movies should be crass. They should be reflective of the times around us. I would not consider myself a movie critic in any way. However there is a movie I watched recently that made me think that I could be one. If I get a chance, I will write about that. ———-\nNayak the hero has a newer version starring Anil Kapoor. This review is not about that. This is about the great Satyajit Ray. I have read about him and his movie Pather Panchali but never had a chance to actually see any of his work. This was his first work.\n\nNayak is about an actor who is troubled by his dreams. If you look at his work, he tells the story about Bengal. Throughout the movie, whatever was said is still to this day true. This amy have been a progressive movie for Bengal. There are a few instances which stood out through out the movie - The actor travels in first class and is met with a lot of fanfare at both the destinations - The actors blame people for not watching his moviews - Movies used to last and were given a lot of time on the screen. In today’s world, movies are made for weekends, then it was month’s - In terms of dignity, stage &gt;&gt; advertising &gt;&gt; movies - In terms of moview, movies &gt;&gt; stage &gt;&gt; advertising\nSharmila Tagore interviews Uttam Kumar and sees his life a little more closely which makes her decide not to publish the interview. There were three chapters in hero’s life which felt really close to the heart. - Arindam Mukherjee had a stage mentor Sankar Da who does not want Arinday to leave stage for movies. Arindam obliges but Sankar Da dies before the play has its premiere. Arindam decides to go to the movies but Sankar Da haunts him in his dreams. That entire scene of Arindam being happy at all the money being thrown at him and then sinking and not being helped by Sankar Da is very iconoclastic - The next is the scene with his long time friend who is a labor’s union leader. His friend wants Arindam to lead from the front but he does not. Even when Arindam does get famous, Arindam does not have the courage to address them - The third scene is probably the most painful one. Arindam is relatively new to acting in films and is humiliated on the sets by a senior actor. Later, when Arindam climbs up he is visited by this senior actor as a frail human being. The whole movie is very well shot.\nThere is one more scene which is probably amazing to look at and is more significant. Arindam to get back to sleep drinks on the train and is looking out of the tracks. Tracks which are glistened by a light spark following the tracks. I have spent a considerable weekends in my childhood train travelling back and forth and this was something that I observed during all parts of the train. That light on the track follows."
  },
  {
    "objectID": "posts/Weekly Updates - 12.html",
    "href": "posts/Weekly Updates - 12.html",
    "title": "Weekly Updates - 12/2025",
    "section": "",
    "text": "I have been away lately. Having a new born is not easy. My wife* had a baby last year :-) and the first 6 months were really demanding for her. Maternity leave was an easy time to visit our parents for long stretches of time.\nWe shifted from Gurgaon to Patna for a couple of months and then to Hazaribagh,my wife’s hometown, later. The last week was our last week in Hazaribagh.\nHazaribagh, as a town is a very quiet place to live in. Actually every place is a quiet to live in considering my lifestyle of not participating in other people’s lives. Hazaribagh was a hill station once or may it be still is. The recent rise in temperature has shown that hill stations are not hill stations any more. It does not have too many attractions per se and that may have helped preserve its beauty. It has a hill, a very big lake with numerous smaller ones adjacent to it, and is blessed with lush greenery. The clear winner among the three was the lake. It is huge in size and can be captivating in sunsets. It nestles a lot of birds who keep on chirping.\nA few more thoughts which popped:\n\nI used to think Diversity hirings were not fair. But then again life itself is not fair. I will borrow from the wisdom of people who are wiser than me. I saw 4 women change the course of their lives becacuse of their family. It made me think whether life was really worth it.\nTeam management is a tough. People find reasons to not be accountable. Everyday there is a new issue and trust as a manager cannot be given. It needs to be earnt.\nHaving smaller routines is probably a good idea. I basically put all these points as notes on Whatsapp and have fleshed out the ideas more here.\nThe difference between great and best is the ability to go through the same thing again and again without getting bored. That is how you gain perfection.\n\nThings I think I could write a blog about: 1. Indian healthcare system is following the same trajectory as the US. May be documentign what is about to come would be a good idea 2. Document all agents we have seen recently especially from Adobe & SalesForce. 3. Document all the blogs that I read in the last week\n*Before you think this not to be right words I would like you to consider a few ablations that I had tried. When I said - My wife and I had a baby, my MD asked what my contribution to the baby was. To be honest, my contribution does not compare ,much to hers but I was there. Anyways, I have come to these choice of words because it works if I say it with a smile."
  },
  {
    "objectID": "posts/Attention_Mechanisms.html",
    "href": "posts/Attention_Mechanisms.html",
    "title": "Chapter 03: Coding Attention Mechanisms",
    "section": "",
    "text": "This chapter deals with attention mechanisms. At a very high level, the flow of the chapters is as follows:"
  },
  {
    "objectID": "posts/Attention_Mechanisms.html#better-but-slightly-more-compute-constrained-approach",
    "href": "posts/Attention_Mechanisms.html#better-but-slightly-more-compute-constrained-approach",
    "title": "Chapter 03: Coding Attention Mechanisms",
    "section": "1.1 Better but slightly more compute-constrained approach",
    "text": "1.1 Better but slightly more compute-constrained approach\nNaturally, we should follow how human translators work: take input in the language, synthesize it, and then return the output in the target language. In a computational sense, this is what Encoder-Decoders are. The Encoder acts like the translator listening to the source sentence and forming a mental “summary” of the idea. This summary—technically called a context vector—is then passed to the Decoder, which begins speaking the sentence in the new language. Before Transformers, Recurrent Neural Networks (RNNs) were the standard norm. RNNs are well-suited for sequential data because they process information one step at a time, feeding the output of the previous word into the current step to maintain a sense of history."
  },
  {
    "objectID": "posts/Attention_Mechanisms.html#encoder-decoder-rnns-trouble",
    "href": "posts/Attention_Mechanisms.html#encoder-decoder-rnns-trouble",
    "title": "Chapter 03: Coding Attention Mechanisms",
    "section": "1.2 Encoder-Decoder RNNs trouble",
    "text": "1.2 Encoder-Decoder RNNs trouble\nOnce we move to encoder-decoder RNNs, the challenges are two-fold. As the input text is fed into the encoder, it processes it sequentially and updates its hidden state. It captures the entire meaning of the sentence in the final hidden state. The decoder then takes this “bottleneck” state to start generating the translation.\nThe big limitation is that RNNs can’t directly access earlier hidden states from the encoder during decoding. This leads to lost context.\n\nThe Translator’s Struggle: Imagine our translator is listening to a 50-word sentence. By the time the speaker finishes, the translator has a general “vibe” of the sentence, but they’ve likely forgotten the specific adjectives used at the very beginning. Because the RNN forces the entire meaning into one single vector, the “signal” from the start of the sequence washes out by the time it reaches the end."
  },
  {
    "objectID": "posts/Attention_Mechanisms.html#the-solution-bahdanau-attention",
    "href": "posts/Attention_Mechanisms.html#the-solution-bahdanau-attention",
    "title": "Chapter 03: Coding Attention Mechanisms",
    "section": "1.3 The Solution: Bahdanau Attention",
    "text": "1.3 The Solution: Bahdanau Attention\nThe current solution is the Attention Mechanism, which captures these data dependencies. Instead of forcing the translator to rely on a single, fading memory of the whole sentence, the Bahdanau Attention mechanism allows the translator to “re-read” specific parts of the source text as they generate each word.\nWhen the decoder is about to produce a word, it looks back at all the encoder’s hidden states and assigns a “weight” to them. If the translator is currently translating a noun, the attention mechanism tells them to “focus” more on the original noun and its modifiers in the source sentence, rather than the distant verbs.\nMathematically, the context vector for each decoding step is a weighted sum of all encoder hidden states : \\[c_t = \\sum_{i=1}^{T} \\alpha_{ti} h_i\\] Where \\(\\alpha_{ti}\\) represents the “attention score,” or how much focus the translator is giving to word while producing word . This ensures that no matter how long the sequence is, the important details are never truly lost.\n\nLet’s now jump on how the Attention mechanism works Attention mechanism is like the engine that drives the entire transformer block. Once we get a hang of the attention mechanism, the rest of the parts are relatively easy to fill in."
  },
  {
    "objectID": "posts/Attention_Mechanisms.html#step-1",
    "href": "posts/Attention_Mechanisms.html#step-1",
    "title": "Chapter 03: Coding Attention Mechanisms",
    "section": "2.1 Step 1",
    "text": "2.1 Step 1\nGenerating intermediate weight W which is the dot product on the embedding vector for each token.\n\n\nCode\nimport torch\n\n# Declaring an input token of size 6 with embedding size of 3\ninputs = torch.tensor(\n    [[0.43, 0.15, 0.89], # Your     (x^1)\n   [0.55, 0.87, 0.66], # journey  (x^2)\n   [0.57, 0.85, 0.64], # starts   (x^3)\n   [0.22, 0.58, 0.33], # with     (x^4)\n   [0.77, 0.25, 0.10], # one      (x^5)\n   [0.05, 0.80, 0.55]] # step     (x^6)\n)\n\n# For demonstration purposes, we select one input token\nquery = inputs[1]\n\n# \nattn_scores_2 = torch.empty(inputs.shape[0])\n# print(attn_scores_2)\n\n# Calculating step 1. This results in a vector of the same size as the context length\n# Context length here means the number of tokens\nfor i , x_i in enumerate(inputs):\n    attn_scores_2[i] = torch.dot(x_i, query)\n\n\nprint(f\"Attention score for second word i.e Journey {attn_scores_2}\")\n\n\nAttention score for second word i.e Journey tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n\n\nJust for more clarity, the dimension is equal to the number of tokens because the embeddding vector of the token ‘journey’ now has a dot product with all the tokens in the input text."
  },
  {
    "objectID": "posts/Attention_Mechanisms.html#step-2",
    "href": "posts/Attention_Mechanisms.html#step-2",
    "title": "Chapter 03: Coding Attention Mechanisms",
    "section": "2.2 Step 2:",
    "text": "2.2 Step 2:\nNormalizing the intermediate weights now. The weight should sum up on a scale of 0 to 1. The easiest way to do this is using softmax layer.\n\n\nCode\nattn_weights_2 = torch.softmax(attn_scores_2, dim =0)\nprint(f\"This is normalizing the vector of immediate weights {attn_weights_2}\")\nprint(f\"Sum should be 1: {attn_weights_2.sum()}\")\n\n\nThis is normalizing the vector of immediate weights tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\nSum should be 1: 1.0"
  },
  {
    "objectID": "posts/Attention_Mechanisms.html#step-3",
    "href": "posts/Attention_Mechanisms.html#step-3",
    "title": "Chapter 03: Coding Attention Mechanisms",
    "section": "2.3 Step 3:",
    "text": "2.3 Step 3:\nA weighted sum of the embeddings vector and the normalized attention weight.\n\n\nCode\nquery = inputs[1]\ncontext_vec_2 = torch.zeros(query.shape)\nfor i, x_i in enumerate(inputs):\n    context_vec_2 += attn_weights_2[i]*x_i\nprint(f\"The final attention weight: {context_vec_2}\")\n\n\nThe final attention weight: tensor([0.4419, 0.6515, 0.5683])\n\n\nThe resultant vector for one token is the same as the input token embeddings size From a code perspective, this is a relatively poor. But for demonstration purposes, this will do.\nWhen reviewing the code, an obvious question is the difference between torch.zeros vs torch.empty. THe way are pretty much the same.\n\nTorch.zero initializes matrix of all zeros. The memory allocation for this is done. Torch.empty allocates memory but does not initialize it. This means that we should not use it with a sum operation. This means that if we print torch.empty it will return whatever is present in the memory allocation from before. The reason we use it is because it is slightly faster."
  },
  {
    "objectID": "posts/Attention_Mechanisms.html#step-4",
    "href": "posts/Attention_Mechanisms.html#step-4",
    "title": "Chapter 03: Coding Attention Mechanisms",
    "section": "2.4 Step 4:",
    "text": "2.4 Step 4:\nSo far, we have only done this for one token. Let’s extend this to the whole list of input tokens.\n\n\nCode\nattn_scores = torch.empty(inputs.shape)\nprint(attn_scores)\nfor i, x_i in enumerate(inputs):\n    for j , x_j in enumerate(inputs):\n        attn_scores[i] = torch.dot(x_i, x_j)\nprint(f\"Attention score for all words {attn_scores}\")\n\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\nAttention score for all words tensor([[0.6310, 0.6310, 0.6310],\n        [1.0865, 1.0865, 1.0865],\n        [1.0605, 1.0605, 1.0605],\n        [0.6565, 0.6565, 0.6565],\n        [0.2935, 0.2935, 0.2935],\n        [0.9450, 0.9450, 0.9450]])\n\n\nA slightly advanced approach leveraging linear algebra is:\n\n\nCode\n# Step 1\nattn_scores = inputs @ inputs.T \n\n# Step 2\nattn_weights = torch.softmax(attn_scores, dim =-1) # Do it on the last dimension of the vector, which in this case is equal to token size.\n\n# Step 3\n\nall_context_vectors = attn_weights @ inputs\nprint(all_context_vectors)\n\n\ntensor([[0.4421, 0.5931, 0.5790],\n        [0.4419, 0.6515, 0.5683],\n        [0.4431, 0.6496, 0.5671],\n        [0.4304, 0.6298, 0.5510],\n        [0.4671, 0.5910, 0.5266],\n        [0.4177, 0.6503, 0.5645]])\n\n\nTo summarize iteration 1, we did a watered down version of self attention. We took one token as an input, dot product across all tokens embeddings, normalize the resultant weights across token length and finally we do a sum proudct of this normalized weight with each embedding vector. We repeat the same exercise for all tokens. Few important things to consider for the scaled version. attn_scores has dimensions: token_length * token_length attn_weights: token_length * token_length all_context_vectors: token_length * embedding_size"
  },
  {
    "objectID": "posts/Attention_Mechanisms.html#translators-learnings-from-this-step",
    "href": "posts/Attention_Mechanisms.html#translators-learnings-from-this-step",
    "title": "Chapter 03: Coding Attention Mechanisms",
    "section": "2.5 Translator’s Learnings from this step",
    "text": "2.5 Translator’s Learnings from this step\nAt this stage, our translator has stopped using a word-for-word dictionary. They’ve realized that words are defined by their neighbors. When they see the word “bank,” they now look at the rest of the sentence to see if there are words like “river” or “money.” They are building a rough ‘vibe’ of the sentence, but they’re still using a fixed set of rules and can’t yet adapt their focus based on the specific task at hand."
  },
  {
    "objectID": "posts/Attention_Mechanisms.html#translators-learnings-from-this-step-1",
    "href": "posts/Attention_Mechanisms.html#translators-learnings-from-this-step-1",
    "title": "Chapter 03: Coding Attention Mechanisms",
    "section": "3.1 Translator’s Learnings from this step",
    "text": "3.1 Translator’s Learnings from this step\nNow, the translator has learned that not all relationships are equal. By introducing \\(W_q\\), \\(W_k\\), and \\(W_v\\), they’ve developed three distinct mental frameworks: What am I looking for? (Query), What do I offer to others? (Key), and What is the actual meaning? (Value). They are no longer just looking at neighbors; they are actively learning which connections are the most important for understanding the “soul” of the sentence. There is one hack that the translator is using right now. It has a look up to future words."
  },
  {
    "objectID": "posts/Attention_Mechanisms.html#the-steps-are-the-following",
    "href": "posts/Attention_Mechanisms.html#the-steps-are-the-following",
    "title": "Chapter 03: Coding Attention Mechanisms",
    "section": "4.1 The steps are the following:",
    "text": "4.1 The steps are the following:\n\nCalculate Attention scores\nApply SoftMax and get attention weights\nMask with 0 above the diagonal\nNormalize the rows of the masked attention scores\n\nImplementing this pretty much the same way as before. We will do small codes with one go, then we will formalize with a class\n\n\nCode\n# First two steps combined\nqueries = sa_v2.W_query(inputs)\n\nkeys = sa_v2.W_key(inputs)\n\nattnn_scores = queries @ keys.T\nattn_weights = torch.softmax(attn_scores/ keys.shape[-1]** 0.5, dim = -1)\n\nprint(attn_weights)\n\n# Third step - this looks a whole lot complex but is rather simple. If you only think about the values that tril is generating.\ncontext_length = attn_scores.shape[0]\nmasked_simple = torch.tril(torch.ones(context_length,context_length))\nprint(masked_simple)\n\n# Fourth step\nrow_sums = masked_simple.sum(dim=-1, keepdim=True)\nmasked_simple_norm = masked_simple/row_sums\nprint(masked_simple_norm)\n\n\ntensor([[0.1972, 0.1910, 0.1894, 0.1361, 0.1344, 0.1520],\n        [0.1476, 0.2164, 0.2134, 0.1365, 0.1240, 0.1621],\n        [0.1479, 0.2157, 0.2129, 0.1366, 0.1260, 0.1608],\n        [0.1505, 0.1952, 0.1933, 0.1525, 0.1375, 0.1711],\n        [0.1571, 0.1874, 0.1885, 0.1453, 0.1819, 0.1399],\n        [0.1473, 0.2033, 0.1996, 0.1500, 0.1160, 0.1839]])\ntensor([[1., 0., 0., 0., 0., 0.],\n        [1., 1., 0., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 1., 0., 0.],\n        [1., 1., 1., 1., 1., 0.],\n        [1., 1., 1., 1., 1., 1.]])\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000],\n        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000],\n        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]])\n\n\nThis is a pretty neat implementation which hits our objective of causal attention model right on the head. However, there is a room for a few improvements primarily from a mathematical standpoint. 1. Use negative infinity to get rid of that extra normalization at step 4. The whole idea of step 4 was normalize all the values left after zeroing upper traingle of the matrix.When we use negative infinity with softmax the output is 0 and 2. Dropouts addition to avoid overfitting - This is a technique where randomly selected hidden layer inputs are ignored during training. This avoids overfitting by making sure that the model does not rely heavily on a specific set of hidden layer inputs. Dropouts is only used during training and not inference.\nWe can combine all of this in one go and create our final class. We will also make it work with batches.\n\n\nCode\nbatch = torch.stack((inputs,inputs), dim = 0)\nprint(batch.shape) ## The shape now must be 2 * 6 * 3 which is 2 items in the batch. 6 tokens in each batch. Each token with an embedding size of 3\n\nclass CausalAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n        super().__init__()\n        self.d_out = d_out\n        self.W_query = nn.Linear(d_in,d_out,bias = qkv_bias)\n        self.W_key = nn.Linear(d_in,d_out,bias = qkv_bias)\n        self.W_value = nn.Linear(d_in,d_out,bias = qkv_bias)\n        self.dropout = nn.Dropout(dropout) # The input here indicates the percentage of values in the matrix that must be zero\n\n        # This tells pytorch that these weights are non trainable so no gradient-descent for them. The mask is saved in state_dict and is loaded/saced with the model\n        self.register_buffer(\n            'mask',\n            torch.triu(torch.ones(context_length,context_length),diagonal=1)\n        ) # Will take this in a bit\n\n    def forward(self, x):\n        b, num_tokens, d_in = x.shape # This is the reason that we do not need to initialize the d_in. d_in is derived from an object of the class\n\n        # Same step as the previous loop\n        # Shape of x = 2,6,3\n        # Shape of key, query, value is 2,6,2\n        keys = self.W_key(x) \n        queries = self.W_query(x)\n        values = self.W_value(x)\n\n        # Pretty much the same as the previous step. This is just doing transformations so that keys merge properly.\n        # Shape of attention scores is 2,6,6\n        # Keys.transpose essentially means switching the 1st dimenion with the second. so the dimesion is now 2,2,6\n        # queries (2,6,2) @ keys.transpose(1,2) (2,2,6) ==&gt; (2,6,6)\n        # this is because torch considers everythin after the last two dimesnions as batch. A better way to say this would be anything the matrix multipllication here is 6,2 @ 2,6 --&gt; 6,6 and then there are two batches in each.\n\n        attn_scores = queries @ keys.transpose(1,2)\n        \n        # This is making sure that all the values that are masked are now \n        # Fill elemts of the tensor with value where mask is true. The shape of the mask must be broadcastable\n        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens],-torch.inf)\n\n        # This is the same step as before.\n        attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim = -1)\n\n        # Droput we had discussed\n        # Zeroes the values of the matrix at random. the shape of the matrix is the same\n\n        attn_weights = self.dropout(attn_weights)\n\n        # This the same step as before.\n        # attn_weights dim = (2,6,6). values dim (2,6,2) --&gt; 2,6,2\n        context_vec = attn_weights @ values\n\n        return context_vec\n\n\ntorch.Size([2, 6, 3])\n\n\nThe wraps up Causal Attention. Causal attention is the base on which the multi- headed attention is based on. Causal Attention also known as masked attention is a specialized form of self-attention. It restricts a model to only review previous and current inputs in a sequence when processing any given token when computing attention scores."
  },
  {
    "objectID": "posts/Attention_Mechanisms.html#translators-learnings-from-this-step-2",
    "href": "posts/Attention_Mechanisms.html#translators-learnings-from-this-step-2",
    "title": "Chapter 03: Coding Attention Mechanisms",
    "section": "4.2 Translator’s Learnings from this step",
    "text": "4.2 Translator’s Learnings from this step\nOur translator is now writing a book. They’ve learned a vital rule: no spoilers. By masking the future, the translator forces themselves to generate the story one word at a time, using only what has been “said” so far. This discipline ensures the translation flows logically from start to finish, preventing the model from ‘cheating’ by looking at the end of the sentence before it has even started the beginning."
  },
  {
    "objectID": "posts/Attention_Mechanisms.html#going-back-to-translator",
    "href": "posts/Attention_Mechanisms.html#going-back-to-translator",
    "title": "Chapter 03: Coding Attention Mechanisms",
    "section": "6.1 Going back to translator",
    "text": "6.1 Going back to translator\nFinally, our single translator has become a full editorial team. One expert focuses on the tense of the verbs, another tracks the gender of the pronouns, and a third looks for poetic metaphors. By working in parallel, they catch nuances that a single mind would miss. They then sit down together (the final linear projection) to merge their individual notes into one perfect, polished masterpiece."
  },
  {
    "objectID": "posts/ISIC.html",
    "href": "posts/ISIC.html",
    "title": "A Hopeful Guide to Navigating Spinal Injury: Our Family’s Experience at ISIC",
    "section": "",
    "text": "My father has had two major surgeries in the neck at the Indian Spinal Injury Center, and both have been successful. But this post isn’t just about that. I want this post to give you hope that there is a hospital which could really help you in case you suffer from a spine injury. The world does not turn into a bleak place. Please treat this as a general playbook from one family to another, not as medical advice. Anything I mention that sounds like medical advice here should be discussed with your own doctors."
  },
  {
    "objectID": "posts/ISIC.html#the-main-leverage-is-time",
    "href": "posts/ISIC.html#the-main-leverage-is-time",
    "title": "A Hopeful Guide to Navigating Spinal Injury: Our Family’s Experience at ISIC",
    "section": "The main leverage is time",
    "text": "The main leverage is time\nTime is of the essence when dealing with spinal injuries. The progression of these injuries is rapid and can lead to significant deterioration within a short period, often no more than 10 days to reach a critical stage. Spinal injuries follow an almost exponential curve of decline if not addressed promptly.\nInitial symptoms may include a loss of balance, followed by numbness in the fingers and legs. This can then progress to a lack of coordination in the legs, causing a limping gait. In advanced stages, individuals may lose motor functions and control over bladder and bowel movements, leading to a profound loss of confidence. My father experienced this severe stage during his first spinal injury.\nFortunately, we were able to intervene earlier during his second injury. It is crucial to understand that this deterioration can occur within 1 to 3 days. A fall can significantly accelerate this process. Therefore, it is vital not to underestimate the severity of these injuries and to prevent misdiagnosis. My father’s initial symptoms were mistakenly attributed to diabetes."
  },
  {
    "objectID": "posts/ISIC.html#we-are-in-the-same-boat-you-and-i",
    "href": "posts/ISIC.html#we-are-in-the-same-boat-you-and-i",
    "title": "A Hopeful Guide to Navigating Spinal Injury: Our Family’s Experience at ISIC",
    "section": "We are in the same boat you and I",
    "text": "We are in the same boat you and I\nIndian Healthcare for all its pros and cons is working like an engine. There are great hospitals near Delhi which are semi private. ISIC is one of them. You could go to AIMS for spinal injuries. But I would suggest against that. The reason is the backlog that AIMS has. No doubt, AIMS is the most affordable. It has probably the best doctors from India. But I think ISIC does a reasonable job of maintaining the balance between cost and good service. I don’t want to delve into past laurels because they are pretty much useless when it comes to the present day running of the hospital but ISIC runs smoothly. If you have Insurance they will be happy to take you in. I did not deal in cash but cash processing is also smooth judging by the queue."
  },
  {
    "objectID": "posts/ISIC.html#overcoming-the-surgery",
    "href": "posts/ISIC.html#overcoming-the-surgery",
    "title": "A Hopeful Guide to Navigating Spinal Injury: Our Family’s Experience at ISIC",
    "section": "Overcoming the surgery",
    "text": "Overcoming the surgery\nOne of the best things about this hospital is its rehabilitation unit. It is where the post-operative care happens, and this is not treated as an add-on; it is considered part of the treatment. All of the people I interacted with were very patient with patients. They would answer all your questions. My father was able to walk after 2 months of the surgery. I consider this to be a miracle of sorts. I think the hospital staff does a lot of things really well, but there is little being done to ensure transparency between departments. The doctors are transparent, but they do not always help the nursing staff explain their decisions. This leads to confusion for the attendants. The ICU times are perfect and do not need change. The hospital even gave me some slack for feeding my father without the nurse, which I am incredibly grateful for. Insurance paperwork is really a pain and there are 6 desks to visit. For an attendant, it may seem overwhelming. It is streamlined and people are really helpful, but from day 1, the paperwork seems like a lot. My only benchmark is Max Gurgaon near the Huda Metro Station where the paperwork is just a few signatures. But the positive is everyone tries to help. The staff in the Admissions office help you even if they are non-chalant"
  },
  {
    "objectID": "posts/ISIC.html#a-practical-guide-to-attendants",
    "href": "posts/ISIC.html#a-practical-guide-to-attendants",
    "title": "A Hopeful Guide to Navigating Spinal Injury: Our Family’s Experience at ISIC",
    "section": "A practical guide to attendants",
    "text": "A practical guide to attendants\n\nCoffee at the main cafeteria is expensive. Go to the retail store and they will sell you the same coffee for cheap. I needed a lot of coffee.\nJunior doctors are your friends. Please listen to them\nGuards are better in the night than during the day\nHave good relations with the nurses, they do more paperwork in a day than what you do in a year. They will help you when you need it. Don’t ask for your test results from them because the doctor is supposed to inform you of that. If the doctors become of aware of this, nurses get a stick from the head nurse\nWhatever games you are thinking that the hospital is playing to increase your bills, it is not\nICU is Intensive Care Unit. Avoid touching anything and use copious amounts of hand sanitizer if you must touch something\nHDU is a High Dependency Unit. This is confusing to a lot of the patients. This means that the patient is out of danger but still needs active monitoring. The timings of visit are different from ICU. But rules are the same as the ICU. The doctor is there during visiting times to give you updates.\nDo not get nurses in trouble with the doctors\nFloor managers are there for assessing the arrangements. Please do not use them to get your way because they are the admin\nIf you are the sole attendant and there is no place to sleep, the best place is your car or near the emergency beside the Cafe Coffee Day. Do not go the attendant’s ward because that place is likely to be filled and is a den of mosquitos"
  },
  {
    "objectID": "posts/ISIC.html#closing-notes",
    "href": "posts/ISIC.html#closing-notes",
    "title": "A Hopeful Guide to Navigating Spinal Injury: Our Family’s Experience at ISIC",
    "section": "Closing Notes",
    "text": "Closing Notes\nThere are diseases which you think do not make sense but you need to make sense of them. Spine injuries are one of them. Between these surgeries I have spent a month in the hospital. These are the toughest times you can endure. If anything, I want you to remember that you are in safe hands. Please trust your primary team and do not self medicate. I hope if you are looking for hope. This may be one."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Why even bother with a blog?",
    "section": "",
    "text": "It is 2025, AI which can match humans is surely upon us. Every day we have breakthroughs in AI which would put careers to end.\nProbably the first to go out the window would be Software Engineers. Probably is a key word here! I don’t want Software Engineers automated. But hey Engineering in all forms has always been a cost centre for the business. And when the factory workers were being automated by assembly line noone considered their feelings. Also the fooling around with three instances of Dev, QA, Prod and making databases and selling them as a Gold Pill for businesses(SalesForce) was probably not the right move.\nThese models or atleast the genesis of these models was based on the entirety of Internet data. Noone cared about the Copyright data and Terms of Service. Data from Reddit which was at one time the Front Page of the Internet was used. (It isn’t now, it is mostly a porn website too scared of Wallstreet to call it one.) At the time of writing, one of the leaders of AI movement Open AI is claiming that a more cost effective model originating from China was trained on Open AI’s outputs. That is not very open is it?\nIf a machine can generate text with pretty much the same intent as me, the text by itself would not matter. Instead I matter and ergo the content I put out there matters.\nBefore I go on ranting about why Blogging is amazing, I want to put one thought upfront. I do not support the argument of writing makes you think better. I think it just is. I will write more about this and it could be writings for another blog.\nI want to write because I feel it can connect me with people who share similar interests as mine. There are faster means out there. I prefer blogging. I should ahve started blogging a decade earlier because I was convinced. But my inherent laziness I have found a way to do this for 10 years. 10 years is 3653 days. I feel like an idiot.\nAt the begining, it was shame guilt fear of failure. What if I could not write perfectly? What if my words do not resonate the intended audience? Later it was business and then followed by more guilt of why did I not do this before.\nI would not drive more into why I did not to do ti? Why now? There is plenty of writings for going forward.\nI want to write about my learings in the week. I read a lot of mathematics papers. I try to code most of the papers but at the end of the year it always feels like I have done nothing. I also undertake multiple courses. None of that is documented anyhwere.\nAll of what I am writing doing and documenting is going to be redundant because of AI. But this blog should continue regardless.\nTo document what I was. To be a person who could be reached out to. To show what the World means to me. For I cannot claim to be a champion of this world yet!"
  },
  {
    "objectID": "posts/Weekly Updates - 13.html",
    "href": "posts/Weekly Updates - 13.html",
    "title": "Weekly Updates - 13/2025",
    "section": "",
    "text": "After contemplating my 10-year corporate journey for several months, I’ve come to an unexpected realization. When I started, I believed that dedicating a decade to my profession would make me exceptionally skilled at it.\nIn my early career days, I naively thought five years would be sufficient to achieve meaningful accomplishments in data science. Looking back, I can only smile at my youthful optimism.\nThe reality is that I’ve rarely paused to critically evaluate my career trajectory. There’s a sense that I’ve simply drifted along the current. Despite making conscious efforts toward promotion, these attempts haven’t significantly impacted my professional growth so far.\n\n\n\n\nChatGPT released their new image generation model, sparking excitement about Ghibli-style transformations and igniting a Twitter discourse. My thoughts:\n\nOn AI “stealing” artistic styles: As an Indian, I view digital rights and AI art debates through a unique lens. The concept feels somewhat abstract in my context.\nGlobal perspective: Most people engaging with these tools aren’t Ghibli aficionados - they’re following trends. Meanwhile, Ghibli gains worldwide exposure through these interactions.\nCorporate caution: Meta likely has similar technology in development but hesitates on release due to copyright concerns. They’re playing it safe while LLMs force us to reconsider intellectual property frameworks altogether.\nHumorous observation: Given India’s unique relationship with copyright (considering T-series and Pritam’s success), perhaps we could offer “Judiciary as a Service” to tech giants navigating digital rights challenges.\n\n\n\n\nGemini 2.5 appears to outperform Claude 3.7. It’s fascinating that Google maintains technical superiority yet struggles with market perception in AI. The best technology doesn’t always win the market.\nComparing ChatGPT, Gemini, Claude, and Grok reveals ChatGPT’s strength lies primarily in marketing - an advantage that may fade without developing stronger B2B applications beyond their core model.\n\n\n\nSatya Nadella stands out for his exceptional clarity as a CEO. His conversation with Dwarkesh about AGI timeline assessment was remarkably thoughtful. Nadella suggests that true AGI would manifest as a 10% GDP increase within three years - indicating that economic benchmarks remain the truest measure of technological impact, and that current “intelligence as a service” hasn’t yet achieved substantial enterprise adoption.\n\n\n\n\nI’m interested in exploring Agents further, though I should first document HTML5 generation for model explainability. I also plan to write about import statements and self-contained UV environments."
  },
  {
    "objectID": "posts/Weekly Updates - 13.html#noteworthy-observations-this-week",
    "href": "posts/Weekly Updates - 13.html#noteworthy-observations-this-week",
    "title": "Weekly Updates - 13/2025",
    "section": "",
    "text": "ChatGPT released their new image generation model, sparking excitement about Ghibli-style transformations and igniting a Twitter discourse. My thoughts:\n\nOn AI “stealing” artistic styles: As an Indian, I view digital rights and AI art debates through a unique lens. The concept feels somewhat abstract in my context.\nGlobal perspective: Most people engaging with these tools aren’t Ghibli aficionados - they’re following trends. Meanwhile, Ghibli gains worldwide exposure through these interactions.\nCorporate caution: Meta likely has similar technology in development but hesitates on release due to copyright concerns. They’re playing it safe while LLMs force us to reconsider intellectual property frameworks altogether.\nHumorous observation: Given India’s unique relationship with copyright (considering T-series and Pritam’s success), perhaps we could offer “Judiciary as a Service” to tech giants navigating digital rights challenges.\n\n\n\n\nGemini 2.5 appears to outperform Claude 3.7. It’s fascinating that Google maintains technical superiority yet struggles with market perception in AI. The best technology doesn’t always win the market.\nComparing ChatGPT, Gemini, Claude, and Grok reveals ChatGPT’s strength lies primarily in marketing - an advantage that may fade without developing stronger B2B applications beyond their core model.\n\n\n\nSatya Nadella stands out for his exceptional clarity as a CEO. His conversation with Dwarkesh about AGI timeline assessment was remarkably thoughtful. Nadella suggests that true AGI would manifest as a 10% GDP increase within three years - indicating that economic benchmarks remain the truest measure of technological impact, and that current “intelligence as a service” hasn’t yet achieved substantial enterprise adoption."
  },
  {
    "objectID": "posts/Weekly Updates - 13.html#looking-forward",
    "href": "posts/Weekly Updates - 13.html#looking-forward",
    "title": "Weekly Updates - 13/2025",
    "section": "",
    "text": "I’m interested in exploring Agents further, though I should first document HTML5 generation for model explainability. I also plan to write about import statements and self-contained UV environments."
  },
  {
    "objectID": "posts/GPT_from_Scratch.html",
    "href": "posts/GPT_from_Scratch.html",
    "title": "Implementing a GPT model from scratch to generate text",
    "section": "",
    "text": "One idea that has stuck with me while working with these Transformers is that they are algorithms with many moving components. To understand why a particular component exists, we need to understand the experimentation process behind it. It is imperative that we understand the moving parts involved in the algorithm first; we can worry about the experimentation bit later.\nThink of it from a sorting algorithm perspective: there are many sorting algorithms out there. We gradually evolved from one kind of sorting algorithm to another through rapid experimentation. We could think of Transformers in the same way—we started out with translation tasks using architectures like RNNs and gradually shifted to Transformers, which represent a more advanced form of sequence processing. If we spend too much time on the history of every tiny component right now, we would be in a constant “experimentation phase.” I think to learn Transformers, you don’t necessarily need that history right now. That is something we can take up once we’ve gone through the entire exercise of creating a Transformer from scratch.\nA quick summary of what we have learned so far: Large Language Models are large deep learning neural networks. Transformers are the core blocks on which LLMs are built. The core idea behind Transformers is multi-headed attention.\nIn the previous section, we worked on our understanding of Attention models. We learned how Multi-Headed Attention is a combination of self-attention and causal attention. We also learned how multi-headed attention is the engine of the Transformer models. In this section, we are going to build on that and implement the remaining components. The end objective of this chapter is to develop a working model which takes input text and returns output text based on the Transformer architecture.\nTo start, let’s define all the components of the Transformer to clearly see what other components require our attention.\nCode\ngraph TD\n    %% Input Section\n    Input([\"Every effort moves you\"]) --&gt; Tokenized[Tokenized text]\n    \n    subgraph GPT [\"GPT model\"]\n        direction TB\n        \n        %% Initial Embeddings\n        Tokenized --&gt; TE[Token embedding layer]\n        TE --&gt; PE[Positional embedding layer]\n        PE --&gt; Drop1[Dropout]\n\n        %% Transformer Block Subgraph\n        subgraph TransformerBlock [\"Transformer block (Repeated 12 times)\"]\n            direction TB\n            \n            %% First Residual Connection\n            Drop1 --&gt; LN1[LayerNorm 1]\n            LN1 --&gt; MHA[Masked multi-head attention]\n            MHA --&gt; Drop2[Dropout]\n            Drop2 --&gt; Add1((+))\n            \n            %% Residual Line 1\n            Drop1 -- \"Residual\" --- Add1\n\n            %% Second Residual Connection\n            Add1 --&gt; LN2[LayerNorm 2]\n            LN2 --&gt; FF[Feed forward]\n            FF --&gt; Drop3[Dropout]\n            Drop3 --&gt; Add2((+))\n            \n            %% Residual Line 2\n            Add1 -- \"Residual\" --- Add2\n        end\n\n        %% Final Output Layers\n        Add2 --&gt; FLN[Final LayerNorm]\n        FLN --&gt; Linear[Linear output layer]\n    end\n\n    %% Output Tensor\n    Linear --&gt; Tensor[\"&lt;b&gt;A 4 x 50,257-dimensional tensor&lt;/b&gt;&lt;br/&gt;[[ -0.0055, ..., -0.4747],&lt;br/&gt;[ 0.2663, ..., -0.4224],&lt;br/&gt;[ 1.1146, ..., 0.0276],&lt;br/&gt;[ -0.8239, ..., -0.3993]]\"]\n\n    %% Explanatory Note\n    Tensor -.-&gt; Goal[\"&lt;b&gt;Goal:&lt;/b&gt; Convert back to text&lt;br/&gt;(Last row represents 'forward')\"]\n\n    %% Styling\n    classDef blueFill fill:#88ccee,stroke:#333,stroke-width:1px;\n    classDef greyFill fill:#e0e0e0,stroke:#333,stroke-width:1px;\n    classDef darkFill fill:#555,color:#fff,stroke:#333,stroke-width:1px;\n    \n    class TransformerBlock blueFill;\n    class GPT,Tokenized,TE,PE,Drop1,LN1,LN2,FF,Drop2,Drop3,FLN,Linear,Tensor greyFill;\n    class MHA darkFill;\n\n\n\n\n\ngraph TD\n    %% Input Section\n    Input([\"Every effort moves you\"]) --&gt; Tokenized[Tokenized text]\n    \n    subgraph GPT [\"GPT model\"]\n        direction TB\n        \n        %% Initial Embeddings\n        Tokenized --&gt; TE[Token embedding layer]\n        TE --&gt; PE[Positional embedding layer]\n        PE --&gt; Drop1[Dropout]\n\n        %% Transformer Block Subgraph\n        subgraph TransformerBlock [\"Transformer block (Repeated 12 times)\"]\n            direction TB\n            \n            %% First Residual Connection\n            Drop1 --&gt; LN1[LayerNorm 1]\n            LN1 --&gt; MHA[Masked multi-head attention]\n            MHA --&gt; Drop2[Dropout]\n            Drop2 --&gt; Add1((+))\n            \n            %% Residual Line 1\n            Drop1 -- \"Residual\" --- Add1\n\n            %% Second Residual Connection\n            Add1 --&gt; LN2[LayerNorm 2]\n            LN2 --&gt; FF[Feed forward]\n            FF --&gt; Drop3[Dropout]\n            Drop3 --&gt; Add2((+))\n            \n            %% Residual Line 2\n            Add1 -- \"Residual\" --- Add2\n        end\n\n        %% Final Output Layers\n        Add2 --&gt; FLN[Final LayerNorm]\n        FLN --&gt; Linear[Linear output layer]\n    end\n\n    %% Output Tensor\n    Linear --&gt; Tensor[\"&lt;b&gt;A 4 x 50,257-dimensional tensor&lt;/b&gt;&lt;br/&gt;[[ -0.0055, ..., -0.4747],&lt;br/&gt;[ 0.2663, ..., -0.4224],&lt;br/&gt;[ 1.1146, ..., 0.0276],&lt;br/&gt;[ -0.8239, ..., -0.3993]]\"]\n\n    %% Explanatory Note\n    Tensor -.-&gt; Goal[\"&lt;b&gt;Goal:&lt;/b&gt; Convert back to text&lt;br/&gt;(Last row represents 'forward')\"]\n\n    %% Styling\n    classDef blueFill fill:#88ccee,stroke:#333,stroke-width:1px;\n    classDef greyFill fill:#e0e0e0,stroke:#333,stroke-width:1px;\n    classDef darkFill fill:#555,color:#fff,stroke:#333,stroke-width:1px;\n    \n    class TransformerBlock blueFill;\n    class GPT,Tokenized,TE,PE,Drop1,LN1,LN2,FF,Drop2,Drop3,FLN,Linear,Tensor greyFill;\n    class MHA darkFill;\nWith this block, we get a clear picture of all the components needed to build a GPT-2 model. We will build this model with a backbone which lays out all the components of the model. We then fill this backbone with all the relevant code.\nCode\ngraph LR\n    subgraph Core [\"Core Components\"]\n        direction TB\n        B2[\"2 Layer normalization\"]\n        B3[\"3 GELU activation\"]\n        B4[\"4 Feed forward network\"]\n        B5[\"5 Shortcut connections\"]\n    end\n\n    subgraph Integration\n        B6[\"6 Transformer block\"]\n        B7[\"7 Final GPT architecture\"]\n    end\n\n    subgraph Initial [\"Initial Phase\"]\n        B1[\"1 GPT backbone\"]\n    end\n\n    %% Connections\n    B2 --&gt; B6\n    B3 --&gt; B6\n    B4 --&gt; B6\n    B5 --&gt; B6\n    B6 --&gt; B7\n\n    %% Styling\n    style Core fill:none,stroke:#ccc\n    style Integration fill:#e1f5fe,stroke:#01579b\n    style Initial fill:none,stroke:#ccc\n\n\n\n\n\ngraph LR\n    subgraph Core [\"Core Components\"]\n        direction TB\n        B2[\"2 Layer normalization\"]\n        B3[\"3 GELU activation\"]\n        B4[\"4 Feed forward network\"]\n        B5[\"5 Shortcut connections\"]\n    end\n\n    subgraph Integration\n        B6[\"6 Transformer block\"]\n        B7[\"7 Final GPT architecture\"]\n    end\n\n    subgraph Initial [\"Initial Phase\"]\n        B1[\"1 GPT backbone\"]\n    end\n\n    %% Connections\n    B2 --&gt; B6\n    B3 --&gt; B6\n    B4 --&gt; B6\n    B5 --&gt; B6\n    B6 --&gt; B7\n\n    %% Styling\n    style Core fill:none,stroke:#ccc\n    style Integration fill:#e1f5fe,stroke:#01579b\n    style Initial fill:none,stroke:#ccc\nBefore we start building, we need to define the arguments required for these components and a full-fledged Transformer block to run."
  },
  {
    "objectID": "posts/GPT_from_Scratch.html#defining-the-configuration-for-gpt-2-class-of-models",
    "href": "posts/GPT_from_Scratch.html#defining-the-configuration-for-gpt-2-class-of-models",
    "title": "Implementing a GPT model from scratch to generate text",
    "section": "0.1 Defining the configuration for GPT-2 class of models",
    "text": "0.1 Defining the configuration for GPT-2 class of models\n\n\nCode\nGPT_CONFIG_124M = {\n    \"vocab_size\": 50257, # Vocabulary_Size\n    \"context_length\": 1024, # Context Length\n    \"emb_dim\": 768, # Embedding dimension\n    \"n_heads\": 12, # Number of Attention Heads \n    \"n_layers\": 12, # Number of layers\n    \"drop_rate\": 0.1, # DropOut Rate\n    \"qkv_bias\": False  # Query-Key-Value Bias\n}\n\n\nDefining these variables:\n\nvocab_size: This refers to the vocabulary of 50,257 words as per the tokenizer (BPE).\ncontext_length: The maximum number of input tokens the model can handle via positional embeddings.\nemb_dim: The dimensionality of each token vector projection.\nn_heads: The number of attention heads in the multi-head attention mechanism.\nn_layers: The number of Transformer blocks in the model.\ndrop_rate: The intensity of the dropout mechanism to prevent overfitting.\nqkv_bias: This determines the bias vector of the linear layers in the multi-head attention. We disabled this in the previous chapter."
  },
  {
    "objectID": "posts/GPT_from_Scratch.html#perhaps-in-the-next-blog-we-can-go-top-down-on-the-gpt-2-architecture-or-dive-into-one-of-the-foundational-papers.",
    "href": "posts/GPT_from_Scratch.html#perhaps-in-the-next-blog-we-can-go-top-down-on-the-gpt-2-architecture-or-dive-into-one-of-the-foundational-papers.",
    "title": "Implementing a GPT model from scratch to generate text",
    "section": "8.1 Perhaps in the next blog, we can go top-down on the GPT-2 architecture or dive into one of the foundational papers.",
    "text": "8.1 Perhaps in the next blog, we can go top-down on the GPT-2 architecture or dive into one of the foundational papers."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pratyush Sinha",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nFeb 8, 2026\n\n\nImplementing a GPT model from scratch to generate text\n\n\nPratyush Sinha\n\n\n\n\n\n\nFeb 1, 2026\n\n\nChapter 03: Coding Attention Mechanisms\n\n\nPratyush Sinha\n\n\n\n\n\n\nApr 8, 2025\n\n\nMovie Review: Nayak - the Real hero by Satyajeet Ray\n\n\nPratyush Sinha\n\n\n\n\n\n\nMar 30, 2025\n\n\nWeekly Updates - 13/2025\n\n\nPratyush Sinha\n\n\n\n\n\n\nMar 23, 2025\n\n\nWeekly Updates - 12/2025\n\n\nPratyush Sinha\n\n\n\n\n\n\nFeb 9, 2025\n\n\nWhy even bother with a blog?\n\n\nPratyush Sinha\n\n\n\n\n\n\nJan 9, 2025\n\n\nA Hopeful Guide to Navigating Spinal Injury: Our Family’s Experience at ISIC\n\n\nPratyush Sinha\n\n\n\n\n\n\nNo matching items"
  }
]