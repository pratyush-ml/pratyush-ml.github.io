[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! Welcome to my blog. I am Pratyush Sinha from India. I have been working in Tech for the last 10 years. First as a Trainee Decision Scientist(Very Glorified SAS programmer) and now as a Decision Science Consultant(Not glorified enough Machine Learning Engineer). In the career spanning over a decade, I have had a chance to work SAS, Python, C++, and a fair bit of VBA. I do not claim to be an expert in any of these languages but I think I can reason through most codebases thrown at me. I have worked with Forecasting algorithms a lot, NLP and more recently LLMs.\nI currently spend a lot of my day in meetings trying to understand and build solutions our clients have need.\nAside from work, I mostly spend my time with my family and reading. Whatever remains of the day is consumed by Liverpool Football Club.\nThe idea behind this blog is very simple - Share a lot of my learnings and hopefully meet people who have similar interests."
  },
  {
    "objectID": "posts/Movie Review -  Nayak The Hero by Satyajeet Ray.html",
    "href": "posts/Movie Review -  Nayak The Hero by Satyajeet Ray.html",
    "title": "Movie Review: Nayak - the Real hero by Satyajeet Ray",
    "section": "",
    "text": "I am a swucker for movies. I don’t like these rowdy movies - Saiyaara, Houseful 5. These are the kind of movies. I don’t think movies should be crass. They should be reflective of the times around us. I would not consider myself a movie critic in any way. However there is a movie I watched recently that made me think that I could be one. If I get a chance, I will write about that. ———-\nNayak the hero has a newer version starring Anil Kapoor. This review is not about that. This is about the great Satyajit Ray. I have read about him and his movie Pather Panchali but never had a chance to actually see any of his work. This was his first work.\n\nNayak is about an actor who is troubled by his dreams. If you look at his work, he tells the story about Bengal. Throughout the movie, whatever was said is still to this day true. This amy have been a progressive movie for Bengal. There are a few instances which stood out through out the movie - The actor travels in first class and is met with a lot of fanfare at both the destinations - The actors blame people for not watching his moviews - Movies used to last and were given a lot of time on the screen. In today’s world, movies are made for weekends, then it was month’s - In terms of dignity, stage &gt;&gt; advertising &gt;&gt; movies - In terms of moview, movies &gt;&gt; stage &gt;&gt; advertising\nSharmila Tagore interviews Uttam Kumar and sees his life a little more closely which makes her decide not to publish the interview. There were three chapters in hero’s life which felt really close to the heart. - Arindam Mukherjee had a stage mentor Sankar Da who does not want Arinday to leave stage for movies. Arindam obliges but Sankar Da dies before the play has its premiere. Arindam decides to go to the movies but Sankar Da haunts him in his dreams. That entire scene of Arindam being happy at all the money being thrown at him and then sinking and not being helped by Sankar Da is very iconoclastic - The next is the scene with his long time friend who is a labor’s union leader. His friend wants Arindam to lead from the front but he does not. Even when Arindam does get famous, Arindam does not have the courage to address them - The third scene is probably the most painful one. Arindam is relatively new to acting in films and is humiliated on the sets by a senior actor. Later, when Arindam climbs up he is visited by this senior actor as a frail human being. The whole movie is very well shot.\nThere is one more scene which is probably amazing to look at and is more significant. Arindam to get back to sleep drinks on the train and is looking out of the tracks. Tracks which are glistened by a light spark following the tracks. I have spent a considerable weekends in my childhood train travelling back and forth and this was something that I observed during all parts of the train. That light on the track follows."
  },
  {
    "objectID": "posts/Pretraining.html",
    "href": "posts/Pretraining.html",
    "title": "Pretraining on unlabeled data",
    "section": "",
    "text": "In the previous chapter, we learnt how to build a GPT model from scratch. In this chapter, we learn how to train the model and eventually load the OpenAI weights.\nI have structured this post a bit differently than the book; applying a machine learning perspective helped me see patterns that made a new layout feel more intuitive. Although I’m using GPT-2 as the primary example, this structure holds up for other models as well."
  },
  {
    "objectID": "posts/Pretraining.html#there-are-some-obvious-questions-here",
    "href": "posts/Pretraining.html#there-are-some-obvious-questions-here",
    "title": "Pretraining on unlabeled data",
    "section": "1.1 There are some obvious questions here:",
    "text": "1.1 There are some obvious questions here:\nQ: How do we define training and validation set? A: We define training and validation sets using a single split. It’s important to note that the validation set is not the training set—the model never learns from it directly. The reason I mention this distinction is that we do not re-sample the validation set over and over again; it stays static. Think of the validation set as a “hold-out” exam that you do not feed the model during the training of weights.\nQ: How do we calculate loss? A: We use a metric called Cross Entropy Loss. This is the same metric we use for calculating the batch loss during optimization. However, the metrics we use to judge performance might be different. In an ML model, we need a “math heavy” loss metric (something smooth that we can calculate gradients over) when we are optimizing our algorithm, and an “intuitive” metric (like Accuracy) when we are just measuring performance. Think about Cross Entropy loss as the engine for optimization, while things like F1-Score or Accuracy are the dashboard gauges—they are great for humans to read, but not suitable for calculating gradients.\nQ: What is Cross Entropy Loss? A: Cross Entropy Loss is a way to calculate the difference between two probability distributions. Technically, it is measured as the negative logarithm of the predicted probability for the correct class. In simpler terms, if the model predicts a low probability for the actual answer, the Cross Entropy loss shoots up high.\nQ: If this is supervised learning, what is the \\(y\\) variable? A: The variable is the token sequence shifted by 1. Think of this like a forecasting problem: we have a \\(y\\) variable (the target) that we are trying to predict, which leads our current input value by 1 step. This is often called “Next Token Prediction.” With each new token predicted, we append this new token to our sequence to generate the next one.\nEnough talk, let’s build this structure step by step."
  },
  {
    "objectID": "posts/Pretraining.html#defining-training-validation-sets",
    "href": "posts/Pretraining.html#defining-training-validation-sets",
    "title": "Pretraining on unlabeled data",
    "section": "2.1 Defining training & Validation Sets",
    "text": "2.1 Defining training & Validation Sets\nWe start by loading the data we had downloaded in Chapter 02.\n\n\nCode\nimport tiktoken\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\nfile_path = \"the-verdict.txt\"\nwith open(file_path,\"r\",encoding=\"utf-8\") as file:\n    text_data = file.read()\n\ntotal_characters = len(text_data)\ntotal_tokens = len(tokenizer.encode(text_data))\nprint(\"Character length: \", total_characters) \nprint(\"Tokens:\", total_tokens)\n\n\nCharacter length:  20479\nTokens: 5145\n\n\nHaving loaded the file, we now split this into training and validation set.\n\n\nCode\ntrain_ratio = 0.9\n\nsplit_idx = int(train_ratio * total_characters) # 0.9 * 20479 = 18431\ntrain_data = text_data[:split_idx]\nval_data = text_data[split_idx:]\n\n\nThis is as simple as that. This approach has a few problems. We might be breaking a token mid word. But for now this would suffice."
  },
  {
    "objectID": "posts/Pretraining.html#defining-the-x-and-y-variables",
    "href": "posts/Pretraining.html#defining-the-x-and-y-variables",
    "title": "Pretraining on unlabeled data",
    "section": "2.2 Defining the \\(X\\) and \\(y\\) variables",
    "text": "2.2 Defining the \\(X\\) and \\(y\\) variables\nThis is mostly a recap from Chapter 02 of the book. In the last section of Chapter 02, we had created batches of token with the following code. The class GPTDatasetV1 defines the class which gives the structure on how to load the datasets with DataLoader.\nThe DataLoader using this schema, defined by GPTDatasetV1, loads the datasets.\nI am using the same code as Chapter 02 with more comments for clarity.\n\n\nCode\nimport tiktoken\n# This is the GPTDatasetV1 class\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n\nclass GPTDatasetV1(Dataset):\n    def __init__(self, txt, tokenizer, max_length, stride):\n        \"\"\"\n        txt: Input dataset\n        tokenizer: Tokenizer for encoding text to numbers\n        max_length: Maximum length of tokens to be in a batch\n        stride: When we move from one batch to another, this defines the jump between two successive tokens\n\n        \"\"\"\n\n        self.input_ids = []\n        self.target_ids = []\n        token_ids = tokenizer.encode(txt) # We encode text with the tokenizer\n\n        for i in range(0, len(token_ids) - max_length, stride): # Max length is the length of each batch aka tokens in each batch\n            input_chunk = token_ids[i: i + max_length]\n            target_chunk = token_ids[i + 1: i + max_length + 1] # This is the target chunk or the y variable\n            # Another thing to note here is that both target and input are of the same length. This is different from forecasting problems\n\n            self.input_ids.append(torch.tensor(input_chunk))\n            self.target_ids.append(torch.tensor(target_chunk))\n\n    def __len__(self):\n        return len(self.input_ids)\n    \n    def __getitem__(self, index):\n        return self.input_ids[index], self.target_ids[index]\n\n\n\ndef create_dataloader_v1(txt, batch_size = 4, max_length = 256,\n                         stride = 128, shuffle=True, drop_last = True,\n                         num_workers = 0):\n\n    \"\"\"\n    batch_size: The number of independent sequences (samples) processed simultaneously in a single training step.\n    max_length: The fixed number of tokens contained within each individual input sequence (also known as the context window).\n    shuffle: Randomizes the order of the prepared chunks at the start of every epoch to prevent the model from learning the document's sequence.\n    drop_last: Discards the final batch if it contains fewer samples than the specified batch_size, ensuring consistent tensor shapes for the GPU.\n    num_workers: The number of additional CPU subprocesses used to parallelize the data loading and preprocessing to keep the GPU fed.\n    \"\"\"\n    tokenizer = tiktoken.get_encoding(\"gpt2\")\n\n    dataset = GPTDatasetV1(txt, tokenizer,max_length,stride)\n\n    dataloader = DataLoader(dataset=dataset,\n                            batch_size=batch_size,\n                            shuffle=shuffle,\n                            drop_last=drop_last,\n                            num_workers=num_workers)\n    \n    return dataloader\n\n\nHere is a back of the envelope calculation to help visualize this. This helped me immensely make sense of all the components\nAssumption: - We have 3000 tokens. - BPE tokenizer is being used to have the same tokenizer\nCalculations:\nDataset Variable Calculations: Total Tokens(N) = 3000 Max Length(L) = 256 Stride(S) = 128 Stop point(N-L): 3000 - 256 = 2744\nRange starting from 0 Indices for the dataset variable are: 0, 128, 256, 384, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688.\nThis marks the end of loop because anything beyond that would that the max length(256) would not be filled.\n2688 + 256 = 2934. The remaining 66 tokens are not used anywhere in our calculations. We could use these tokens with padding or special tokens for closing the loop. But for now we ignore this.\nDataset Size: 22 samples (Chunks)\nDataLoader Variable Calculations: Total Samples: 22 Batch_size = 4 Calculation: 22/4 = 5 with a remainder of 2\nThere are a total of 5 batches now. Drop_last = True means that would mean that the last 2 would be discarded. This is done to maximize GPU utilization. With shuffle = False, batches would be picked in the same order in which they were present in the Dataset. Here 0,1,2,3 represent the chunk order. Batch 1: Samples [0,1,2,3] Batch 2: Samples [4,5,6,7] Batch 3: Samples [8, 9, 10, 11] Batch 4: Samples [12,13,14,15] Batch 5: Samples [16,17,18,19]\nIf we set shuffle = True, the batch gets filled just not in the serialized order of the Dataset Variable. Shuffling helps in avoid correlations as sequential chunks are highly correlated. If we do not shuffle, the model might memorize the general patters. Batch 1: Samples [0,16,13,6] Batch 2: Samples [4,5,9,7] Batch 3: Samples [8, 3, 11, 17] Batch 4: Samples [12,2,14,18] Batch 5: Samples [16,10,15,19]\nWith this we have defined the \\(X\\) and \\(y\\) variables in a clear way. We can now use this in our analysis.\n\n\nCode\ntrain_loader = create_dataloader_v1(\n    train_data,\n    batch_size = 2,\n    max_length = GPT_CONFIG_124M[\"context_length\"],\n    stride = GPT_CONFIG_124M[\"context_length\"],\n    drop_last = True,\n    shuffle = True,\n    num_workers = 0\n)\n\nval_loader = create_dataloader_v1(\n    val_data,\n    batch_size = 2, # In practice, batch sizes, could go up to 1024.\n    max_length = GPT_CONFIG_124M[\"context_length\"],\n    stride = GPT_CONFIG_124M[\"context_length\"],\n    drop_last = True,\n    shuffle = True,\n    num_workers = 0\n)\n\nprint(\"Train Loader:\")\nfor x, y in train_loader:\n    print(x.shape, y.shape)\n\nprint(\"Validation Loader\")\nfor x, y in val_loader:\n    print(x.shape, y.shape)\n\n\nTrain Loader:\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\nValidation Loader\ntorch.Size([2, 256]) torch.Size([2, 256])"
  },
  {
    "objectID": "posts/Pretraining.html#defining-cross-entropy-loss",
    "href": "posts/Pretraining.html#defining-cross-entropy-loss",
    "title": "Pretraining on unlabeled data",
    "section": "2.3 Defining Cross Entropy loss",
    "text": "2.3 Defining Cross Entropy loss\nWe now move to create the cross-entropy loss function for a batch. We calculate losses at batch level.\n\n\nCode\ndef calc_loss_batch(input_batch, target_batch, model, device):\n    input_batch = input_batch.to(device)\n    target_batch = target_batch.to(device)\n    logits = model(input_batch) # The output is logits here. \n    # logits.flatten(0,1) means that we flatten [batch,sequence_length,tokenizer_vocab_size] to [(batch * sequence_length),tokenizer_vocab_size]\n    # target_batch.flatten flattens [batch, sequence_length] to [batch * sequence_length]\n    loss = torch.nn.functional.cross_entropy(logits.flatten(0,1), target_batch.flatten())\n    \n    return loss\n\n\nWe now calculate the training and validation sets.\n\n\nCode\ndef calc_loss_loader(data_loader, model, device, num_batches = None):\n    total_loss = 0\n    if len(data_loader) == 0:\n        return float(\"nan\")\n    elif num_batches is None:\n        num_batches = len(data_loader)\n    else:\n        num_batches = min(num_batches, len(data_loader)) # this reduces the number of batches to the total number of batches in the data loader\n    for i, (input_batch,target_batch) in enumerate(data_loader):\n        if i&lt;num_batches:\n            loss = calc_loss_batch(\n                input_batch, target_batch, model, device\n            )\n            total_loss += loss.item() # Summing the loss for each batch\n        else:\n            break\n    return total_loss/num_batches # Average loss over all batches\n\n\nThis is a generic function that works for both training and validation loss. Ideally, the validation loss should be higher than the training loss, but both should be decreasing as we increase the number of epochs.\nIt iterates over all batches, calculates the total loss across all batches and calculates the average. We use num_batches to use a smnall number of batches to speed up the evaluation during model training.\nWe will now calculate thee loss for both training and validation. Ideally the values should be significantly high.\n\n\nCode\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nwith torch.no_grad():\n    train_loss = calc_loss_loader(train_loader,model,device)\n    val_loss = calc_loss_loader(train_loader, model, device)\n\nprint(\"Training loss\", train_loss)\nprint(\"Validation loss\", val_loss)\n\n\nTraining loss 10.987583690219456\nValidation loss 10.987583584255642\n\n\nWe should not see training and validation loss only as stand alone metrics but losses over epochs.\n\nOne idea that I am still not able to wrap my head around is how do we generate the sequence length tokens in one go."
  },
  {
    "objectID": "posts/Pretraining.html#creating-a-training-loop-for-epochs-and-batch",
    "href": "posts/Pretraining.html#creating-a-training-loop-for-epochs-and-batch",
    "title": "Pretraining on unlabeled data",
    "section": "2.4 Creating a training loop for epochs and batch",
    "text": "2.4 Creating a training loop for epochs and batch\nThe goal here is to create two loops: Iterate over epochs(make multiple passes at the data), Iterate over batches. There would be a lot of new components that we introduce here which we need to introduce in one more iteration. What are the parameters that we are going to need to make this function work: - model: The model which is going to be used - train_loader, val_loader: Training dataset in DataLoader Object, Validation dataset in DataLoader object - device: CPU or GPU - num_epochs: Total passes at the whole data we want our model to make - optimizer: An optimizer that helps us adjust weights\n\n\nCode\ndef train_model_simple(model, train_loader, val_loader, optimizer,device, num_epochs, eval_freq, eval_iter, start_context, tokenizer):\n    # Initialize training, validation losses. Also tracking the total tokens seen as a result of this\n    train_losses, val_losses, track_tokens_seen = [], [], []\n\n    tokens_seen,global_step = 0, -1\n    \n    for epoch in range(num_epochs): # Epoch training loop\n        model.train()\n        for input_batch, target_batch in train_loader:\n            optimizer.zero_grad() # We reset the loss gradients from the previous batch iteration\n            loss = calc_loss_batch(input_batch, target_batch,model, device)\n            loss.backward() # We calculat the loss gradients\n            optimizer.step() # Update model weights using loss gradients\n            tokens_seen += input_batch.numel()\n            global_step += 1\n\n            if global_step % eval_freq == 0:\n                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n                print(f\"Ep {epoch +1} (Step {global_step:06d}):\"\n                f\"Train loss {train_loss:.3f},\"\n                f\"Val loss {val_loss:.3f}\")\n\n        generate_and_print_sample(model,tokenizer,  device, start_context) # Prints sample text after each epoch. Great for sense checks.\n    return train_losses, val_losses, track_tokens_seen\n\n\nLet’s build all the remaining functions that we need. We start with building the evaluate model that returns training and validation loss. It take the model as an input because we need to change the model to inference model and change it back to training mode at the end. This is also we have a conditional parameter to avoid calculating training and validation losses after each batch calculation.\n\n\nCode\ndef evaluate_model(model, train_loader, val_loader,device, eval_iter):\n    model.eval()\n    with torch.no_grad(): # This tells the model to stop calculating gradients.\n        train_loss = calc_loss_loader(\n            train_loader,model, device, num_batches = eval_iter\n        )\n        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n    model.train()\n    return train_loss, val_loss\n\n\nWe will now improve on the generate and print simple which helps us visualize what the model has generater for a sense checks. We will create text to token mapping and an inverse function. These were all defined in the previous chapter.\n\n\nCode\nfrom gpt import generate_text_simple\n\n\ndef text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text, allowed_special = {'&lt;|endoftext|&gt;'})\n    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n    return encoded_tensor\n\ndef token_ids_to_text(token_ids, tokenizer):\n    flat = token_ids.squeeze(0)\n    return tokenizer.decode(flat.tolist())\n\n\ndef generate_and_print_sample(model, tokenizer, device, start_context):\n    model.eval()\n    context_size = model.pos_emb.weight.shape[0]\n    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n    with torch.no_grad():\n        token_ids = generate_text_simple(model=model, idx=encoded, max_new_tokens=50, context_size = context_size)\n\n    decoded_text = token_ids_to_text(token_ids, tokenizer)\n    print(decoded_text.replace(\"\\n\",\"\")) # We do this to generate more compact text.\n    model.train()\n\n\nWe are only left with one other component to complete our training loop - The optimizer. Below code creates an optimizer AdamW(Adam with Weight Decay) and calls the training loop.\n\n\nCode\ntorch.manual_seed(123)\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.to(device)\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr = 0.0004,\n    weight_decay = 0.1\n)\nnum_epochs = 20\ntrain_losses, val_losses, tokens_seen = train_model_simple(\n    model, train_loader,val_loader,optimizer, device,\n    num_epochs=num_epochs, eval_freq=5, eval_iter = 5,\n    start_context=\"Every effort moves you\", tokenizer = tokenizer\n)\n\n\nEp 1 (Step 000000):Train loss 9.781,Val loss 9.933\nEp 1 (Step 000005):Train loss 8.072,Val loss 8.341\nEvery effort moves you,,,,,,,,,,,,.\nEp 2 (Step 000010):Train loss 6.752,Val loss 7.044\nEp 2 (Step 000015):Train loss 6.094,Val loss 6.597\nEvery effort moves you, the,, the,, the,,,,.\nEp 3 (Step 000020):Train loss 6.197,Val loss 6.821\nEp 3 (Step 000025):Train loss 5.237,Val loss 6.372\nEvery effort moves you.\"I was the picture.\"I was the the picture.\"I the honour, and I had\"I was.\nEp 4 (Step 000030):Train loss 4.857,Val loss 6.277\nEp 4 (Step 000035):Train loss 4.207,Val loss 6.262\nEvery effort moves you, and I had been the picture of the picture.\nEp 5 (Step 000040):Train loss 4.078,Val loss 6.243\nEvery effort moves you know it was not to the picture--I to the\nEp 6 (Step 000045):Train loss 3.225,Val loss 6.132\nEp 6 (Step 000050):Train loss 2.393,Val loss 6.151\nEvery effort moves you know,\" was not that I felt as it--I had a good-rooms, and he was, and in fact, and I had been the moment--as Jack himself, as his own painting, of Jack's \"strong. Gisburn\nEp 7 (Step 000055):Train loss 2.074,Val loss 6.237\nEp 7 (Step 000060):Train loss 1.679,Val loss 6.239\nEvery effort moves you?\"\" on a little Mrs.\"--as such--had not till his--and that one of Jack's degree to the donkey.\"I-c.\"I looked up his pictures--because he had been his\nEp 8 (Step 000065):Train loss 1.281,Val loss 6.287\nEp 8 (Step 000070):Train loss 0.988,Val loss 6.333\nEvery effort moves you?\"\"Yes--quite insensible to the fact with a laugh: \"Yes--and by me!\"He laughed again, and threw back his glory, and as once one had I turned, and down the room, when I\nEp 9 (Step 000075):Train loss 0.646,Val loss 6.379\nEp 9 (Step 000080):Train loss 0.412,Val loss 6.438\nEvery effort moves you?\"\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\nEp 10 (Step 000085):Train loss 0.332,Val loss 6.514\nEvery effort moves you?\"\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"He laughed again, and threw back the window-curtains, moved aside a _jardiniere_ full of\nEp 11 (Step 000090):Train loss 0.232,Val loss 6.596\nEp 11 (Step 000095):Train loss 0.198,Val loss 6.677\nEvery effort moves you?\"\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\nEp 12 (Step 000100):Train loss 0.157,Val loss 6.720\nEp 12 (Step 000105):Train loss 0.131,Val loss 6.754\nEvery effort moves you?\"\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\nEp 13 (Step 000110):Train loss 0.108,Val loss 6.812\nEp 13 (Step 000115):Train loss 0.086,Val loss 6.875\nEvery effort moves you?\"\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"He laughed again, and threw back the window-curtains, moved aside a _jardiniere_ full of\nEp 14 (Step 000120):Train loss 0.069,Val loss 6.867\nEp 14 (Step 000125):Train loss 0.081,Val loss 6.903\nEvery effort moves you?\"\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\nEp 15 (Step 000130):Train loss 0.078,Val loss 7.066\nEvery effort moves you?\"\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\nEp 16 (Step 000135):Train loss 0.061,Val loss 7.005\nEp 16 (Step 000140):Train loss 0.049,Val loss 6.966\nEvery effort moves you?\"I glanced after him, struck by his last word. Victor Grindle was, in fact, becoming the man of the moment--as Jack himself, one might put it, had been the man of the hour. The\nEp 17 (Step 000145):Train loss 0.074,Val loss 7.117\nEp 17 (Step 000150):Train loss 0.042,Val loss 6.979\nEvery effort moves you?\"\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\nEp 18 (Step 000155):Train loss 0.038,Val loss 7.082\nEp 18 (Step 000160):Train loss 0.026,Val loss 7.170\nEvery effort moves you?\"\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\nEp 19 (Step 000165):Train loss 0.020,Val loss 7.069\nEp 19 (Step 000170):Train loss 0.019,Val loss 7.124\nEvery effort moves you?\"\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\nEp 20 (Step 000175):Train loss 0.013,Val loss 7.140\nEvery effort moves you?\"\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n\n\nThis tells us that as the model goes through multiple epochs, the training loss decreases but the validation does not decrease as much.\nLooking at training and validation losses over epochs is incredibly hard. We should create a plot to visualize it a little better.\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\ndef plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n    fig, ax1 = plt.subplots(figsize=(5,3))\n    ax1.plot(epochs_seen, train_losses, label=\"Training Loss\")\n    ax1.plot(epochs_seen, val_losses,linestyle=\"-\",label=\"Validation Loss\")\n\n    ax1.set_xlabel(\"Epochs\")\n    ax1.set_ylabel(\"Loss\")\n    ax1.legend(loc=\"upper right\")\n    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n    ax2 = ax1.twiny()\n    ax2.plot(tokens_seen, train_losses, alpha=0)\n    ax2.set_xlabel(\"Tokens seen\")\n    fig.tight_layout()\n    plt.show()\n\nepochs_tensor = torch.linspace(0,num_epochs,len(train_losses))\nplot_losses(epochs_tensor,tokens_seen, train_losses, val_losses)\n\n\n\n\n\n\n\n\n\nThis plot shows the divergence between training and validation losses. The validation loss being higher than the training data indicates that the model is overfiting on the training data.\nThis closes the training loop. We started out with a general purpose guide on training a deep learning model and then built a specific GPT-2 version of the training loop. We build specific components that helped us train and look at the training and validation data with its specific losses. We now move to something very specific to LLMs."
  },
  {
    "objectID": "posts/Pretraining.html#temperature-scaling",
    "href": "posts/Pretraining.html#temperature-scaling",
    "title": "Pretraining on unlabeled data",
    "section": "3.1 Temperature Scaling",
    "text": "3.1 Temperature Scaling\nTemperature works by adding a probabilistic selection process to the next-token generation task. In the greedy approach(generate_text_simple) we sample the token with the highest probability. To generate text with more variety, we replace argmax with a function that samples from a probability distribution.\nIllustrating this below to reduce the fluff. We start with a dictionary and its reverse map.\nWe introduce a next_token_logits which we assume is the logits output from the GPT2 model for “every effort moves you”. We then implement the greedy algorithm that selects the maximum probability after softmaxing over the logits.\n\n\nCode\nvocab = {\n    \"closer\": 0,\n    \"every\": 1,\n    \"effort\": 2,\n    \"forward\": 3,\n    \"inches\": 4,\n    \"moves\": 5,\n    \"pizza\": 6,\n    \"toward\": 7,\n    \"you\": 8\n    } \n\ninverse_vocab = {v: k for k, v in vocab.items()}\n\nnext_token_logits = torch.tensor([4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79])\n\nprobas = torch.softmax(next_token_logits, dim=0)\nnext_token_id = torch.argmax(probas).item()\nprint(inverse_vocab[next_token_id])\n\n\nforward\n\n\nThe largest logit in the sequence is at 4th. The softmax over the logits would mean that the highest probabilty would be at 4th. Thus, the next token is 4th which in our case is “forward”.\nWe now move to a probabilistic sampling process where we replace the argmax with multinomial sampling.\n\n\nCode\ntorch.manual_seed(123)\nnext_token_id = torch.multinomial(probas, num_samples=1).item()\nprint(inverse_vocab[next_token_id])\n\n\nforward\n\n\nThis will also print the same output as before - “forward”, because multinomial distribution is dependent on the probability score. The way it is different from the Greedy approach is because if we generate multiple samples, we would see a probability distribution instead of a single word upfront.\n\n\nCode\ndef print_sampled_tokens(probas):\n    torch.manual_seed(123)\n    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n    sample_ids = torch.bincount(torch.tensor(sample))\n    for i, freq in enumerate(sample_ids):\n        print(f\"{freq} x {inverse_vocab[i]}\")\n\nprint_sampled_tokens(probas)\n\n\n73 x closer\n0 x every\n0 x effort\n582 x forward\n2 x inches\n0 x moves\n0 x pizza\n343 x toward\n\n\nThis allows some variation on the text. We could improve on this by adding a temperature scaling which is a fancy description for dividing the logits by a number greater than 0.\n\n\nCode\ndef softmax_with_temperature(logits, temperature):\n    scaled_logits = logits/temperature\n    return torch.softmax(scaled_logits, dim=0)\n\n\nTemperatures greater than 1 would result in a more uniform distribution of token probabilities and temperatures smaller than 1 will result in sharper or peaky distribution. Here is a plot that illustrates the temperatures impact:\n\n\nCode\ntemperatures = [1,0.1,5]\nscaled_probas = [softmax_with_temperature(next_token_logits,T) for T in temperatures]\n\nx = torch.arange(len(vocab))\n\nbar_width = 0.15\n\nfix, ax = plt.subplots(figsize=(5,3))\n\nfor i,T in enumerate(temperatures):\n    rects = ax.bar(x + i * bar_width, scaled_probas[i],\n    bar_width, label = f\"Temperature = {T}\")\nax.set_ylabel(\"Probabibility\")\nax.set_xticks(x)\nax.set_xticklabels(vocab.keys(),rotation=90)\nax.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThis is pretty much in line with what we had discussed. The smaller the temperature the more sharper the distribtution. Using a temperature of 1 means no temperature scaling."
  },
  {
    "objectID": "posts/Pretraining.html#top-k-sampling",
    "href": "posts/Pretraining.html#top-k-sampling",
    "title": "Pretraining on unlabeled data",
    "section": "3.2 Top-k Sampling",
    "text": "3.2 Top-k Sampling\nTop-k Sampling works by restrict the sampled tokens to the top-k most likely tokkens and exclude all other tokens from the selection process by masking their probability scores. This relies on the softmax of negative infinity is zero.\n\n\nCode\ntop_k = 3\ntop_logits, top_pos = torch.topk(next_token_logits, top_k)\nprint(\"Top logits:\", top_logits)\nprint(\"Top positions:\", top_pos)\n\n\nTop logits: tensor([6.7500, 6.2800, 4.5100])\nTop positions: tensor([3, 7, 0])\n\n\nThis gives us the top 3 tokens based on their logits values. We now create the masking of the tokens.\n\n\nCode\nnew_logits = torch.where(\n    condition=next_token_logits &lt; top_logits[-1], # identify the logits less than top 3\n    input=torch.tensor(float('-inf')), # Replace those less than the criteria with -inf\n    other=next_token_logits # Retains the original logits for all other tokens\n)\n\nprint(new_logits)\n\ntopk_probas = torch.softmax(new_logits, dim=0)\n\nprint(topk_probas)\n\n\ntensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\ntensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n\n\nWe get a three non zero probabilities as a result of this. We can now apply temperature scaling and multinominal functions on top of this."
  },
  {
    "objectID": "posts/Pretraining.html#modify-the-text-generation-function",
    "href": "posts/Pretraining.html#modify-the-text-generation-function",
    "title": "Pretraining on unlabeled data",
    "section": "3.3 Modify the text generation function",
    "text": "3.3 Modify the text generation function\nWe can move to create a single text generation function that combines the two approaches discussed earlier.\n\n\nCode\ndef generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k = None, eos_id=None):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:,-context_size:]\n        with torch.no_grad():\n            logits = model(idx_cond)\n        logits = logits[:,-1,:]\n        if top_k is not None:\n            top_logits,_ = torch.topk(logits, top_k)\n            min_val = top_logits[:,-1]\n            logits = torch.where(\n                logits &lt; min_val,\n                torch.tensor(float('-inf')).to(logits.device),\n                logits\n            )\n        if temperature &gt; 0.0:\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim = -1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n        if idx_next == eos_id:\n            break\n        idx = torch.cat((idx, idx_next), dim=1)\n    return idx\n\n\ntorch.manual_seed(123)\ntoken_ids = generate(model=model,\nidx=text_to_token_ids(\"Every effort moves you\", tokenizer),\nmax_new_tokens=14,\ncontext_size=GPT_CONFIG_124M[\"context_length\"],\ntop_k=25,\ntemperature=1.4\n)\n\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n\n\nOutput text:\n Every effort moves youlit terrace.\n\nI glanced after him, one handsome \"\n\n\nThis function shows how can we use all the three approaches in one go.\nWe finally have tokens being generated from the models. We can proceed to saving these model weights."
  },
  {
    "objectID": "posts/Pretraining.html#the-hardware-reality-compute-flops",
    "href": "posts/Pretraining.html#the-hardware-reality-compute-flops",
    "title": "Pretraining on unlabeled data",
    "section": "5.1 1. The Hardware Reality (Compute & FLOPs)",
    "text": "5.1 1. The Hardware Reality (Compute & FLOPs)\nTraining isn’t just about logic; it’s about physics. I want to calculate the FLOPs (Floating Point Operations) required for a training run and understand how hardware constraints—like VRAM and memory bandwidth—dictate the design of the training loop."
  },
  {
    "objectID": "posts/Pretraining.html#scaling-laws-parameter-math",
    "href": "posts/Pretraining.html#scaling-laws-parameter-math",
    "title": "Pretraining on unlabeled data",
    "section": "5.2 2. Scaling Laws & Parameter Math",
    "text": "5.2 2. Scaling Laws & Parameter Math\nThe relationship between model size (parameters) and dataset size (tokens) isn’t arbitrary. I want to explore the mathematical scaling laws that determine whether a model is “compute-optimal” and how these ratios have shifted from the original GPT-2 days to the modern reasoning models of today."
  },
  {
    "objectID": "posts/Pretraining.html#the-closed-loop-audit",
    "href": "posts/Pretraining.html#the-closed-loop-audit",
    "title": "Pretraining on unlabeled data",
    "section": "5.3 3. The “Closed-Loop” Audit",
    "text": "5.3 3. The “Closed-Loop” Audit\nBecause this model is small enough to be transparent, I want to track a single batch of data from the tokenizer’s sub-word embeddings through the attention layers to the final logits. Mapping the exact transformation of parameters at each step will help formalize my intuition before moving on to more complex architectures.\nBy deconstructing the pipeline from the inside out, I’m not just learning to build a model—I’m learning to optimize the engine."
  },
  {
    "objectID": "posts/Weekly Updates - 13.html",
    "href": "posts/Weekly Updates - 13.html",
    "title": "Weekly Updates - 13/2025",
    "section": "",
    "text": "After contemplating my 10-year corporate journey for several months, I’ve come to an unexpected realization. When I started, I believed that dedicating a decade to my profession would make me exceptionally skilled at it.\nIn my early career days, I naively thought five years would be sufficient to achieve meaningful accomplishments in data science. Looking back, I can only smile at my youthful optimism.\nThe reality is that I’ve rarely paused to critically evaluate my career trajectory. There’s a sense that I’ve simply drifted along the current. Despite making conscious efforts toward promotion, these attempts haven’t significantly impacted my professional growth so far.\n\n\n\n\nChatGPT released their new image generation model, sparking excitement about Ghibli-style transformations and igniting a Twitter discourse. My thoughts:\n\nOn AI “stealing” artistic styles: As an Indian, I view digital rights and AI art debates through a unique lens. The concept feels somewhat abstract in my context.\nGlobal perspective: Most people engaging with these tools aren’t Ghibli aficionados - they’re following trends. Meanwhile, Ghibli gains worldwide exposure through these interactions.\nCorporate caution: Meta likely has similar technology in development but hesitates on release due to copyright concerns. They’re playing it safe while LLMs force us to reconsider intellectual property frameworks altogether.\nHumorous observation: Given India’s unique relationship with copyright (considering T-series and Pritam’s success), perhaps we could offer “Judiciary as a Service” to tech giants navigating digital rights challenges.\n\n\n\n\nGemini 2.5 appears to outperform Claude 3.7. It’s fascinating that Google maintains technical superiority yet struggles with market perception in AI. The best technology doesn’t always win the market.\nComparing ChatGPT, Gemini, Claude, and Grok reveals ChatGPT’s strength lies primarily in marketing - an advantage that may fade without developing stronger B2B applications beyond their core model.\n\n\n\nSatya Nadella stands out for his exceptional clarity as a CEO. His conversation with Dwarkesh about AGI timeline assessment was remarkably thoughtful. Nadella suggests that true AGI would manifest as a 10% GDP increase within three years - indicating that economic benchmarks remain the truest measure of technological impact, and that current “intelligence as a service” hasn’t yet achieved substantial enterprise adoption.\n\n\n\n\nI’m interested in exploring Agents further, though I should first document HTML5 generation for model explainability. I also plan to write about import statements and self-contained UV environments."
  },
  {
    "objectID": "posts/Weekly Updates - 13.html#noteworthy-observations-this-week",
    "href": "posts/Weekly Updates - 13.html#noteworthy-observations-this-week",
    "title": "Weekly Updates - 13/2025",
    "section": "",
    "text": "ChatGPT released their new image generation model, sparking excitement about Ghibli-style transformations and igniting a Twitter discourse. My thoughts:\n\nOn AI “stealing” artistic styles: As an Indian, I view digital rights and AI art debates through a unique lens. The concept feels somewhat abstract in my context.\nGlobal perspective: Most people engaging with these tools aren’t Ghibli aficionados - they’re following trends. Meanwhile, Ghibli gains worldwide exposure through these interactions.\nCorporate caution: Meta likely has similar technology in development but hesitates on release due to copyright concerns. They’re playing it safe while LLMs force us to reconsider intellectual property frameworks altogether.\nHumorous observation: Given India’s unique relationship with copyright (considering T-series and Pritam’s success), perhaps we could offer “Judiciary as a Service” to tech giants navigating digital rights challenges.\n\n\n\n\nGemini 2.5 appears to outperform Claude 3.7. It’s fascinating that Google maintains technical superiority yet struggles with market perception in AI. The best technology doesn’t always win the market.\nComparing ChatGPT, Gemini, Claude, and Grok reveals ChatGPT’s strength lies primarily in marketing - an advantage that may fade without developing stronger B2B applications beyond their core model.\n\n\n\nSatya Nadella stands out for his exceptional clarity as a CEO. His conversation with Dwarkesh about AGI timeline assessment was remarkably thoughtful. Nadella suggests that true AGI would manifest as a 10% GDP increase within three years - indicating that economic benchmarks remain the truest measure of technological impact, and that current “intelligence as a service” hasn’t yet achieved substantial enterprise adoption."
  },
  {
    "objectID": "posts/Weekly Updates - 13.html#looking-forward",
    "href": "posts/Weekly Updates - 13.html#looking-forward",
    "title": "Weekly Updates - 13/2025",
    "section": "",
    "text": "I’m interested in exploring Agents further, though I should first document HTML5 generation for model explainability. I also plan to write about import statements and self-contained UV environments."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Why even bother with a blog?",
    "section": "",
    "text": "It is 2025, AI which can match humans is surely upon us. Every day we have breakthroughs in AI which would put careers to end.\nProbably the first to go out the window would be Software Engineers. Probably is a key word here! I don’t want Software Engineers automated. But hey Engineering in all forms has always been a cost centre for the business. And when the factory workers were being automated by assembly line noone considered their feelings. Also the fooling around with three instances of Dev, QA, Prod and making databases and selling them as a Gold Pill for businesses(SalesForce) was probably not the right move.\nThese models or atleast the genesis of these models was based on the entirety of Internet data. Noone cared about the Copyright data and Terms of Service. Data from Reddit which was at one time the Front Page of the Internet was used. (It isn’t now, it is mostly a porn website too scared of Wallstreet to call it one.) At the time of writing, one of the leaders of AI movement Open AI is claiming that a more cost effective model originating from China was trained on Open AI’s outputs. That is not very open is it?\nIf a machine can generate text with pretty much the same intent as me, the text by itself would not matter. Instead I matter and ergo the content I put out there matters.\nBefore I go on ranting about why Blogging is amazing, I want to put one thought upfront. I do not support the argument of writing makes you think better. I think it just is. I will write more about this and it could be writings for another blog.\nI want to write because I feel it can connect me with people who share similar interests as mine. There are faster means out there. I prefer blogging. I should ahve started blogging a decade earlier because I was convinced. But my inherent laziness I have found a way to do this for 10 years. 10 years is 3653 days. I feel like an idiot.\nAt the begining, it was shame guilt fear of failure. What if I could not write perfectly? What if my words do not resonate the intended audience? Later it was business and then followed by more guilt of why did I not do this before.\nI would not drive more into why I did not to do ti? Why now? There is plenty of writings for going forward.\nI want to write about my learings in the week. I read a lot of mathematics papers. I try to code most of the papers but at the end of the year it always feels like I have done nothing. I also undertake multiple courses. None of that is documented anyhwere.\nAll of what I am writing doing and documenting is going to be redundant because of AI. But this blog should continue regardless.\nTo document what I was. To be a person who could be reached out to. To show what the World means to me. For I cannot claim to be a champion of this world yet!"
  },
  {
    "objectID": "posts/ISIC.html",
    "href": "posts/ISIC.html",
    "title": "A Hopeful Guide to Navigating Spinal Injury: Our Family’s Experience at ISIC",
    "section": "",
    "text": "My father has had two major surgeries in the neck at the Indian Spinal Injury Center, and both have been successful. But this post isn’t just about that. I want this post to give you hope that there is a hospital which could really help you in case you suffer from a spine injury. The world does not turn into a bleak place. Please treat this as a general playbook from one family to another, not as medical advice. Anything I mention that sounds like medical advice here should be discussed with your own doctors."
  },
  {
    "objectID": "posts/ISIC.html#the-main-leverage-is-time",
    "href": "posts/ISIC.html#the-main-leverage-is-time",
    "title": "A Hopeful Guide to Navigating Spinal Injury: Our Family’s Experience at ISIC",
    "section": "The main leverage is time",
    "text": "The main leverage is time\nTime is of the essence when dealing with spinal injuries. The progression of these injuries is rapid and can lead to significant deterioration within a short period, often no more than 10 days to reach a critical stage. Spinal injuries follow an almost exponential curve of decline if not addressed promptly.\nInitial symptoms may include a loss of balance, followed by numbness in the fingers and legs. This can then progress to a lack of coordination in the legs, causing a limping gait. In advanced stages, individuals may lose motor functions and control over bladder and bowel movements, leading to a profound loss of confidence. My father experienced this severe stage during his first spinal injury.\nFortunately, we were able to intervene earlier during his second injury. It is crucial to understand that this deterioration can occur within 1 to 3 days. A fall can significantly accelerate this process. Therefore, it is vital not to underestimate the severity of these injuries and to prevent misdiagnosis. My father’s initial symptoms were mistakenly attributed to diabetes."
  },
  {
    "objectID": "posts/ISIC.html#we-are-in-the-same-boat-you-and-i",
    "href": "posts/ISIC.html#we-are-in-the-same-boat-you-and-i",
    "title": "A Hopeful Guide to Navigating Spinal Injury: Our Family’s Experience at ISIC",
    "section": "We are in the same boat you and I",
    "text": "We are in the same boat you and I\nIndian Healthcare for all its pros and cons is working like an engine. There are great hospitals near Delhi which are semi private. ISIC is one of them. You could go to AIMS for spinal injuries. But I would suggest against that. The reason is the backlog that AIMS has. No doubt, AIMS is the most affordable. It has probably the best doctors from India. But I think ISIC does a reasonable job of maintaining the balance between cost and good service. I don’t want to delve into past laurels because they are pretty much useless when it comes to the present day running of the hospital but ISIC runs smoothly. If you have Insurance they will be happy to take you in. I did not deal in cash but cash processing is also smooth judging by the queue."
  },
  {
    "objectID": "posts/ISIC.html#overcoming-the-surgery",
    "href": "posts/ISIC.html#overcoming-the-surgery",
    "title": "A Hopeful Guide to Navigating Spinal Injury: Our Family’s Experience at ISIC",
    "section": "Overcoming the surgery",
    "text": "Overcoming the surgery\nOne of the best things about this hospital is its rehabilitation unit. It is where the post-operative care happens, and this is not treated as an add-on; it is considered part of the treatment. All of the people I interacted with were very patient with patients. They would answer all your questions. My father was able to walk after 2 months of the surgery. I consider this to be a miracle of sorts. I think the hospital staff does a lot of things really well, but there is little being done to ensure transparency between departments. The doctors are transparent, but they do not always help the nursing staff explain their decisions. This leads to confusion for the attendants. The ICU times are perfect and do not need change. The hospital even gave me some slack for feeding my father without the nurse, which I am incredibly grateful for. Insurance paperwork is really a pain and there are 6 desks to visit. For an attendant, it may seem overwhelming. It is streamlined and people are really helpful, but from day 1, the paperwork seems like a lot. My only benchmark is Max Gurgaon near the Huda Metro Station where the paperwork is just a few signatures. But the positive is everyone tries to help. The staff in the Admissions office help you even if they are non-chalant"
  },
  {
    "objectID": "posts/ISIC.html#a-practical-guide-to-attendants",
    "href": "posts/ISIC.html#a-practical-guide-to-attendants",
    "title": "A Hopeful Guide to Navigating Spinal Injury: Our Family’s Experience at ISIC",
    "section": "A practical guide to attendants",
    "text": "A practical guide to attendants\n\nCoffee at the main cafeteria is expensive. Go to the retail store and they will sell you the same coffee for cheap. I needed a lot of coffee.\nJunior doctors are your friends. Please listen to them\nGuards are better in the night than during the day\nHave good relations with the nurses, they do more paperwork in a day than what you do in a year. They will help you when you need it. Don’t ask for your test results from them because the doctor is supposed to inform you of that. If the doctors become of aware of this, nurses get a stick from the head nurse\nWhatever games you are thinking that the hospital is playing to increase your bills, it is not\nICU is Intensive Care Unit. Avoid touching anything and use copious amounts of hand sanitizer if you must touch something\nHDU is a High Dependency Unit. This is confusing to a lot of the patients. This means that the patient is out of danger but still needs active monitoring. The timings of visit are different from ICU. But rules are the same as the ICU. The doctor is there during visiting times to give you updates.\nDo not get nurses in trouble with the doctors\nFloor managers are there for assessing the arrangements. Please do not use them to get your way because they are the admin\nIf you are the sole attendant and there is no place to sleep, the best place is your car or near the emergency beside the Cafe Coffee Day. Do not go the attendant’s ward because that place is likely to be filled and is a den of mosquitos"
  },
  {
    "objectID": "posts/ISIC.html#closing-notes",
    "href": "posts/ISIC.html#closing-notes",
    "title": "A Hopeful Guide to Navigating Spinal Injury: Our Family’s Experience at ISIC",
    "section": "Closing Notes",
    "text": "Closing Notes\nThere are diseases which you think do not make sense but you need to make sense of them. Spine injuries are one of them. Between these surgeries I have spent a month in the hospital. These are the toughest times you can endure. If anything, I want you to remember that you are in safe hands. Please trust your primary team and do not self medicate. I hope if you are looking for hope. This may be one."
  },
  {
    "objectID": "posts/Attention_Mechanisms.html",
    "href": "posts/Attention_Mechanisms.html",
    "title": "Chapter 03: Coding Attention Mechanisms",
    "section": "",
    "text": "This chapter deals with attention mechanisms. At a very high level, the flow of the chapters is as follows:"
  },
  {
    "objectID": "posts/Attention_Mechanisms.html#better-but-slightly-more-compute-constrained-approach",
    "href": "posts/Attention_Mechanisms.html#better-but-slightly-more-compute-constrained-approach",
    "title": "Chapter 03: Coding Attention Mechanisms",
    "section": "1.1 Better but slightly more compute-constrained approach",
    "text": "1.1 Better but slightly more compute-constrained approach\nNaturally, we should follow how human translators work: take input in the language, synthesize it, and then return the output in the target language. In a computational sense, this is what Encoder-Decoders are. The Encoder acts like the translator listening to the source sentence and forming a mental “summary” of the idea. This summary—technically called a context vector—is then passed to the Decoder, which begins speaking the sentence in the new language. Before Transformers, Recurrent Neural Networks (RNNs) were the standard norm. RNNs are well-suited for sequential data because they process information one step at a time, feeding the output of the previous word into the current step to maintain a sense of history."
  },
  {
    "objectID": "posts/Attention_Mechanisms.html#encoder-decoder-rnns-trouble",
    "href": "posts/Attention_Mechanisms.html#encoder-decoder-rnns-trouble",
    "title": "Chapter 03: Coding Attention Mechanisms",
    "section": "1.2 Encoder-Decoder RNNs trouble",
    "text": "1.2 Encoder-Decoder RNNs trouble\nOnce we move to encoder-decoder RNNs, the challenges are two-fold. As the input text is fed into the encoder, it processes it sequentially and updates its hidden state. It captures the entire meaning of the sentence in the final hidden state. The decoder then takes this “bottleneck” state to start generating the translation.\nThe big limitation is that RNNs can’t directly access earlier hidden states from the encoder during decoding. This leads to lost context.\n\nThe Translator’s Struggle: Imagine our translator is listening to a 50-word sentence. By the time the speaker finishes, the translator has a general “vibe” of the sentence, but they’ve likely forgotten the specific adjectives used at the very beginning. Because the RNN forces the entire meaning into one single vector, the “signal” from the start of the sequence washes out by the time it reaches the end."
  },
  {
    "objectID": "posts/Attention_Mechanisms.html#the-solution-bahdanau-attention",
    "href": "posts/Attention_Mechanisms.html#the-solution-bahdanau-attention",
    "title": "Chapter 03: Coding Attention Mechanisms",
    "section": "1.3 The Solution: Bahdanau Attention",
    "text": "1.3 The Solution: Bahdanau Attention\nThe current solution is the Attention Mechanism, which captures these data dependencies. Instead of forcing the translator to rely on a single, fading memory of the whole sentence, the Bahdanau Attention mechanism allows the translator to “re-read” specific parts of the source text as they generate each word.\nWhen the decoder is about to produce a word, it looks back at all the encoder’s hidden states and assigns a “weight” to them. If the translator is currently translating a noun, the attention mechanism tells them to “focus” more on the original noun and its modifiers in the source sentence, rather than the distant verbs.\nMathematically, the context vector for each decoding step is a weighted sum of all encoder hidden states : \\[c_t = \\sum_{i=1}^{T} \\alpha_{ti} h_i\\] Where \\(\\alpha_{ti}\\) represents the “attention score,” or how much focus the translator is giving to word while producing word . This ensures that no matter how long the sequence is, the important details are never truly lost.\n\nLet’s now jump on how the Attention mechanism works Attention mechanism is like the engine that drives the entire transformer block. Once we get a hang of the attention mechanism, the rest of the parts are relatively easy to fill in."
  },
  {
    "objectID": "posts/Attention_Mechanisms.html#step-1",
    "href": "posts/Attention_Mechanisms.html#step-1",
    "title": "Chapter 03: Coding Attention Mechanisms",
    "section": "2.1 Step 1",
    "text": "2.1 Step 1\nGenerating intermediate weight W which is the dot product on the embedding vector for each token.\n\n\nCode\nimport torch\n\n# Declaring an input token of size 6 with embedding size of 3\ninputs = torch.tensor(\n    [[0.43, 0.15, 0.89], # Your     (x^1)\n   [0.55, 0.87, 0.66], # journey  (x^2)\n   [0.57, 0.85, 0.64], # starts   (x^3)\n   [0.22, 0.58, 0.33], # with     (x^4)\n   [0.77, 0.25, 0.10], # one      (x^5)\n   [0.05, 0.80, 0.55]] # step     (x^6)\n)\n\n# For demonstration purposes, we select one input token\nquery = inputs[1]\n\n# \nattn_scores_2 = torch.empty(inputs.shape[0])\n# print(attn_scores_2)\n\n# Calculating step 1. This results in a vector of the same size as the context length\n# Context length here means the number of tokens\nfor i , x_i in enumerate(inputs):\n    attn_scores_2[i] = torch.dot(x_i, query)\n\n\nprint(f\"Attention score for second word i.e Journey {attn_scores_2}\")\n\n\nAttention score for second word i.e Journey tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n\n\nJust for more clarity, the dimension is equal to the number of tokens because the embeddding vector of the token ‘journey’ now has a dot product with all the tokens in the input text."
  },
  {
    "objectID": "posts/Attention_Mechanisms.html#step-2",
    "href": "posts/Attention_Mechanisms.html#step-2",
    "title": "Chapter 03: Coding Attention Mechanisms",
    "section": "2.2 Step 2:",
    "text": "2.2 Step 2:\nNormalizing the intermediate weights now. The weight should sum up on a scale of 0 to 1. The easiest way to do this is using softmax layer.\n\n\nCode\nattn_weights_2 = torch.softmax(attn_scores_2, dim =0)\nprint(f\"This is normalizing the vector of immediate weights {attn_weights_2}\")\nprint(f\"Sum should be 1: {attn_weights_2.sum()}\")\n\n\nThis is normalizing the vector of immediate weights tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\nSum should be 1: 1.0"
  },
  {
    "objectID": "posts/Attention_Mechanisms.html#step-3",
    "href": "posts/Attention_Mechanisms.html#step-3",
    "title": "Chapter 03: Coding Attention Mechanisms",
    "section": "2.3 Step 3:",
    "text": "2.3 Step 3:\nA weighted sum of the embeddings vector and the normalized attention weight.\n\n\nCode\nquery = inputs[1]\ncontext_vec_2 = torch.zeros(query.shape)\nfor i, x_i in enumerate(inputs):\n    context_vec_2 += attn_weights_2[i]*x_i\nprint(f\"The final attention weight: {context_vec_2}\")\n\n\nThe final attention weight: tensor([0.4419, 0.6515, 0.5683])\n\n\nThe resultant vector for one token is the same as the input token embeddings size From a code perspective, this is a relatively poor. But for demonstration purposes, this will do.\nWhen reviewing the code, an obvious question is the difference between torch.zeros vs torch.empty. THe way are pretty much the same.\n\nTorch.zero initializes matrix of all zeros. The memory allocation for this is done. Torch.empty allocates memory but does not initialize it. This means that we should not use it with a sum operation. This means that if we print torch.empty it will return whatever is present in the memory allocation from before. The reason we use it is because it is slightly faster."
  },
  {
    "objectID": "posts/Attention_Mechanisms.html#step-4",
    "href": "posts/Attention_Mechanisms.html#step-4",
    "title": "Chapter 03: Coding Attention Mechanisms",
    "section": "2.4 Step 4:",
    "text": "2.4 Step 4:\nSo far, we have only done this for one token. Let’s extend this to the whole list of input tokens.\n\n\nCode\nattn_scores = torch.empty(inputs.shape)\nprint(attn_scores)\nfor i, x_i in enumerate(inputs):\n    for j , x_j in enumerate(inputs):\n        attn_scores[i] = torch.dot(x_i, x_j)\nprint(f\"Attention score for all words {attn_scores}\")\n\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\nAttention score for all words tensor([[0.6310, 0.6310, 0.6310],\n        [1.0865, 1.0865, 1.0865],\n        [1.0605, 1.0605, 1.0605],\n        [0.6565, 0.6565, 0.6565],\n        [0.2935, 0.2935, 0.2935],\n        [0.9450, 0.9450, 0.9450]])\n\n\nA slightly advanced approach leveraging linear algebra is:\n\n\nCode\n# Step 1\nattn_scores = inputs @ inputs.T \n\n# Step 2\nattn_weights = torch.softmax(attn_scores, dim =-1) # Do it on the last dimension of the vector, which in this case is equal to token size.\n\n# Step 3\n\nall_context_vectors = attn_weights @ inputs\nprint(all_context_vectors)\n\n\ntensor([[0.4421, 0.5931, 0.5790],\n        [0.4419, 0.6515, 0.5683],\n        [0.4431, 0.6496, 0.5671],\n        [0.4304, 0.6298, 0.5510],\n        [0.4671, 0.5910, 0.5266],\n        [0.4177, 0.6503, 0.5645]])\n\n\nTo summarize iteration 1, we did a watered down version of self attention. We took one token as an input, dot product across all tokens embeddings, normalize the resultant weights across token length and finally we do a sum proudct of this normalized weight with each embedding vector. We repeat the same exercise for all tokens. Few important things to consider for the scaled version. attn_scores has dimensions: token_length * token_length attn_weights: token_length * token_length all_context_vectors: token_length * embedding_size"
  },
  {
    "objectID": "posts/Attention_Mechanisms.html#translators-learnings-from-this-step",
    "href": "posts/Attention_Mechanisms.html#translators-learnings-from-this-step",
    "title": "Chapter 03: Coding Attention Mechanisms",
    "section": "2.5 Translator’s Learnings from this step",
    "text": "2.5 Translator’s Learnings from this step\nAt this stage, our translator has stopped using a word-for-word dictionary. They’ve realized that words are defined by their neighbors. When they see the word “bank,” they now look at the rest of the sentence to see if there are words like “river” or “money.” They are building a rough ‘vibe’ of the sentence, but they’re still using a fixed set of rules and can’t yet adapt their focus based on the specific task at hand."
  },
  {
    "objectID": "posts/Attention_Mechanisms.html#translators-learnings-from-this-step-1",
    "href": "posts/Attention_Mechanisms.html#translators-learnings-from-this-step-1",
    "title": "Chapter 03: Coding Attention Mechanisms",
    "section": "3.1 Translator’s Learnings from this step",
    "text": "3.1 Translator’s Learnings from this step\nNow, the translator has learned that not all relationships are equal. By introducing \\(W_q\\), \\(W_k\\), and \\(W_v\\), they’ve developed three distinct mental frameworks: What am I looking for? (Query), What do I offer to others? (Key), and What is the actual meaning? (Value). They are no longer just looking at neighbors; they are actively learning which connections are the most important for understanding the “soul” of the sentence. There is one hack that the translator is using right now. It has a look up to future words."
  },
  {
    "objectID": "posts/Attention_Mechanisms.html#the-steps-are-the-following",
    "href": "posts/Attention_Mechanisms.html#the-steps-are-the-following",
    "title": "Chapter 03: Coding Attention Mechanisms",
    "section": "4.1 The steps are the following:",
    "text": "4.1 The steps are the following:\n\nCalculate Attention scores\nApply SoftMax and get attention weights\nMask with 0 above the diagonal\nNormalize the rows of the masked attention scores\n\nImplementing this pretty much the same way as before. We will do small codes with one go, then we will formalize with a class\n\n\nCode\n# First two steps combined\nqueries = sa_v2.W_query(inputs)\n\nkeys = sa_v2.W_key(inputs)\n\nattnn_scores = queries @ keys.T\nattn_weights = torch.softmax(attn_scores/ keys.shape[-1]** 0.5, dim = -1)\n\nprint(attn_weights)\n\n# Third step - this looks a whole lot complex but is rather simple. If you only think about the values that tril is generating.\ncontext_length = attn_scores.shape[0]\nmasked_simple = torch.tril(torch.ones(context_length,context_length))\nprint(masked_simple)\n\n# Fourth step\nrow_sums = masked_simple.sum(dim=-1, keepdim=True)\nmasked_simple_norm = masked_simple/row_sums\nprint(masked_simple_norm)\n\n\ntensor([[0.1972, 0.1910, 0.1894, 0.1361, 0.1344, 0.1520],\n        [0.1476, 0.2164, 0.2134, 0.1365, 0.1240, 0.1621],\n        [0.1479, 0.2157, 0.2129, 0.1366, 0.1260, 0.1608],\n        [0.1505, 0.1952, 0.1933, 0.1525, 0.1375, 0.1711],\n        [0.1571, 0.1874, 0.1885, 0.1453, 0.1819, 0.1399],\n        [0.1473, 0.2033, 0.1996, 0.1500, 0.1160, 0.1839]])\ntensor([[1., 0., 0., 0., 0., 0.],\n        [1., 1., 0., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 1., 0., 0.],\n        [1., 1., 1., 1., 1., 0.],\n        [1., 1., 1., 1., 1., 1.]])\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000],\n        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000],\n        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]])\n\n\nThis is a pretty neat implementation which hits our objective of causal attention model right on the head. However, there is a room for a few improvements primarily from a mathematical standpoint. 1. Use negative infinity to get rid of that extra normalization at step 4. The whole idea of step 4 was normalize all the values left after zeroing upper traingle of the matrix.When we use negative infinity with softmax the output is 0 and 2. Dropouts addition to avoid overfitting - This is a technique where randomly selected hidden layer inputs are ignored during training. This avoids overfitting by making sure that the model does not rely heavily on a specific set of hidden layer inputs. Dropouts is only used during training and not inference.\nWe can combine all of this in one go and create our final class. We will also make it work with batches.\n\n\nCode\nbatch = torch.stack((inputs,inputs), dim = 0)\nprint(batch.shape) ## The shape now must be 2 * 6 * 3 which is 2 items in the batch. 6 tokens in each batch. Each token with an embedding size of 3\n\nclass CausalAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n        super().__init__()\n        self.d_out = d_out\n        self.W_query = nn.Linear(d_in,d_out,bias = qkv_bias)\n        self.W_key = nn.Linear(d_in,d_out,bias = qkv_bias)\n        self.W_value = nn.Linear(d_in,d_out,bias = qkv_bias)\n        self.dropout = nn.Dropout(dropout) # The input here indicates the percentage of values in the matrix that must be zero\n\n        # This tells pytorch that these weights are non trainable so no gradient-descent for them. The mask is saved in state_dict and is loaded/saced with the model\n        self.register_buffer(\n            'mask',\n            torch.triu(torch.ones(context_length,context_length),diagonal=1)\n        ) # Will take this in a bit\n\n    def forward(self, x):\n        b, num_tokens, d_in = x.shape # This is the reason that we do not need to initialize the d_in. d_in is derived from an object of the class\n\n        # Same step as the previous loop\n        # Shape of x = 2,6,3\n        # Shape of key, query, value is 2,6,2\n        keys = self.W_key(x) \n        queries = self.W_query(x)\n        values = self.W_value(x)\n\n        # Pretty much the same as the previous step. This is just doing transformations so that keys merge properly.\n        # Shape of attention scores is 2,6,6\n        # Keys.transpose essentially means switching the 1st dimenion with the second. so the dimesion is now 2,2,6\n        # queries (2,6,2) @ keys.transpose(1,2) (2,2,6) ==&gt; (2,6,6)\n        # this is because torch considers everythin after the last two dimesnions as batch. A better way to say this would be anything the matrix multipllication here is 6,2 @ 2,6 --&gt; 6,6 and then there are two batches in each.\n\n        attn_scores = queries @ keys.transpose(1,2)\n        \n        # This is making sure that all the values that are masked are now \n        # Fill elemts of the tensor with value where mask is true. The shape of the mask must be broadcastable\n        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens],-torch.inf)\n\n        # This is the same step as before.\n        attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim = -1)\n\n        # Droput we had discussed\n        # Zeroes the values of the matrix at random. the shape of the matrix is the same\n\n        attn_weights = self.dropout(attn_weights)\n\n        # This the same step as before.\n        # attn_weights dim = (2,6,6). values dim (2,6,2) --&gt; 2,6,2\n        context_vec = attn_weights @ values\n\n        return context_vec\n\n\ntorch.Size([2, 6, 3])\n\n\nThe wraps up Causal Attention. Causal attention is the base on which the multi- headed attention is based on. Causal Attention also known as masked attention is a specialized form of self-attention. It restricts a model to only review previous and current inputs in a sequence when processing any given token when computing attention scores."
  },
  {
    "objectID": "posts/Attention_Mechanisms.html#translators-learnings-from-this-step-2",
    "href": "posts/Attention_Mechanisms.html#translators-learnings-from-this-step-2",
    "title": "Chapter 03: Coding Attention Mechanisms",
    "section": "4.2 Translator’s Learnings from this step",
    "text": "4.2 Translator’s Learnings from this step\nOur translator is now writing a book. They’ve learned a vital rule: no spoilers. By masking the future, the translator forces themselves to generate the story one word at a time, using only what has been “said” so far. This discipline ensures the translation flows logically from start to finish, preventing the model from ‘cheating’ by looking at the end of the sentence before it has even started the beginning."
  },
  {
    "objectID": "posts/Attention_Mechanisms.html#going-back-to-translator",
    "href": "posts/Attention_Mechanisms.html#going-back-to-translator",
    "title": "Chapter 03: Coding Attention Mechanisms",
    "section": "6.1 Going back to translator",
    "text": "6.1 Going back to translator\nFinally, our single translator has become a full editorial team. One expert focuses on the tense of the verbs, another tracks the gender of the pronouns, and a third looks for poetic metaphors. By working in parallel, they catch nuances that a single mind would miss. They then sit down together (the final linear projection) to merge their individual notes into one perfect, polished masterpiece."
  },
  {
    "objectID": "posts/Weekly Updates - 12.html",
    "href": "posts/Weekly Updates - 12.html",
    "title": "Weekly Updates - 12/2025",
    "section": "",
    "text": "I have been away lately. Having a new born is not easy. My wife* had a baby last year :-) and the first 6 months were really demanding for her. Maternity leave was an easy time to visit our parents for long stretches of time.\nWe shifted from Gurgaon to Patna for a couple of months and then to Hazaribagh,my wife’s hometown, later. The last week was our last week in Hazaribagh.\nHazaribagh, as a town is a very quiet place to live in. Actually every place is a quiet to live in considering my lifestyle of not participating in other people’s lives. Hazaribagh was a hill station once or may it be still is. The recent rise in temperature has shown that hill stations are not hill stations any more. It does not have too many attractions per se and that may have helped preserve its beauty. It has a hill, a very big lake with numerous smaller ones adjacent to it, and is blessed with lush greenery. The clear winner among the three was the lake. It is huge in size and can be captivating in sunsets. It nestles a lot of birds who keep on chirping.\nA few more thoughts which popped:\n\nI used to think Diversity hirings were not fair. But then again life itself is not fair. I will borrow from the wisdom of people who are wiser than me. I saw 4 women change the course of their lives becacuse of their family. It made me think whether life was really worth it.\nTeam management is a tough. People find reasons to not be accountable. Everyday there is a new issue and trust as a manager cannot be given. It needs to be earnt.\nHaving smaller routines is probably a good idea. I basically put all these points as notes on Whatsapp and have fleshed out the ideas more here.\nThe difference between great and best is the ability to go through the same thing again and again without getting bored. That is how you gain perfection.\n\nThings I think I could write a blog about: 1. Indian healthcare system is following the same trajectory as the US. May be documentign what is about to come would be a good idea 2. Document all agents we have seen recently especially from Adobe & SalesForce. 3. Document all the blogs that I read in the last week\n*Before you think this not to be right words I would like you to consider a few ablations that I had tried. When I said - My wife and I had a baby, my MD asked what my contribution to the baby was. To be honest, my contribution does not compare ,much to hers but I was there. Anyways, I have come to these choice of words because it works if I say it with a smile."
  },
  {
    "objectID": "posts/GPT_from_Scratch.html",
    "href": "posts/GPT_from_Scratch.html",
    "title": "Implementing a GPT model from scratch to generate text",
    "section": "",
    "text": "One idea that has stuck with me while working with these Transformers is that they are algorithms with many moving components. To understand why a particular component exists, we need to understand the experimentation process behind it. It is imperative that we understand the moving parts involved in the algorithm first; we can worry about the experimentation bit later.\nThink of it from a sorting algorithm perspective: there are many sorting algorithms out there. We gradually evolved from one kind of sorting algorithm to another through rapid experimentation. We could think of Transformers in the same way—we started out with translation tasks using architectures like RNNs and gradually shifted to Transformers, which represent a more advanced form of sequence processing. If we spend too much time on the history of every tiny component right now, we would be in a constant “experimentation phase.” I think to learn Transformers, you don’t necessarily need that history right now. That is something we can take up once we’ve gone through the entire exercise of creating a Transformer from scratch.\nA quick summary of what we have learned so far: Large Language Models are large deep learning neural networks. Transformers are the core blocks on which LLMs are built. The core idea behind Transformers is multi-headed attention.\nIn the previous section, we worked on our understanding of Attention models. We learned how Multi-Headed Attention is a combination of self-attention and causal attention. We also learned how multi-headed attention is the engine of the Transformer models. In this section, we are going to build on that and implement the remaining components. The end objective of this chapter is to develop a working model which takes input text and returns output text based on the Transformer architecture.\nTo start, let’s define all the components of the Transformer to clearly see what other components require our attention.\nCode\ngraph TD\n    %% Input Section\n    Input([\"Every effort moves you\"]) --&gt; Tokenized[Tokenized text]\n    \n    subgraph GPT [\"GPT model\"]\n        direction TB\n        \n        %% Initial Embeddings\n        Tokenized --&gt; TE[Token embedding layer]\n        TE --&gt; PE[Positional embedding layer]\n        PE --&gt; Drop1[Dropout]\n\n        %% Transformer Block Subgraph\n        subgraph TransformerBlock [\"Transformer block (Repeated 12 times)\"]\n            direction TB\n            \n            %% First Residual Connection\n            Drop1 --&gt; LN1[LayerNorm 1]\n            LN1 --&gt; MHA[Masked multi-head attention]\n            MHA --&gt; Drop2[Dropout]\n            Drop2 --&gt; Add1((+))\n            \n            %% Residual Line 1\n            Drop1 -- \"Residual\" --- Add1\n\n            %% Second Residual Connection\n            Add1 --&gt; LN2[LayerNorm 2]\n            LN2 --&gt; FF[Feed forward]\n            FF --&gt; Drop3[Dropout]\n            Drop3 --&gt; Add2((+))\n            \n            %% Residual Line 2\n            Add1 -- \"Residual\" --- Add2\n        end\n\n        %% Final Output Layers\n        Add2 --&gt; FLN[Final LayerNorm]\n        FLN --&gt; Linear[Linear output layer]\n    end\n\n    %% Output Tensor\n    Linear --&gt; Tensor[\"&lt;b&gt;A 4 x 50,257-dimensional tensor&lt;/b&gt;&lt;br/&gt;[[ -0.0055, ..., -0.4747],&lt;br/&gt;[ 0.2663, ..., -0.4224],&lt;br/&gt;[ 1.1146, ..., 0.0276],&lt;br/&gt;[ -0.8239, ..., -0.3993]]\"]\n\n    %% Explanatory Note\n    Tensor -.-&gt; Goal[\"&lt;b&gt;Goal:&lt;/b&gt; Convert back to text&lt;br/&gt;(Last row represents 'forward')\"]\n\n    %% Styling\n    classDef blueFill fill:#88ccee,stroke:#333,stroke-width:1px;\n    classDef greyFill fill:#e0e0e0,stroke:#333,stroke-width:1px;\n    classDef darkFill fill:#555,color:#fff,stroke:#333,stroke-width:1px;\n    \n    class TransformerBlock blueFill;\n    class GPT,Tokenized,TE,PE,Drop1,LN1,LN2,FF,Drop2,Drop3,FLN,Linear,Tensor greyFill;\n    class MHA darkFill;\n\n\n\n\n\ngraph TD\n    %% Input Section\n    Input([\"Every effort moves you\"]) --&gt; Tokenized[Tokenized text]\n    \n    subgraph GPT [\"GPT model\"]\n        direction TB\n        \n        %% Initial Embeddings\n        Tokenized --&gt; TE[Token embedding layer]\n        TE --&gt; PE[Positional embedding layer]\n        PE --&gt; Drop1[Dropout]\n\n        %% Transformer Block Subgraph\n        subgraph TransformerBlock [\"Transformer block (Repeated 12 times)\"]\n            direction TB\n            \n            %% First Residual Connection\n            Drop1 --&gt; LN1[LayerNorm 1]\n            LN1 --&gt; MHA[Masked multi-head attention]\n            MHA --&gt; Drop2[Dropout]\n            Drop2 --&gt; Add1((+))\n            \n            %% Residual Line 1\n            Drop1 -- \"Residual\" --- Add1\n\n            %% Second Residual Connection\n            Add1 --&gt; LN2[LayerNorm 2]\n            LN2 --&gt; FF[Feed forward]\n            FF --&gt; Drop3[Dropout]\n            Drop3 --&gt; Add2((+))\n            \n            %% Residual Line 2\n            Add1 -- \"Residual\" --- Add2\n        end\n\n        %% Final Output Layers\n        Add2 --&gt; FLN[Final LayerNorm]\n        FLN --&gt; Linear[Linear output layer]\n    end\n\n    %% Output Tensor\n    Linear --&gt; Tensor[\"&lt;b&gt;A 4 x 50,257-dimensional tensor&lt;/b&gt;&lt;br/&gt;[[ -0.0055, ..., -0.4747],&lt;br/&gt;[ 0.2663, ..., -0.4224],&lt;br/&gt;[ 1.1146, ..., 0.0276],&lt;br/&gt;[ -0.8239, ..., -0.3993]]\"]\n\n    %% Explanatory Note\n    Tensor -.-&gt; Goal[\"&lt;b&gt;Goal:&lt;/b&gt; Convert back to text&lt;br/&gt;(Last row represents 'forward')\"]\n\n    %% Styling\n    classDef blueFill fill:#88ccee,stroke:#333,stroke-width:1px;\n    classDef greyFill fill:#e0e0e0,stroke:#333,stroke-width:1px;\n    classDef darkFill fill:#555,color:#fff,stroke:#333,stroke-width:1px;\n    \n    class TransformerBlock blueFill;\n    class GPT,Tokenized,TE,PE,Drop1,LN1,LN2,FF,Drop2,Drop3,FLN,Linear,Tensor greyFill;\n    class MHA darkFill;\nWith this block, we get a clear picture of all the components needed to build a GPT-2 model. We will build this model with a backbone which lays out all the components of the model. We then fill this backbone with all the relevant code.\nCode\ngraph LR\n    subgraph Core [\"Core Components\"]\n        direction TB\n        B2[\"2 Layer normalization\"]\n        B3[\"3 GELU activation\"]\n        B4[\"4 Feed forward network\"]\n        B5[\"5 Shortcut connections\"]\n    end\n\n    subgraph Integration\n        B6[\"6 Transformer block\"]\n        B7[\"7 Final GPT architecture\"]\n    end\n\n    subgraph Initial [\"Initial Phase\"]\n        B1[\"1 GPT backbone\"]\n    end\n\n    %% Connections\n    B2 --&gt; B6\n    B3 --&gt; B6\n    B4 --&gt; B6\n    B5 --&gt; B6\n    B6 --&gt; B7\n\n    %% Styling\n    style Core fill:none,stroke:#ccc\n    style Integration fill:#e1f5fe,stroke:#01579b\n    style Initial fill:none,stroke:#ccc\n\n\n\n\n\ngraph LR\n    subgraph Core [\"Core Components\"]\n        direction TB\n        B2[\"2 Layer normalization\"]\n        B3[\"3 GELU activation\"]\n        B4[\"4 Feed forward network\"]\n        B5[\"5 Shortcut connections\"]\n    end\n\n    subgraph Integration\n        B6[\"6 Transformer block\"]\n        B7[\"7 Final GPT architecture\"]\n    end\n\n    subgraph Initial [\"Initial Phase\"]\n        B1[\"1 GPT backbone\"]\n    end\n\n    %% Connections\n    B2 --&gt; B6\n    B3 --&gt; B6\n    B4 --&gt; B6\n    B5 --&gt; B6\n    B6 --&gt; B7\n\n    %% Styling\n    style Core fill:none,stroke:#ccc\n    style Integration fill:#e1f5fe,stroke:#01579b\n    style Initial fill:none,stroke:#ccc\nBefore we start building, we need to define the arguments required for these components and a full-fledged Transformer block to run."
  },
  {
    "objectID": "posts/GPT_from_Scratch.html#defining-the-configuration-for-gpt-2-class-of-models",
    "href": "posts/GPT_from_Scratch.html#defining-the-configuration-for-gpt-2-class-of-models",
    "title": "Implementing a GPT model from scratch to generate text",
    "section": "0.1 Defining the configuration for GPT-2 class of models",
    "text": "0.1 Defining the configuration for GPT-2 class of models\n\n\nCode\nGPT_CONFIG_124M = {\n    \"vocab_size\": 50257, # Vocabulary_Size\n    \"context_length\": 1024, # Context Length\n    \"emb_dim\": 768, # Embedding dimension\n    \"n_heads\": 12, # Number of Attention Heads \n    \"n_layers\": 12, # Number of layers\n    \"drop_rate\": 0.1, # DropOut Rate\n    \"qkv_bias\": False  # Query-Key-Value Bias\n}\n\n\nDefining these variables:\n\nvocab_size: This refers to the vocabulary of 50,257 words as per the tokenizer (BPE).\ncontext_length: The maximum number of input tokens the model can handle via positional embeddings.\nemb_dim: The dimensionality of each token vector projection.\nn_heads: The number of attention heads in the multi-head attention mechanism.\nn_layers: The number of Transformer blocks in the model.\ndrop_rate: The intensity of the dropout mechanism to prevent overfitting.\nqkv_bias: This determines the bias vector of the linear layers in the multi-head attention. We disabled this in the previous chapter."
  },
  {
    "objectID": "posts/GPT_from_Scratch.html#perhaps-in-the-next-blog-we-can-go-top-down-on-the-gpt-2-architecture-or-dive-into-one-of-the-foundational-papers.",
    "href": "posts/GPT_from_Scratch.html#perhaps-in-the-next-blog-we-can-go-top-down-on-the-gpt-2-architecture-or-dive-into-one-of-the-foundational-papers.",
    "title": "Implementing a GPT model from scratch to generate text",
    "section": "8.1 Perhaps in the next blog, we can go top-down on the GPT-2 architecture or dive into one of the foundational papers.",
    "text": "8.1 Perhaps in the next blog, we can go top-down on the GPT-2 architecture or dive into one of the foundational papers."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pratyush Sinha",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nPretraining on unlabeled data\n\n\n\nLearning\n\nPython\n\nBook-Notes\n\n\n\nA summary of my learnings and code implementations from Chapter 05 of the book.\n\n\n\nPratyush Sinha\n\n\nFeb 19, 2026\n\n\n\n\n\n\n\n\n\n\n\nImplementing a GPT model from scratch to generate text\n\n\n\nLearning\n\nPython\n\nBook-Notes\n\n\n\nA summary of my learnings and code implementations from Chapter 04 of the book.\n\n\n\nPratyush Sinha\n\n\nFeb 8, 2026\n\n\n\n\n\n\n\n\n\n\n\nChapter 03: Coding Attention Mechanisms\n\n\n\nLearning\n\nPython\n\nBook-Notes\n\n\n\nA summary of my learnings and code implementations from Chapter 03 of the book.\n\n\n\nPratyush Sinha\n\n\nFeb 1, 2026\n\n\n\n\n\n\n\n\n\n\n\nMovie Review: Nayak - the Real hero by Satyajeet Ray\n\n\n\nMovie Reviews\n\n\n\n\n\n\n\nPratyush Sinha\n\n\nApr 8, 2025\n\n\n\n\n\n\n\n\n\n\n\nWeekly Updates - 13/2025\n\n\n\nWeekly Updates\n\nlife\n\nlearnings\n\n\n\n\n\n\n\nPratyush Sinha\n\n\nMar 30, 2025\n\n\n\n\n\n\n\n\n\n\n\nWeekly Updates - 12/2025\n\n\n\nWeekly Updates\n\nlife\n\n\n\n\n\n\n\nPratyush Sinha\n\n\nMar 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nWhy even bother with a blog?\n\n\n\nmusings\n\nlife\n\n\n\n\n\n\n\nPratyush Sinha\n\n\nFeb 9, 2025\n\n\n\n\n\n\n\n\n\n\n\nA Hopeful Guide to Navigating Spinal Injury: Our Family’s Experience at ISIC\n\n\n\nPersonal Experiences\n\n\n\n\n\n\n\nPratyush Sinha\n\n\nJan 9, 2025\n\n\n\n\n\n\nNo matching items"
  }
]