<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Pratyush Sinha">
<meta name="dcterms.date" content="2026-02-01">
<meta name="description" content="A summary of my learnings and code implementations from Chapter 03 of the book.">

<title>Chapter 03: Coding Attention Mechanisms – Pratyush Sinha</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-b651517ce65839d647a86e2780455cfb.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-19300dba7cdce1167f8ebad4c9f773c0.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-81fe5dcfeb64615fe1e55f802320c342.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../site_libs/bootstrap/bootstrap-19300dba7cdce1167f8ebad4c9f773c0.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Pratyush Sinha</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/ayushiyerji"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/pratyush20p/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Chapter 03: Coding Attention Mechanisms</h1>
            <p class="subtitle lead">Learning Notes from LLMs from Scratch</p>
                  <div>
        <div class="description">
          A summary of my learnings and code implementations from Chapter 03 of the book.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Learning</div>
                <div class="quarto-category">Python</div>
                <div class="quarto-category">Book-Notes</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Pratyush Sinha </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 1, 2026</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#naive-approach-word-to-word-translation" id="toc-naive-approach-word-to-word-translation" class="nav-link active" data-scroll-target="#naive-approach-word-to-word-translation"><span class="header-section-number">1</span> Naive Approach: Word-to-Word Translation</a>
  <ul class="collapse">
  <li><a href="#better-but-slightly-more-compute-constrained-approach" id="toc-better-but-slightly-more-compute-constrained-approach" class="nav-link" data-scroll-target="#better-but-slightly-more-compute-constrained-approach"><span class="header-section-number">1.1</span> Better but slightly more compute-constrained approach</a></li>
  <li><a href="#encoder-decoder-rnns-trouble" id="toc-encoder-decoder-rnns-trouble" class="nav-link" data-scroll-target="#encoder-decoder-rnns-trouble"><span class="header-section-number">1.2</span> Encoder-Decoder RNNs trouble</a></li>
  <li><a href="#the-solution-bahdanau-attention" id="toc-the-solution-bahdanau-attention" class="nav-link" data-scroll-target="#the-solution-bahdanau-attention"><span class="header-section-number">1.3</span> The Solution: Bahdanau Attention</a></li>
  </ul></li>
  <li><a href="#attending-different-parts-of-the-input-with-self-attention" id="toc-attending-different-parts-of-the-input-with-self-attention" class="nav-link" data-scroll-target="#attending-different-parts-of-the-input-with-self-attention"><span class="header-section-number">2</span> Attending Different Parts of the Input with Self Attention</a>
  <ul class="collapse">
  <li><a href="#step-1" id="toc-step-1" class="nav-link" data-scroll-target="#step-1"><span class="header-section-number">2.1</span> Step 1</a></li>
  <li><a href="#step-2" id="toc-step-2" class="nav-link" data-scroll-target="#step-2"><span class="header-section-number">2.2</span> Step 2:</a></li>
  <li><a href="#step-3" id="toc-step-3" class="nav-link" data-scroll-target="#step-3"><span class="header-section-number">2.3</span> Step 3:</a></li>
  <li><a href="#step-4" id="toc-step-4" class="nav-link" data-scroll-target="#step-4"><span class="header-section-number">2.4</span> Step 4:</a></li>
  <li><a href="#translators-learnings-from-this-step" id="toc-translators-learnings-from-this-step" class="nav-link" data-scroll-target="#translators-learnings-from-this-step"><span class="header-section-number">2.5</span> Translator’s Learnings from this step</a></li>
  </ul></li>
  <li><a href="#implementing-self-attention-with-trainable-weights." id="toc-implementing-self-attention-with-trainable-weights." class="nav-link" data-scroll-target="#implementing-self-attention-with-trainable-weights."><span class="header-section-number">3</span> Implementing self attention with trainable weights.</a>
  <ul class="collapse">
  <li><a href="#translators-learnings-from-this-step-1" id="toc-translators-learnings-from-this-step-1" class="nav-link" data-scroll-target="#translators-learnings-from-this-step-1"><span class="header-section-number">3.1</span> Translator’s Learnings from this step</a></li>
  </ul></li>
  <li><a href="#causal-attention" id="toc-causal-attention" class="nav-link" data-scroll-target="#causal-attention"><span class="header-section-number">4</span> Causal Attention</a>
  <ul class="collapse">
  <li><a href="#the-steps-are-the-following" id="toc-the-steps-are-the-following" class="nav-link" data-scroll-target="#the-steps-are-the-following"><span class="header-section-number">4.1</span> The steps are the following:</a></li>
  <li><a href="#translators-learnings-from-this-step-2" id="toc-translators-learnings-from-this-step-2" class="nav-link" data-scroll-target="#translators-learnings-from-this-step-2"><span class="header-section-number">4.2</span> Translator’s Learnings from this step</a></li>
  </ul></li>
  <li><a href="#multi-headed-attention" id="toc-multi-headed-attention" class="nav-link" data-scroll-target="#multi-headed-attention"><span class="header-section-number">5</span> Multi-Headed Attention</a></li>
  <li><a href="#summarizing-thoughts" id="toc-summarizing-thoughts" class="nav-link" data-scroll-target="#summarizing-thoughts"><span class="header-section-number">6</span> Summarizing thoughts</a>
  <ul class="collapse">
  <li><a href="#going-back-to-translator" id="toc-going-back-to-translator" class="nav-link" data-scroll-target="#going-back-to-translator"><span class="header-section-number">6.1</span> Going back to translator</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">






<p>This chapter deals with attention mechanisms. At a very high level, the flow of the chapters is as follows:</p>
<ul>
<li><strong>Simplified Self-Attention:</strong> Introduces the broad idea.</li>
<li><strong>Self-Attention:</strong> Trainable weights that form the basis of the mechanism used in LLMs.</li>
<li><strong>Causal Attention:</strong> A type of self-attention that only considers previous and current inputs in the sequence, ensuring the temporal order of text generation.</li>
<li><strong>Multi-Head Attention:</strong> An extension of self-attention and causal attention that enables models to simultaneously attend to information from different representation spaces.</li>
</ul>
<hr>
<section id="naive-approach-word-to-word-translation" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Naive Approach: Word-to-Word Translation</h1>
<p>Imagine a translator who only has a pocket dictionary but no understanding of grammar. To translate “The early bird catches the worm,” they simply swap each word for its equivalent in the target language.</p>
<p>The immediate problem is that words cannot always be translated using a simple one-to-one replacement; a single word in one language might require three in another. The second problem is structural: some languages arrange elements differently (e.g., Subject-Verb-Object vs.&nbsp;Subject-Object-Verb). Our dictionary-wielding translator would produce a “word salad” that lacks any coherent flow or meaning.</p>
<section id="better-but-slightly-more-compute-constrained-approach" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="better-but-slightly-more-compute-constrained-approach"><span class="header-section-number">1.1</span> Better but slightly more compute-constrained approach</h2>
<p>Naturally, we should follow how human translators work: take input in the language, synthesize it, and then return the output in the target language. In a computational sense, this is what <strong>Encoder-Decoders</strong> are. The <strong>Encoder</strong> acts like the translator listening to the source sentence and forming a mental “summary” of the idea. This summary—technically called a <strong>context vector</strong>—is then passed to the <strong>Decoder</strong>, which begins speaking the sentence in the new language. Before Transformers, <strong>Recurrent Neural Networks (RNNs)</strong> were the standard norm. RNNs are well-suited for sequential data because they process information one step at a time, feeding the output of the previous word into the current step to maintain a sense of history.</p>
</section>
<section id="encoder-decoder-rnns-trouble" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="encoder-decoder-rnns-trouble"><span class="header-section-number">1.2</span> Encoder-Decoder RNNs trouble</h2>
<p>Once we move to encoder-decoder RNNs, the challenges are two-fold. As the input text is fed into the encoder, it processes it sequentially and updates its <strong>hidden state</strong>. It captures the entire meaning of the sentence in the final hidden state. The decoder then takes this “bottleneck” state to start generating the translation.</p>
<p>The big limitation is that RNNs can’t directly access earlier hidden states from the encoder during decoding. This leads to <strong>lost context</strong>.</p>
<blockquote class="blockquote">
<p><strong>The Translator’s Struggle:</strong> Imagine our translator is listening to a 50-word sentence. By the time the speaker finishes, the translator has a general “vibe” of the sentence, but they’ve likely forgotten the specific adjectives used at the very beginning. Because the RNN forces the entire meaning into one single vector, the “signal” from the start of the sequence washes out by the time it reaches the end.</p>
</blockquote>
</section>
<section id="the-solution-bahdanau-attention" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="the-solution-bahdanau-attention"><span class="header-section-number">1.3</span> The Solution: Bahdanau Attention</h2>
<p>The current solution is the <strong>Attention Mechanism</strong>, which captures these data dependencies. Instead of forcing the translator to rely on a single, fading memory of the whole sentence, the <strong>Bahdanau Attention</strong> mechanism allows the translator to “re-read” specific parts of the source text as they generate each word.</p>
<p>When the decoder is about to produce a word, it looks back at all the encoder’s hidden states and assigns a “weight” to them. If the translator is currently translating a noun, the attention mechanism tells them to “focus” more on the original noun and its modifiers in the source sentence, rather than the distant verbs.</p>
<p>Mathematically, the context vector for each decoding step is a weighted sum of all encoder hidden states : <span class="math display">\[c_t = \sum_{i=1}^{T} \alpha_{ti} h_i\]</span> Where <span class="math inline">\(\alpha_{ti}\)</span> represents the “attention score,” or how much focus the translator is giving to word while producing word . This ensures that no matter how long the sequence is, the important details are never truly lost.</p>
<hr>
<p><strong>Let’s now jump on how the Attention mechanism works</strong> Attention mechanism is like the engine that drives the entire transformer block. Once we get a hang of the attention mechanism, the rest of the parts are relatively easy to fill in.</p>
</section>
</section>
<section id="attending-different-parts-of-the-input-with-self-attention" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Attending Different Parts of the Input with Self Attention</h1>
<p><strong>Simplified Self-Attention:</strong> In this section we aim to create create context vectors. Context Vectors are just a glorified embeddings vector or in other words an enriched embedding vector.</p>
<p>A simplified self attention would have the following three steps:</p>
<blockquote class="blockquote">
<p>The first three steps are being implemented for each token.</p>
</blockquote>
<ol type="1">
<li>Calculate the intermediate weight W which is the dot product on the embedding vector for each token.</li>
<li>Normalize this resultant weights matrix across all the tokens</li>
<li>Finally we do a weighted sum of the embeddings vector based on the weight.</li>
<li>Scale to final all tokens</li>
</ol>
<p><em>This looks awfully similar to the idea of relative position. Why do we need context vectors when we have relation position encodoing before creating the embeddings vector?</em></p>
<blockquote class="blockquote">
<p>There is a fundamental difference in their purposes:</p>
</blockquote>
<blockquote class="blockquote">
<p>Position Encodings: Tell the model where a word is in a sentence (e.g., “This word is at index 2”). It provides the “coordinates.”</p>
</blockquote>
<blockquote class="blockquote">
<p>Context Vectors (Self-Attention): Tell the model what the word means in relation to others. Even if the model knows a word is at index 2, it doesn’t know if that word is a verb acting on the noun at index 5 until self-attention runs.</p>
</blockquote>
<blockquote class="blockquote">
<p>Crucial Distinction: Position encodings are additive data (spatial info); Context vectors are calculated relationships (semantic info).</p>
</blockquote>
<section id="step-1" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="step-1"><span class="header-section-number">2.1</span> Step 1</h2>
<p>Generating intermediate weight W which is the dot product on the embedding vector for each token.</p>
<div id="1207f60e" class="cell" data-execution_count="2">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Declaring an input token of size 6 with embedding size of 3</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> torch.tensor(</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    [[<span class="fl">0.43</span>, <span class="fl">0.15</span>, <span class="fl">0.89</span>], <span class="co"># Your     (x^1)</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>   [<span class="fl">0.55</span>, <span class="fl">0.87</span>, <span class="fl">0.66</span>], <span class="co"># journey  (x^2)</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>   [<span class="fl">0.57</span>, <span class="fl">0.85</span>, <span class="fl">0.64</span>], <span class="co"># starts   (x^3)</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>   [<span class="fl">0.22</span>, <span class="fl">0.58</span>, <span class="fl">0.33</span>], <span class="co"># with     (x^4)</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>   [<span class="fl">0.77</span>, <span class="fl">0.25</span>, <span class="fl">0.10</span>], <span class="co"># one      (x^5)</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>   [<span class="fl">0.05</span>, <span class="fl">0.80</span>, <span class="fl">0.55</span>]] <span class="co"># step     (x^6)</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># For demonstration purposes, we select one input token</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span> inputs[<span class="dv">1</span>]</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>attn_scores_2 <span class="op">=</span> torch.empty(inputs.shape[<span class="dv">0</span>])</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># print(attn_scores_2)</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculating step 1. This results in a vector of the same size as the context length</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Context length here means the number of tokens</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i , x_i <span class="kw">in</span> <span class="bu">enumerate</span>(inputs):</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    attn_scores_2[i] <span class="op">=</span> torch.dot(x_i, query)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Attention score for second word i.e Journey </span><span class="sc">{</span>attn_scores_2<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Attention score for second word i.e Journey tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])</code></pre>
</div>
</div>
<p>Just for more clarity, the dimension is equal to the number of tokens because the embeddding vector of the token ‘journey’ now has a dot product with all the tokens in the input text.</p>
</section>
<section id="step-2" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="step-2"><span class="header-section-number">2.2</span> Step 2:</h2>
<p>Normalizing the intermediate weights now. The weight should sum up on a scale of 0 to 1. The easiest way to do this is using softmax layer.</p>
<div id="e57aa626" class="cell" data-execution_count="3">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>attn_weights_2 <span class="op">=</span> torch.softmax(attn_scores_2, dim <span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"This is normalizing the vector of immediate weights </span><span class="sc">{</span>attn_weights_2<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sum should be 1: </span><span class="sc">{</span>attn_weights_2<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>This is normalizing the vector of immediate weights tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])
Sum should be 1: 1.0</code></pre>
</div>
</div>
</section>
<section id="step-3" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="step-3"><span class="header-section-number">2.3</span> Step 3:</h2>
<p>A weighted sum of the embeddings vector and the normalized attention weight.</p>
<div id="91056fdf" class="cell" data-execution_count="4">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span> inputs[<span class="dv">1</span>]</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>context_vec_2 <span class="op">=</span> torch.zeros(query.shape)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, x_i <span class="kw">in</span> <span class="bu">enumerate</span>(inputs):</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    context_vec_2 <span class="op">+=</span> attn_weights_2[i]<span class="op">*</span>x_i</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The final attention weight: </span><span class="sc">{</span>context_vec_2<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>The final attention weight: tensor([0.4419, 0.6515, 0.5683])</code></pre>
</div>
</div>
<p>The resultant vector for one token is the same as the input token embeddings size From a code perspective, this is a relatively poor. But for demonstration purposes, this will do.</p>
<p>When reviewing the code, an obvious question is the difference between torch.zeros vs torch.empty. THe way are pretty much the same.</p>
<blockquote class="blockquote">
<p>Torch.zero initializes matrix of all zeros. The memory allocation for this is done. Torch.empty allocates memory but does not initialize it. This means that we should not use it with a sum operation. This means that if we print torch.empty it will return whatever is present in the memory allocation from before. The reason we use it is because it is slightly faster.</p>
</blockquote>
</section>
<section id="step-4" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="step-4"><span class="header-section-number">2.4</span> Step 4:</h2>
<p>So far, we have only done this for one token. Let’s extend this to the whole list of input tokens.</p>
<div id="c97bb160" class="cell" data-execution_count="5">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>attn_scores <span class="op">=</span> torch.empty(inputs.shape)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attn_scores)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, x_i <span class="kw">in</span> <span class="bu">enumerate</span>(inputs):</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j , x_j <span class="kw">in</span> <span class="bu">enumerate</span>(inputs):</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        attn_scores[i] <span class="op">=</span> torch.dot(x_i, x_j)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Attention score for all words </span><span class="sc">{</span>attn_scores<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]])
Attention score for all words tensor([[0.6310, 0.6310, 0.6310],
        [1.0865, 1.0865, 1.0865],
        [1.0605, 1.0605, 1.0605],
        [0.6565, 0.6565, 0.6565],
        [0.2935, 0.2935, 0.2935],
        [0.9450, 0.9450, 0.9450]])</code></pre>
</div>
</div>
<p>A slightly advanced approach leveraging linear algebra is:</p>
<div id="30494de5" class="cell" data-execution_count="6">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>attn_scores <span class="op">=</span> inputs <span class="op">@</span> inputs.T </span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>attn_weights <span class="op">=</span> torch.softmax(attn_scores, dim <span class="op">=-</span><span class="dv">1</span>) <span class="co"># Do it on the last dimension of the vector, which in this case is equal to token size.</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>all_context_vectors <span class="op">=</span> attn_weights <span class="op">@</span> inputs</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(all_context_vectors)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[0.4421, 0.5931, 0.5790],
        [0.4419, 0.6515, 0.5683],
        [0.4431, 0.6496, 0.5671],
        [0.4304, 0.6298, 0.5510],
        [0.4671, 0.5910, 0.5266],
        [0.4177, 0.6503, 0.5645]])</code></pre>
</div>
</div>
<p>To summarize iteration 1, we did a watered down version of self attention. We took one token as an input, dot product across all tokens embeddings, normalize the resultant weights across token length and finally we do a sum proudct of this normalized weight with each embedding vector. We repeat the same exercise for all tokens. Few important things to consider for the scaled version. attn_scores has dimensions: token_length * token_length attn_weights: token_length * token_length all_context_vectors: token_length * embedding_size</p>
</section>
<section id="translators-learnings-from-this-step" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="translators-learnings-from-this-step"><span class="header-section-number">2.5</span> Translator’s Learnings from this step</h2>
<p>At this stage, our translator has stopped using a word-for-word dictionary. They’ve realized that words are defined by their neighbors. When they see the word “bank,” they now look at the rest of the sentence to see if there are words like “river” or “money.” They are building a rough ‘vibe’ of the sentence, but they’re still using a fixed set of rules and can’t yet adapt their focus based on the specific task at hand.</p>
<hr>
</section>
</section>
<section id="implementing-self-attention-with-trainable-weights." class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Implementing self attention with trainable weights.</h1>
<p>Trainable weights that form the basis of the mechanism used in LLMs. The previous iteration uses matrix multiplications on its own embeddings. This iteration introduces a new class of trainable weights matrices <span class="math inline">\(W_{q}\)</span>, <span class="math inline">\(W_{k}\)</span>, <span class="math inline">\(W_{v}\)</span>. These weights are randomly initialized and are used to project the input tokens into query, key, value matrices.</p>
<p>We follow the same idea of the previous step, start with one token and then generalize for all tokens.</p>
<div id="f32be53c" class="cell" data-execution_count="7">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Picking one token</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>x_2 <span class="op">=</span> inputs[<span class="dv">1</span>]</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>d_in <span class="op">=</span> inputs.shape[<span class="dv">1</span>] <span class="co"># Picking the embedding size. Here this value is 3</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>d_out <span class="op">=</span> <span class="dv">2</span> <span class="co"># This is output size of the matrix</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co"># We have initialized the random weights of the same size as the token embeddings and arbitary number 2.</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">123</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co"># All these 3 matrices are of the same size 3 * 2</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>W_query <span class="op">=</span> torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>W_key <span class="op">=</span> torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>W_value <span class="op">=</span> torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Multiplying the input token of size 1 * 3 with 3 * 2</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="co"># The result is a matrix of size 1 * 2</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>query_2 <span class="op">=</span> x_2 <span class="op">@</span> W_query </span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>key_2 <span class="op">=</span> x_2 <span class="op">@</span> W_key</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>value_2 <span class="op">=</span> x_2 <span class="op">@</span> W_value</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(query_2)</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="co"># These matrices are updated here but they could very well be used for one token</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>keys <span class="op">=</span> inputs <span class="op">@</span> W_key</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> inputs <span class="op">@</span> W_value</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"keys.shape:"</span>, keys.shape)</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"values.shape:"</span>, values.shape)</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>keys_2 <span class="op">=</span> keys[<span class="dv">1</span>]</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>attn_scores_22 <span class="op">=</span> query_2.dot(keys_2)</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attn_scores_22)</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>attn_scores_2 <span class="op">=</span> query_2 <span class="op">@</span> keys.T</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attn_scores_2)</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>d_k <span class="op">=</span> keys.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>attn_weights_2 <span class="op">=</span> torch.softmax(attn_scores_2 <span class="op">/</span> d_k <span class="op">**</span> <span class="fl">0.5</span>, dim <span class="op">=-</span><span class="dv">1</span>) <span class="co"># Why do we use d_k ** 0.5?</span></span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attn_weights_2)</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>context_vec_2 <span class="op">=</span> attn_weights_2 <span class="op">@</span> values</span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(context_vec_2)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([0.4306, 1.4551])
keys.shape: torch.Size([6, 2])
values.shape: torch.Size([6, 2])
tensor(1.8524)
tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])
tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])
tensor([0.3061, 0.8210])</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p>Why do we need <span class="math inline">\(d_{k}\)</span> ^ 0.5 ? The reason for this normalization is to improve the training performance by avoiding small gradients. This does not matter a lot now, because the embedding size is fairly small but when dealing with large GPT-like LLMs, large dot produts can result in very large gradients during backpropogration due to the softmax function applied to them. As dot products increase, the softmax function behaves like a step function resulting in very very small gradients. These large/small gradients can cause gradients to drastically slow down learning or cause training to stagnate..</p>
</blockquote>
<blockquote class="blockquote">
<p>The scaling by the square root of the embedding dimension is the reason why this self attetnion mechanism is called the scaled-dot product attention.</p>
</blockquote>
<p>This looks fairly complex. This would be way simpler if we introduce some linear algebra. The central idea is, we do two successive matrix multiplications within the three initialized weights that we have introduced. There is one normalization layer in between.The output dimension needs to be the same as in the input i.e token_count * embedding_vector_size. The below code simplifies the above code</p>
<div id="9c0472c6" class="cell" data-execution_count="8">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SelfAttention_v1(nn.Module):</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_in, d_out):</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_query <span class="op">=</span> torch.nn.Parameter(torch.rand(d_in, d_out)) <span class="co"># Got rid of requires gradient = False part</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_key <span class="op">=</span> torch.nn.Parameter(torch.rand(d_in, d_out)) </span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_value <span class="op">=</span> torch.nn.Parameter(torch.rand(d_in, d_out)) </span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>        query <span class="op">=</span> x <span class="op">@</span> <span class="va">self</span>.W_query </span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>        key <span class="op">=</span> x <span class="op">@</span> <span class="va">self</span>.W_key</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> x <span class="op">@</span> <span class="va">self</span>.W_value</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>        attn_scores <span class="op">=</span> query <span class="op">@</span> key.T</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>        attn_weights <span class="op">=</span> torch.softmax(attn_scores <span class="op">/</span> keys.shape[<span class="op">-</span><span class="dv">1</span>]<span class="op">**</span><span class="fl">0.5</span>, dim <span class="op">=</span> <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>        context_vec <span class="op">=</span> attn_weights <span class="op">@</span> values</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> context_vec</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">123</span>)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>sa_v1 <span class="op">=</span> SelfAttention_v1(d_in, d_out)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sa_v1(inputs))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[0.2996, 0.8053],
        [0.3061, 0.8210],
        [0.3058, 0.8203],
        [0.2948, 0.7939],
        [0.2927, 0.7891],
        [0.2990, 0.8040]], grad_fn=&lt;MmBackward0&gt;)</code></pre>
</div>
</div>
<p>There is this interesting quote from the book.</p>
<blockquote class="blockquote">
<p>We can improve the SelfAttention_v1 implementation further by utilizing PyTorch’s nn.Linear layers, which effectively perform matrix multiplication when the bias units are disabled. Additionally, a significant advantage of using nn.Linear instead of manually implementing nn.Parameter(torch.rand(…)) is that nn.Linear has an optimized weight initialization scheme, contributing to more stable and effective model training.</p>
</blockquote>
<p>This absolutely did not make sense to me in the first go. I have tried to simplify this below.</p>
<p>The first sentence is pretty straightforward and does not require much explaination. It means that if we turn bias = False for nn.Linear layer then what we are left with a matrix of the same size as the nn.Parameter. The second bit is the interesting one. “Optimized weight implementation scheme” here is pointing to the weight initialization by torch.rand. When torch.rand is used the sample is pulled from a normal distribution between [0,1]. The challenge with sampling from a normal distribution is two fold</p>
<ul>
<li>All weights are only positive. This restricts the model’s ability to learn complex patterns early on and can shift the output mean significantly.</li>
<li>Weights are relatively high (~0.5). This does not seem too high but is considered high for deep learning algorithms.</li>
</ul>
<p>When we use nn.Linear the advantage is a distribution from [-1,1] with mean 0. This allows weights to be negative or positive, keeping the activations centered around zero and preventing the “exploding internal variance”.</p>
<blockquote class="blockquote">
<p>Here is a deep dive that I did with Gemini. This answers two questions: Weight of 0.5 seemed too low to me. This explains that weights need to be low because we are dealing with a large number of inputs (512). If we do a scaled dot product which is sum(multiply(A*B)), if B has mean of 0.5, that means the result might be 256. This is one step. There would be multiple steps that is going to keep exploding this. After we are done with this, we also need to do a gradient calculation. That would also not make sense because the gradients are going to be volatile. To stop the sums from getting huge, the weights must get smaller as the number of inputs get larger.</p>
</blockquote>
<blockquote class="blockquote">
<p>a further deep dive on the algorithms available for weight initialization of nn.Linear and when to use them is present in the link <a href="https://gemini.google.com/app/d91673816e6cae90">Weight Initialization</a></p>
</blockquote>
<p>We add the linear layer now to improve on the first interation.</p>
<div id="7b29d6a4" class="cell" data-execution_count="9">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SelfAttention_v2(nn.Module):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_in, d_out, qkv_bias<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_query <span class="op">=</span> nn.Linear(d_in, d_out, bias<span class="op">=</span>qkv_bias)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_key   <span class="op">=</span> nn.Linear(d_in, d_out, bias<span class="op">=</span>qkv_bias)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_value <span class="op">=</span> nn.Linear(d_in, d_out, bias<span class="op">=</span>qkv_bias)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>        keys <span class="op">=</span> <span class="va">self</span>.W_key(x)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>        queries <span class="op">=</span> <span class="va">self</span>.W_query(x)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>        values <span class="op">=</span> <span class="va">self</span>.W_value(x)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>        attn_scores <span class="op">=</span> queries <span class="op">@</span> keys.T</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>        attn_weights <span class="op">=</span> torch.softmax(attn_scores <span class="op">/</span> keys.shape[<span class="op">-</span><span class="dv">1</span>]<span class="op">**</span><span class="fl">0.5</span>, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>        context_vec <span class="op">=</span> attn_weights <span class="op">@</span> values</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> context_vec</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">789</span>)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>sa_v2 <span class="op">=</span> SelfAttention_v2(d_in, d_out)</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sa_v2(inputs))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[-0.0739,  0.0713],
        [-0.0748,  0.0703],
        [-0.0749,  0.0702],
        [-0.0760,  0.0685],
        [-0.0763,  0.0679],
        [-0.0754,  0.0693]], grad_fn=&lt;MmBackward0&gt;)</code></pre>
</div>
</div>
<section id="translators-learnings-from-this-step-1" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="translators-learnings-from-this-step-1"><span class="header-section-number">3.1</span> Translator’s Learnings from this step</h2>
<p>Now, the translator has learned that not all relationships are equal. By introducing <span class="math inline">\(W_q\)</span>, <span class="math inline">\(W_k\)</span>, and <span class="math inline">\(W_v\)</span>, they’ve developed three distinct mental frameworks: What am I looking for? (Query), What do I offer to others? (Key), and What is the actual meaning? (Value). They are no longer just looking at neighbors; they are actively learning which connections are the most important for understanding the “soul” of the sentence. There is one hack that the translator is using right now. It has a look up to future words.</p>
<hr>
</section>
</section>
<section id="causal-attention" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Causal Attention</h1>
<p>Causal Attention is a type of self-attention that only considers previous and current inputs in the sequence, ensuring the temporal order of text generation.</p>
<p>Causal Attention builds on the idea of self-attention. It imprves self attention by only considering only the previous and current inputs in a sequence when processing any given token when computing the attention scores. It masks the future tokens. This is fairly simple add on. We can just zero out all the values of the upper triangles of the attention weight matrix.</p>
<section id="the-steps-are-the-following" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="the-steps-are-the-following"><span class="header-section-number">4.1</span> The steps are the following:</h2>
<ul>
<li>Calculate Attention scores</li>
<li>Apply SoftMax and get attention weights</li>
<li>Mask with 0 above the diagonal</li>
<li>Normalize the rows of the masked attention scores</li>
</ul>
<p>Implementing this pretty much the same way as before. We will do small codes with one go, then we will formalize with a class</p>
<div id="d2c1e157" class="cell" data-execution_count="10">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># First two steps combined</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>queries <span class="op">=</span> sa_v2.W_query(inputs)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>keys <span class="op">=</span> sa_v2.W_key(inputs)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>attnn_scores <span class="op">=</span> queries <span class="op">@</span> keys.T</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>attn_weights <span class="op">=</span> torch.softmax(attn_scores<span class="op">/</span> keys.shape[<span class="op">-</span><span class="dv">1</span>]<span class="op">**</span> <span class="fl">0.5</span>, dim <span class="op">=</span> <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attn_weights)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Third step - this looks a whole lot complex but is rather simple. If you only think about the values that tril is generating.</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>context_length <span class="op">=</span> attn_scores.shape[<span class="dv">0</span>]</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>masked_simple <span class="op">=</span> torch.tril(torch.ones(context_length,context_length))</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(masked_simple)</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Fourth step</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>row_sums <span class="op">=</span> masked_simple.<span class="bu">sum</span>(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>masked_simple_norm <span class="op">=</span> masked_simple<span class="op">/</span>row_sums</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(masked_simple_norm)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[0.1972, 0.1910, 0.1894, 0.1361, 0.1344, 0.1520],
        [0.1476, 0.2164, 0.2134, 0.1365, 0.1240, 0.1621],
        [0.1479, 0.2157, 0.2129, 0.1366, 0.1260, 0.1608],
        [0.1505, 0.1952, 0.1933, 0.1525, 0.1375, 0.1711],
        [0.1571, 0.1874, 0.1885, 0.1453, 0.1819, 0.1399],
        [0.1473, 0.2033, 0.1996, 0.1500, 0.1160, 0.1839]])
tensor([[1., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1.]])
tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],
        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000],
        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000],
        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]])</code></pre>
</div>
</div>
<p>This is a pretty neat implementation which hits our objective of causal attention model right on the head. However, there is a room for a few improvements primarily from a mathematical standpoint. 1. Use negative infinity to get rid of that extra normalization at step 4. The whole idea of step 4 was normalize all the values left after zeroing upper traingle of the matrix.When we use negative infinity with softmax the output is 0 and 2. Dropouts addition to avoid overfitting - This is a technique where randomly selected hidden layer inputs are ignored during <strong>training</strong>. This avoids overfitting by making sure that the model does not rely heavily on a specific set of hidden layer inputs. Dropouts is only used during training and not inference.</p>
<p>We can combine all of this in one go and create our final class. We will also make it work with batches.</p>
<div id="33e251ee" class="cell" data-execution_count="11">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> torch.stack((inputs,inputs), dim <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(batch.shape) <span class="co">## The shape now must be 2 * 6 * 3 which is 2 items in the batch. 6 tokens in each batch. Each token with an embedding size of 3</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CausalAttention(nn.Module):</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_in, d_out, context_length, dropout, qkv_bias<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_out <span class="op">=</span> d_out</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_query <span class="op">=</span> nn.Linear(d_in,d_out,bias <span class="op">=</span> qkv_bias)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_key <span class="op">=</span> nn.Linear(d_in,d_out,bias <span class="op">=</span> qkv_bias)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_value <span class="op">=</span> nn.Linear(d_in,d_out,bias <span class="op">=</span> qkv_bias)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout) <span class="co"># The input here indicates the percentage of values in the matrix that must be zero</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This tells pytorch that these weights are non trainable so no gradient-descent for them. The mask is saved in state_dict and is loaded/saced with the model</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>            <span class="st">'mask'</span>,</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>            torch.triu(torch.ones(context_length,context_length),diagonal<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        ) <span class="co"># Will take this in a bit</span></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>        b, num_tokens, d_in <span class="op">=</span> x.shape <span class="co"># This is the reason that we do not need to initialize the d_in. d_in is derived from an object of the class</span></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Same step as the previous loop</span></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Shape of x = 2,6,3</span></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Shape of key, query, value is 2,6,2</span></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>        keys <span class="op">=</span> <span class="va">self</span>.W_key(x) </span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>        queries <span class="op">=</span> <span class="va">self</span>.W_query(x)</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>        values <span class="op">=</span> <span class="va">self</span>.W_value(x)</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pretty much the same as the previous step. This is just doing transformations so that keys merge properly.</span></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Shape of attention scores is 2,6,6</span></span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Keys.transpose essentially means switching the 1st dimenion with the second. so the dimesion is now 2,2,6</span></span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># queries (2,6,2) @ keys.transpose(1,2) (2,2,6) ==&gt; (2,6,6)</span></span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># this is because torch considers everythin after the last two dimesnions as batch. A better way to say this would be anything the matrix multipllication here is 6,2 @ 2,6 --&gt; 6,6 and then there are two batches in each.</span></span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>        attn_scores <span class="op">=</span> queries <span class="op">@</span> keys.transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This is making sure that all the values that are masked are now </span></span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Fill elemts of the tensor with value where mask is true. The shape of the mask must be broadcastable</span></span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a>        attn_scores.masked_fill_(<span class="va">self</span>.mask.<span class="bu">bool</span>()[:num_tokens, :num_tokens],<span class="op">-</span>torch.inf)</span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This is the same step as before.</span></span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a>        attn_weights <span class="op">=</span> torch.softmax(attn_scores<span class="op">/</span>keys.shape[<span class="op">-</span><span class="dv">1</span>]<span class="op">**</span><span class="fl">0.5</span>, dim <span class="op">=</span> <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Droput we had discussed</span></span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Zeroes the values of the matrix at random. the shape of the matrix is the same</span></span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a>        attn_weights <span class="op">=</span> <span class="va">self</span>.dropout(attn_weights)</span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This the same step as before.</span></span>
<span id="cb19-50"><a href="#cb19-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># attn_weights dim = (2,6,6). values dim (2,6,2) --&gt; 2,6,2</span></span>
<span id="cb19-51"><a href="#cb19-51" aria-hidden="true" tabindex="-1"></a>        context_vec <span class="op">=</span> attn_weights <span class="op">@</span> values</span>
<span id="cb19-52"><a href="#cb19-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-53"><a href="#cb19-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> context_vec</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([2, 6, 3])</code></pre>
</div>
</div>
<p>The wraps up Causal Attention. Causal attention is the base on which the multi- headed attention is based on. Causal Attention also known as masked attention is a specialized form of self-attention. It restricts a model to only review previous and current inputs in a sequence when processing any given token when computing attention scores.</p>
</section>
<section id="translators-learnings-from-this-step-2" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="translators-learnings-from-this-step-2"><span class="header-section-number">4.2</span> Translator’s Learnings from this step</h2>
<p>Our translator is now writing a book. They’ve learned a vital rule: no spoilers. By masking the future, the translator forces themselves to generate the story one word at a time, using only what has been “said” so far. This discipline ensures the translation flows logically from start to finish, preventing the model from ‘cheating’ by looking at the end of the sentence before it has even started the beginning.</p>
<hr>
</section>
</section>
<section id="multi-headed-attention" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Multi-Headed Attention</h1>
<p>The term multi-headed attention refers to dividing the attention mechanism into multiple heads each operating indepedent. Here a single causal attention module can be considered single-head attention, where these is only one set of attention weights processing the input sequentially.</p>
<p>The main idea is to run the attention mechanism multiple times with different, learned linear projects.</p>
<div id="a254e66f" class="cell" data-execution_count="12">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadAttentionWrapper(nn.Module):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_in, d_out, context_length, dropout, num_heads, qkv_bias<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Defines the number of Causal Attention Modules</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.heads <span class="op">=</span> nn.ModuleList(</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_heads)]</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Concatenates the declaration from before</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>,x):</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.cat([head(x) <span class="cf">for</span> head <span class="kw">in</span> <span class="va">self</span>.heads], dim <span class="op">=</span> <span class="op">-</span><span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
<p>This is precisely what a multi-headed attention is.</p>
<div id="d605e9e5" class="cell" data-execution_count="13">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">123</span>)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>context_length <span class="op">=</span> batch.shape[<span class="dv">1</span>] <span class="co">#This is the number of tokens</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>d_in, d_out <span class="op">=</span> <span class="dv">3</span>,<span class="dv">2</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>mha <span class="op">=</span> MultiHeadAttentionWrapper(</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    d_in, d_out, context_length, <span class="fl">0.0</span>, num_heads<span class="op">=</span><span class="dv">2</span> </span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>context_vecs <span class="op">=</span> mha(batch)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(context_vecs)</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"contex_vecs.shape:"</span>, context_vecs.shape)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],
         [-0.5874,  0.0058,  0.5891,  0.3257],
         [-0.6300, -0.0632,  0.6202,  0.3860],
         [-0.5675, -0.0843,  0.5478,  0.3589],
         [-0.5526, -0.0981,  0.5321,  0.3428],
         [-0.5299, -0.1081,  0.5077,  0.3493]],

        [[-0.4519,  0.2216,  0.4772,  0.1063],
         [-0.5874,  0.0058,  0.5891,  0.3257],
         [-0.6300, -0.0632,  0.6202,  0.3860],
         [-0.5675, -0.0843,  0.5478,  0.3589],
         [-0.5526, -0.0981,  0.5321,  0.3428],
         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=&lt;CatBackward0&gt;)
contex_vecs.shape: torch.Size([2, 6, 4])</code></pre>
</div>
</div>
<p>The shape of the output vector is now controlled by the number of Attention heads.</p>
<p>Currently, the implementation is sequential. We need to parallelize this. We can do this with Linear Algebra. We need to concatenate these attention weights and then do matrix multiplication at once.</p>
<div id="723b39d4" class="cell" data-execution_count="14">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_in, d_out, context_length, dropout, num_heads, qkv_bias<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span>(d_out <span class="op">%</span>num_heads <span class="op">==</span> <span class="dv">0</span>), <span class="st">"d_out be divisible by the num_heads"</span> <span class="co"># Why do we need to do this?</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_out <span class="op">=</span> d_out</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head_dim <span class="op">=</span> d_out <span class="op">//</span> num_heads <span class="co"># Reduces the projection dimension to match the desired output dim</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># In mulit-headed attention, we don't actually create several seperate linear layers for each head. Instead we perform one large matrix multiplication and then slice the result into smaller for each head. If d_out is not divisible by number of heads, we would end up with fractional dimensions or uneven heads. To mitigate this,we could use padding which can be totally avoided</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_query <span class="op">=</span> nn.Linear(d_in,d_out,bias<span class="op">=</span>qkv_bias) </span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_key <span class="op">=</span> nn.Linear(d_in,d_out,bias<span class="op">=</span>qkv_bias)</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_value <span class="op">=</span> nn.Linear(d_in,d_out,bias<span class="op">=</span>qkv_bias)</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Uses a linear layer to combine individual head outputs</span></span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.out_proj <span class="op">=</span> nn.Linear(d_out, d_out)</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>            <span class="st">"mask"</span>,</span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>            torch.triu(torch.ones(context_length, context_length), diagonal<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>,x):</span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We assume number of heads(num_heads) to 4</span></span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Assuming output dimensions(d_out) to 8</span></span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Dimensions assuming: 5,6,3 &lt;==&gt; (batch, token length, embedding vector)</span></span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a>        b, num_tokens, d_in <span class="op">=</span> x.shape</span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Dimension output (5,6,3) * (3,8) --&gt; (5,6,8)</span></span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a>        keys <span class="op">=</span> <span class="va">self</span>.W_key(x)</span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a>        queries <span class="op">=</span> <span class="va">self</span>.W_query(x)</span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a>        values <span class="op">=</span> <span class="va">self</span>.W_value(x)</span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We implicitly split the matrix by adding a num_heads dimension, then we unroll the last dimension</span></span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (batch, num_tokens, d_out) --&gt; (b, num_tokens, num_heads, head_dim)</span></span>
<span id="cb24-39"><a href="#cb24-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-40"><a href="#cb24-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (5,6,8) --&gt; (5,6,4,2)        </span></span>
<span id="cb24-41"><a href="#cb24-41" aria-hidden="true" tabindex="-1"></a>        keys <span class="op">=</span> keys.view(b, num_tokens,<span class="va">self</span>.num_heads, <span class="va">self</span>.head_dim)</span>
<span id="cb24-42"><a href="#cb24-42" aria-hidden="true" tabindex="-1"></a>        values <span class="op">=</span> values.view(b, num_tokens, <span class="va">self</span>.num_heads, <span class="va">self</span>.head_dim)</span>
<span id="cb24-43"><a href="#cb24-43" aria-hidden="true" tabindex="-1"></a>        queries <span class="op">=</span> queries.view(b, num_tokens, <span class="va">self</span>.num_heads, <span class="va">self</span>.head_dim)</span>
<span id="cb24-44"><a href="#cb24-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-45"><a href="#cb24-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Transposes from shape (b, num_tokens, num_heads, head_dim) --&gt; (b, num_heads, num_tokens, head_dim)</span></span>
<span id="cb24-46"><a href="#cb24-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (5,6,4,2) --&gt; (5,4,6,2)</span></span>
<span id="cb24-47"><a href="#cb24-47" aria-hidden="true" tabindex="-1"></a>        keys <span class="op">=</span> keys.transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb24-48"><a href="#cb24-48" aria-hidden="true" tabindex="-1"></a>        queries <span class="op">=</span> queries.transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb24-49"><a href="#cb24-49" aria-hidden="true" tabindex="-1"></a>        values <span class="op">=</span> values.transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb24-50"><a href="#cb24-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-51"><a href="#cb24-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Can we not combine view and step to one step, view should be able to do it?</span></span>
<span id="cb24-52"><a href="#cb24-52" aria-hidden="true" tabindex="-1"></a>        <span class="co">#  View changes how we interpret the memory. In this case it asking the interpretation of 8 vector of dim 1 to 4 rows and 1 column.</span></span>
<span id="cb24-53"><a href="#cb24-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Transpose changes the physical order. When we move the num_heads dimesnio, we are effectively asking to group all tokens for a single head together</span></span>
<span id="cb24-54"><a href="#cb24-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-55"><a href="#cb24-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Computes dot product for each head</span></span>
<span id="cb24-56"><a href="#cb24-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (5,4,6,2) @ (5,4,2, 6) --&gt; 5,4,6,6</span></span>
<span id="cb24-57"><a href="#cb24-57" aria-hidden="true" tabindex="-1"></a>        attn_scores <span class="op">=</span> queries <span class="op">@</span> keys.transpose(<span class="dv">2</span>,<span class="dv">3</span>)</span>
<span id="cb24-58"><a href="#cb24-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-59"><a href="#cb24-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Masks truncated to the number of tokens</span></span>
<span id="cb24-60"><a href="#cb24-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (6,6)</span></span>
<span id="cb24-61"><a href="#cb24-61" aria-hidden="true" tabindex="-1"></a>        mask_bool <span class="op">=</span> <span class="va">self</span>.mask[:num_tokens,:num_tokens].<span class="bu">bool</span>()</span>
<span id="cb24-62"><a href="#cb24-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-63"><a href="#cb24-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># uses masks to fill the attention scores</span></span>
<span id="cb24-64"><a href="#cb24-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This will broadcast 6 ,6 across </span></span>
<span id="cb24-65"><a href="#cb24-65" aria-hidden="true" tabindex="-1"></a>        attn_scores.masked_fill_(mask_bool, <span class="op">-</span>torch.inf)</span>
<span id="cb24-66"><a href="#cb24-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-67"><a href="#cb24-67" aria-hidden="true" tabindex="-1"></a>        attn_weights <span class="op">=</span> torch.softmax(attn_scores <span class="op">/</span> keys.shape[<span class="op">-</span><span class="dv">1</span>]<span class="op">**</span><span class="fl">0.5</span>, dim <span class="op">=</span> <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb24-68"><a href="#cb24-68" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb24-69"><a href="#cb24-69" aria-hidden="true" tabindex="-1"></a>        attn_weights <span class="op">=</span> <span class="va">self</span>.dropout(attn_weights)</span>
<span id="cb24-70"><a href="#cb24-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-71"><a href="#cb24-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-72"><a href="#cb24-72" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (5,4,6,6) @ (5,4,6,2) --&gt; (5,4,6,2).T --&gt; (5,6,4,2)</span></span>
<span id="cb24-73"><a href="#cb24-73" aria-hidden="true" tabindex="-1"></a>        context_vec <span class="op">=</span> (attn_weights <span class="op">@</span> values).transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb24-74"><a href="#cb24-74" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Combines heads where self.d_out = self.num_heads * self.head_dim</span></span>
<span id="cb24-75"><a href="#cb24-75" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (5,6,4,2) --&gt; (5,6,8) We are doing the unroll here</span></span>
<span id="cb24-76"><a href="#cb24-76" aria-hidden="true" tabindex="-1"></a>        <span class="co"># here contiguous().view() is asking to do the same thing as before but instead of the previous 8 --&gt; 4,2, here it is asking to interpret this as 4,2 to 8</span></span>
<span id="cb24-77"><a href="#cb24-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># After the transpose the data in memory is not longer lined up in a single row ergo not contiguous</span></span>
<span id="cb24-78"><a href="#cb24-78" aria-hidden="true" tabindex="-1"></a>        <span class="co"># View has the requirement for the data to be in a contiguous block to work</span></span>
<span id="cb24-79"><a href="#cb24-79" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calling .contiguis shuffles the physical memory to match the transposed shape so that it can flatten out.</span></span>
<span id="cb24-80"><a href="#cb24-80" aria-hidden="true" tabindex="-1"></a>        context_vec <span class="op">=</span> context_vec.contiguous().view(b, num_tokens, <span class="va">self</span>.d_out)</span>
<span id="cb24-81"><a href="#cb24-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-82"><a href="#cb24-82" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Adding an optional linear projection. This is not strictly necessary but is commonly used in many LLM Architectures</span></span>
<span id="cb24-83"><a href="#cb24-83" aria-hidden="true" tabindex="-1"></a>        context_vec <span class="op">=</span> <span class="va">self</span>.out_proj(context_vec)</span>
<span id="cb24-84"><a href="#cb24-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-85"><a href="#cb24-85" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> context_vec</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
<p>This completes the whole chapter of Coding Attention Mechanism</p>
<hr>
</section>
<section id="summarizing-thoughts" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Summarizing thoughts</h1>
<p>Multi-headed attention is the engine which powers the Transformers. Multi-Headed Attention is like a committtee of experts reading the same sentence. Each expert looks for different structures within the sentence simultaneously. Single headed focusses on synactic relationship. By projecting the input into different subspaces the model can capture a more rich “context vector” than a single attention pass could.</p>
<p>We start by taking <strong>linear Projections</strong> with our input over <em>Q, K , V</em> matrices. In MHA we split these into N distinct heads. Each head performs Scaled Dot Product Attention independently. This is where each expert learns in different subspaces in parallel. Because the each head’s calculation is independent, it is highly efficient to compute on GPUs. We finally stitch the results of all heads back together and pass them through the final linear layer <span class="math inline">\(W_{o}\)</span>. This is re-integration the findings of our committee of experts into a single representation. The final linear layer is important as it glues different experts perscpectives together. The model now learns and weighs these experts into the final output</p>
<section id="going-back-to-translator" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="going-back-to-translator"><span class="header-section-number">6.1</span> Going back to translator</h2>
<p>Finally, our single translator has become a full editorial team. One expert focuses on the tense of the verbs, another tracks the gender of the pronouns, and a third looks for poetic metaphors. By working in parallel, they catch nuances that a single mind would miss. They then sit down together (the final linear projection) to merge their individual notes into one perfect, polished masterpiece.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>