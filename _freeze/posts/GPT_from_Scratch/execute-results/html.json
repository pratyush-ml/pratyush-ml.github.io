{
  "hash": "8588909bbc6c024fb0b3e49e54de87e1",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Implementing a GPT model from scratch to generate text\"\nsubtitle: \"Learning Notes from LLMs from Scratch\"\nauthor: \"Pratyush Sinha\"\ndate: last-modified\ncategories: [Learning, Python, Book-Notes]\ndescription: \"A summary of my learnings and code implementations from Chapter 04 of the book.\"\n\n# This section connects Quarto to your uv environment\njupyter: python3 \n\nexecute:\n  echo: true          # Shows your code in the output\n  warning: false      # Keeps the document clean from library warnings\n  freeze: auto        # Only re-renders when you change the file\n  cache: true         # Speeds up rendering for code-heavy notes\n\nformat:\n  html:\n    toc: true         # Adds a Table of Contents on the right\n    toc-depth: 3      # How many heading levels to show in TOC\n    code-fold: show   # Allows readers to toggle code visibility (starts as shown)\n    code-copy: true   # Adds a copy button to your code blocks\n    number-sections: true\n---\n\nOne idea that has stuck with me while working with these Transformers is that they are algorithms with many moving components. To understand why a particular component exists, we need to understand the experimentation process behind it. It is imperative that we understand the moving parts involved in the algorithm first; we can worry about the experimentation bit later. \n\nThink of it from a sorting algorithm perspective: there are many sorting algorithms out there. We gradually evolved from one kind of sorting algorithm to another through rapid experimentation. We could think of Transformers in the same way—we started out with translation tasks using architectures like RNNs and gradually shifted to Transformers, which represent a more advanced form of sequence processing. If we spend too much time on the history of every tiny component right now, we would be in a constant \"experimentation phase.\" I think to learn Transformers, you don't necessarily need that history right now. That is something we can take up once we've gone through the entire exercise of creating a Transformer from scratch.\n\n\nA quick summary of what we have learned so far:\nLarge Language Models are large deep learning neural networks. Transformers are the core blocks on which LLMs are built. The core idea behind Transformers is multi-headed attention.\n\nIn the previous section, we worked on our understanding of Attention models. We learned how Multi-Headed Attention is a combination of self-attention and causal attention. We also learned how multi-headed attention is the engine of the Transformer models. In this section, we are going to build on that and implement the remaining components. The end objective of this chapter is to develop a working model which takes input text and returns output text based on the Transformer architecture.\n\nTo start, let's define all the components of the Transformer to clearly see what other components require our attention.\n\n![Transformers Block](link)\n\n```{mermaid}\ngraph TD\n    %% Input Section\n    Input([\"Every effort moves you\"]) --> Tokenized[Tokenized text]\n    \n    subgraph GPT [\"GPT model\"]\n        direction TB\n        \n        %% Initial Embeddings\n        Tokenized --> TE[Token embedding layer]\n        TE --> PE[Positional embedding layer]\n        PE --> Drop1[Dropout]\n\n        %% Transformer Block Subgraph\n        subgraph TransformerBlock [\"Transformer block (Repeated 12 times)\"]\n            direction TB\n            \n            %% First Residual Connection\n            Drop1 --> LN1[LayerNorm 1]\n            LN1 --> MHA[Masked multi-head attention]\n            MHA --> Drop2[Dropout]\n            Drop2 --> Add1((+))\n            \n            %% Residual Line 1\n            Drop1 -- \"Residual\" --- Add1\n\n            %% Second Residual Connection\n            Add1 --> LN2[LayerNorm 2]\n            LN2 --> FF[Feed forward]\n            FF --> Drop3[Dropout]\n            Drop3 --> Add2((+))\n            \n            %% Residual Line 2\n            Add1 -- \"Residual\" --- Add2\n        end\n\n        %% Final Output Layers\n        Add2 --> FLN[Final LayerNorm]\n        FLN --> Linear[Linear output layer]\n    end\n\n    %% Output Tensor\n    Linear --> Tensor[\"<b>A 4 x 50,257-dimensional tensor</b><br/>[[ -0.0055, ..., -0.4747],<br/>[ 0.2663, ..., -0.4224],<br/>[ 1.1146, ..., 0.0276],<br/>[ -0.8239, ..., -0.3993]]\"]\n\n    %% Explanatory Note\n    Tensor -.-> Goal[\"<b>Goal:</b> Convert back to text<br/>(Last row represents 'forward')\"]\n\n    %% Styling\n    classDef blueFill fill:#88ccee,stroke:#333,stroke-width:1px;\n    classDef greyFill fill:#e0e0e0,stroke:#333,stroke-width:1px;\n    classDef darkFill fill:#555,color:#fff,stroke:#333,stroke-width:1px;\n    \n    class TransformerBlock blueFill;\n    class GPT,Tokenized,TE,PE,Drop1,LN1,LN2,FF,Drop2,Drop3,FLN,Linear,Tensor greyFill;\n    class MHA darkFill;\n\n```\n\nWith this block, we get a clear picture of all the components needed to build a GPT-2 model. We will build this model with a backbone which lays out all the components of the model. We then fill this backbone with all the relevant code.\n\n```{mermaid}\ngraph LR\n    subgraph Core [\"Core Components\"]\n        direction TB\n        B2[\"2 Layer normalization\"]\n        B3[\"3 GELU activation\"]\n        B4[\"4 Feed forward network\"]\n        B5[\"5 Shortcut connections\"]\n    end\n\n    subgraph Integration\n        B6[\"6 Transformer block\"]\n        B7[\"7 Final GPT architecture\"]\n    end\n\n    subgraph Initial [\"Initial Phase\"]\n        B1[\"1 GPT backbone\"]\n    end\n\n    %% Connections\n    B2 --> B6\n    B3 --> B6\n    B4 --> B6\n    B5 --> B6\n    B6 --> B7\n\n    %% Styling\n    style Core fill:none,stroke:#ccc\n    style Integration fill:#e1f5fe,stroke:#01579b\n    style Initial fill:none,stroke:#ccc\n\n```\n\nBefore we start building, we need to define the arguments required for these components and a full-fledged Transformer block to run.\n\n## Defining the configuration for GPT-2 class of models\n\n::: {#734eef38 .cell execution_count=1}\n``` {.python .cell-code}\nGPT_CONFIG_124M = {\n    \"vocab_size\": 50257, # Vocabulary_Size\n    \"context_length\": 1024, # Context Length\n    \"emb_dim\": 768, # Embedding dimension\n    \"n_heads\": 12, # Number of Attention Heads \n    \"n_layers\": 12, # Number of layers\n    \"drop_rate\": 0.1, # DropOut Rate\n    \"qkv_bias\": False  # Query-Key-Value Bias\n}\n```\n:::\n\n\n**Defining these variables:**\n\n* **vocab_size**: This refers to the vocabulary of 50,257 words as per the tokenizer (BPE).\n* **context_length**: The maximum number of input tokens the model can handle via positional embeddings.\n* **emb_dim**: The dimensionality of each token vector projection.\n* **n_heads**: The number of attention heads in the multi-head attention mechanism.\n* **n_layers**: The number of Transformer blocks in the model.\n* **drop_rate**: The intensity of the dropout mechanism to prevent overfitting.\n* **qkv_bias**: This determines the bias vector of the linear layers in the multi-head attention. We disabled this in the previous chapter.\n\n# Building the GPT Backbone\n\nThis backbone gives an overall structure to the model.\n\n::: {#36da52cf .cell execution_count=2}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\n\nclass DummyGPTModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg['emb_dim'])\n        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg['emb_dim'])\n        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n        self.trf_blocks = nn.Sequential(\n            *[DummyTransformerBlock(cfg)\n            for _ in range(cfg[\"n_layers\"])]\n        )\n        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n        self.out_head = nn.Linear(\n            cfg[\"emb_dim\"],cfg[\"vocab_size\"],bias = False\n        )\n    \n    def forward(self,in_idx):\n        batch_size, seq_len = in_idx.shape\n        tok_embeds = self.tok_emb(in_idx)\n        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n\n        x = tok_embeds + pos_embeds\n        x = self.drop_emb(x)\n        x = self.trf_blocks(x)\n        x = self.final_norm(x)\n        logits = self.out_head(x)\n        return logits \n\nclass DummyTransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n\n    def forward(self, x):\n        return x\n\nclass DummyLayerNorm(nn.Module):\n    def __init__(self, normalized_shape, eps = 1e-5):\n        super().__init__()\n\n    def forward(self, x):\n        return x\n```\n:::\n\n\nThere are three classes we have defined here. These form the backbone of the GPT model and are used multiple times.\n\n::: {#776d429c .cell execution_count=3}\n``` {.python .cell-code}\nimport tiktoken\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")\nbatch = []\ntxt1 = \"Every effort moves you\"\ntxt2 = \"Every day holds a\"\n\nbatch.append(torch.tensor(tokenizer.encode(txt1)))\nbatch.append(torch.tensor(tokenizer.encode(txt2)))\nbatch = torch.stack(batch, dim=0)\nprint(batch)\n\ntorch.manual_seed(123)\nmodel = DummyGPTModel(GPT_CONFIG_124M)\nlogits = model(batch)\nprint(\"Output shape:\", logits.shape)\nprint(logits)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[6109, 3626, 6100,  345],\n        [6109, 1110, 6622,  257]])\nOutput shape: torch.Size([2, 4, 50257])\ntensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n\n        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n       grad_fn=<UnsafeViewBackward0>)\n```\n:::\n:::\n\n\nThe output tensor has two rows corresponding to the two text samples. There are 4 tokens for each text, and finally, there are 50,257-dimensional vectors.\n\n# Normalizing Activations with Layer Normalization\n\nWhy do we need to normalize in the first place? The answer is usually the same:\n\n> Training Neural Networks with many layers can be challenging due to problems like vanishing or exploding gradients. These problems lead to unstable dynamics and make it difficult for the network to effectively adjust its weights, meaning the learning process struggles to find parameters that minimize the loss function.\n\nThis explanation repeats in almost every book I read. Let's dive deeper into it. The idea of vanishing or exploding gradients leading to unstable dynamics is easy to digest—this is Calculus 101. If the gradient (the slope) is too high, the algorithm has a hard time stepping toward the minima and may not land on a stable point. Conversely, if the slope is too small, the step is too tiny for significant change, and it takes too long to reach the minima.\n\nMy main question was: how does training lead to this?\n\nNeural Networks are directed computation graphs.\nInput ---> Computation Graph ---> Output\n\nThe main job of the network is to use weights to minimize an error metric. These weights are optimized during training. Because the graph is directional, computing the gradient for the first layer requires the chain rule. If there are ten layers, the first layer's gradient involves a product of ten differentials. If these differentials are less than 1 (as with many activation functions), multiplying them ten times results in a tiny number—hence, vanishing gradients.\n\nThe same applies to exploding gradients: large gradients multiplied repeatedly don't just grow linearly; they compound exponentially like interest.\n\nOne strategy to improve stability is Layer Normalization. It adjusts the activations of a layer to have a mean of 0 and a variance of 1. This adjustment speeds up the convergence to effective weights and ensures reliable training. In GPT-2, layer normalization is applied before and after the multi-head attention module.\n\nLet's see how layer normalization works with a small tensor before filling in our class.\n\n::: {#0409d4dc .cell execution_count=4}\n``` {.python .cell-code}\ntorch.manual_seed(123)\nbatch_example = torch.randn(2,5)\n\nlayer = nn.Sequential(nn.Linear(5,6), nn.ReLU()) \nout = layer(batch_example)\n\nmean = out.mean(dim=-1, keepdim=True)\nvar = out.var(dim=-1, keepdim=True)\nprint(\"Mean: \", mean)\nprint(\"Variance: \", var)\n\n# After normalization\nout_norm = (out-mean)/torch.sqrt(var)\nmean = out_norm.mean(dim=-1,keepdim=True)\nvar = out_norm.var(dim = -1, keepdim=True)\n\nprint(\"Normalized Layer Outputs:\", out_norm)\nprint(\"Mean:\", mean) \nprint(\"Variance:\", var)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean:  tensor([[0.1324],\n        [0.2170]], grad_fn=<MeanBackward1>)\nVariance:  tensor([[0.0231],\n        [0.0398]], grad_fn=<VarBackward0>)\nNormalized Layer Outputs: tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n       grad_fn=<DivBackward0>)\nMean: tensor([[-5.9605e-08],\n        [ 1.9868e-08]], grad_fn=<MeanBackward1>)\nVariance: tensor([[1.0000],\n        [1.0000]], grad_fn=<VarBackward0>)\n```\n:::\n:::\n\n\nNow we create the class.\n\n::: {#b61eb031 .cell execution_count=5}\n``` {.python .cell-code}\nclass LayerNorm(nn.Module):\n    def __init__(self, emb_dim):\n        super().__init__()\n        self.eps = 1e-5 \n        self.scale = nn.Parameter(torch.ones(emb_dim)) \n        self.shift = nn.Parameter(torch.zeros(emb_dim))\n\n    def forward(self,x):\n        mean = x.mean(dim=-1, keepdim=True)\n        var = x.var(dim=-1, keepdim=True, unbiased=False) \n        norm_x = (x-mean)/torch.sqrt(var + self.eps)\n        return self.scale*norm_x + self.shift\n```\n:::\n\n\n# Implementing a Feed-Forward Network with GELU Activations\n\nGELU stands for Gaussian Error Linear Unit. It looks similar to ReLU (), but mathematically, it is the product of  and the cumulative distribution function (CDF) of a standard normal distribution:\n\n\n\nGELU is like a \"confidence booster.\" If  is high, confidence is high, and the CDF multiplies it by nearly 1. If  is low, the model is less confident, and the CDF multiplies it by a smaller number.\n\n::: {#9b781b7a .cell execution_count=6}\n``` {.python .cell-code}\nclass GELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n    \n    def forward(self,x):\n        return 0.5 * x * (1 + torch.tanh(\n            torch.sqrt(torch.tensor(2/torch.pi))* (x + 0.044715 * torch.pow(x,3))\n        ))\n```\n:::\n\n\nNow we implement the feed-forward neural network.\n\n::: {#ea8bc343 .cell execution_count=7}\n``` {.python .cell-code}\nclass FeedForward(nn.Module):\n    def __init__(self,cfg):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(cfg[\"emb_dim\"],4*cfg[\"emb_dim\"]),\n            GELU(),\n            nn.Linear(4*cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n        )\n\n    def forward(self,x):\n        return self.layers(x)\n```\n:::\n\n\n# Adding Shortcut Connections\n\nShortcut (or Skip/Residual) connections mitigate vanishing gradients. They create an alternative path for the gradient to flow by skipping one or more layers, adding the output of one layer to the output of a later layer.\n\n::: {#66bab67b .cell execution_count=8}\n``` {.python .cell-code}\nclass ExampleDeepNeuralNetwork(nn.Module):\n    def __init__(self, layer_sizes, use_shortcut):\n        super().__init__()\n        self.use_shortcut = use_shortcut\n        self.layers = nn.ModuleList([\n            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n        ])\n\n    def forward(self,x):\n        for layer in self.layers:\n            layer_output = layer(x)\n            if self.use_shortcut and x.shape == layer_output.shape:\n                x = x + layer_output\n            else:\n                x = layer_output\n        return x\n```\n:::\n\n\n# Connecting Attention & Linear Layers in Transformer Blocks\n\nWe have now captured all the components of the GPT-2 architecture. Let's fill in the actual `TransformerBlock`:\n\n::: {#9cd3382e .cell execution_count=9}\n``` {.python .cell-code}\nfrom attention import MultiHeadAttention \n\nclass TransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.att = MultiHeadAttention(\n            d_in = cfg[\"emb_dim\"],\n            d_out = cfg[\"emb_dim\"],\n            context_length = cfg[\"context_length\"],\n            num_heads = cfg[\"n_heads\"],\n            dropout = cfg['drop_rate'],\n            qkv_bias = cfg[\"qkv_bias\"]\n        ) \n\n        self.ff = FeedForward(cfg) \n        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n\n    def forward(self,x):\n        # MHA section\n        shortcut = x\n        x = self.norm1(x) \n        x = self.att(x) \n        x = self.drop_shortcut(x) \n        x = x + shortcut \n        \n        # Feedforward section\n        shortcut = x \n        x = self.norm2(x) \n        x = self.ff(x) \n        x = self.drop_shortcut(x)\n        x = x + shortcut \n        return x\n```\n:::\n\n\n# Coding the GPT Model\n\nTime to stitch everything together.\n\n::: {#dd2d5107 .cell execution_count=10}\n``` {.python .cell-code}\nclass GPTModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"],cfg[\"emb_dim\"])\n        self.pos_emb = nn.Embedding(cfg[\"context_length\"],cfg[\"emb_dim\"])\n        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n\n        self.trf_blocks = nn.Sequential(\n            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n        )\n        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n        self.out_head = nn.Linear(\n            cfg[\"emb_dim\"],cfg[\"vocab_size\"],bias=False\n        )\n\n    def forward(self,in_idx):\n        batch_size, seq_len = in_idx.shape\n        tok_embeds = self.tok_emb(in_idx)\n        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n        x = tok_embeds + pos_embeds\n        x = self.drop_emb(x)\n        x = self.trf_blocks(x)\n        x = self.final_norm(x)\n        logits = self.out_head(x)\n        return logits\n```\n:::\n\n\n# Generating Text\n\nThe model converts input text to an output tensor. We now need to generate text by selecting tokens based on probability distributions.\n\n::: {#c0a41af3 .cell execution_count=11}\n``` {.python .cell-code}\ndef generate_text_simple(model,idx,max_new_tokens,context_size): \n    for _ in range(max_new_tokens):\n        idx_cond = idx[:,-context_size:] \n        with torch.no_grad():\n            logits = model(idx_cond) \n        \n        logits = logits[:,-1,:] \n        probas = torch.softmax(logits, dim=-1)\n        idx_next = torch.argmax(probas, dim=-1, keepdim=True) \n        idx = torch.cat((idx,idx_next), dim=1) \n\n    return idx\n```\n:::\n\n\n# Summarizing Thoughts\n\nIn this chapter, we implemented the full end-to-end architecture of a GPT-2 model. As we’ve seen, GPT has multiple moving parts from an architectural standpoint that must work in harmony.\n\nWe started by defining a configuration file that serves as the blueprint. From there, we built the Transformer block, which houses the two primary engines: **Multi-Head Attention** and **Feed-Forward networks**. We also explored how deep networks are prone to vanishing or exploding gradients and saw how techniques like **Shortcut Connections**, **Layer Normalization**, and **GELU activations** work in tandem to stop this phenomenon. Finally, we learned to decode the output text using the tokenizer.\n\n## Perhaps in the next blog, we can go top-down on the GPT-2 architecture or dive into one of the foundational papers.\n\n",
    "supporting": [
      "GPT_from_Scratch_files"
    ],
    "filters": [],
    "includes": {}
  }
}