{
  "hash": "dafcd9ccbb725433f32131abd9734291",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Chapter 03: Coding Attention Mechanisms\"\nsubtitle: \"Learning Notes from LLMs from Scratch\"\nauthor: \"Pratyush Sinha\"\ndate: last-modified\ncategories: [Learning, Python, Book-Notes]\ndescription: \"A summary of my learnings and code implementations from Chapter 03 of the book.\"\n\n# This section connects Quarto to your uv environment\njupyter: python3 \n\nexecute:\n  echo: true          # Shows your code in the output\n  warning: false      # Keeps the document clean from library warnings\n  freeze: auto        # Only re-renders when you change the file\n  cache: true         # Speeds up rendering for code-heavy notes\n\nformat:\n  html:\n    toc: true         # Adds a Table of Contents on the right\n    toc-depth: 3      # How many heading levels to show in TOC\n    code-fold: show   # Allows readers to toggle code visibility (starts as shown)\n    code-copy: true   # Adds a copy button to your code blocks\n    number-sections: true\n---\n\nThis chapter deals with attention mechanisms. At a very high level, the flow of the chapters is as follows:\n\n\n* **Simplified Self-Attention:** Introduces the broad idea.\n* **Self-Attention:** Trainable weights that form the basis of the mechanism used in LLMs.\n* **Causal Attention:** A type of self-attention that only considers previous and current inputs in the sequence, ensuring the temporal order of text generation.\n* **Multi-Head Attention:** An extension of self-attention and causal attention that enables models to simultaneously attend to information from different representation spaces.\n\n\n---\n\n\n# Naive Approach: Word-to-Word Translation\n\nImagine a translator who only has a pocket dictionary but no understanding of grammar. To translate \"The early bird catches the worm,\" they simply swap each word for its equivalent in the target language.\n\nThe immediate problem is that words cannot always be translated using a simple one-to-one replacement; a single word in one language might require three in another. The second problem is structural: some languages arrange elements differently (e.g., Subject-Verb-Object vs. Subject-Object-Verb). Our dictionary-wielding translator would produce a \"word salad\" that lacks any coherent flow or meaning.\n\n## Better but slightly more compute-constrained approach\n\nNaturally, we should follow how human translators work: take input in the language, synthesize it, and then return the output in the target language. In a computational sense, this is what **Encoder-Decoders** are.\nThe **Encoder** acts like the translator listening to the source sentence and forming a mental \"summary\" of the idea. This summary—technically called a **context vector**—is then passed to the **Decoder**, which begins speaking the sentence in the new language. Before Transformers, **Recurrent Neural Networks (RNNs)** were the standard norm. RNNs are well-suited for sequential data because they process information one step at a time, feeding the output of the previous word into the current step to maintain a sense of history.\n\n## Encoder-Decoder RNNs trouble\nOnce we move to encoder-decoder RNNs, the challenges are two-fold. As the input text is fed into the encoder, it processes it sequentially and updates its **hidden state**. It captures the entire meaning of the sentence in the final hidden state. The decoder then takes this \"bottleneck\" state to start generating the translation.\n\nThe big limitation is that RNNs can’t directly access earlier hidden states from the encoder during decoding. This leads to **lost context**.\n\n> **The Translator's Struggle:** Imagine our translator is listening to a 50-word sentence. By the time the speaker finishes, the translator has a general \"vibe\" of the sentence, but they’ve likely forgotten the specific adjectives used at the very beginning. Because the RNN forces the entire meaning into one single vector, the \"signal\" from the start of the sequence washes out by the time it reaches the end.\n\n## The Solution: Bahdanau Attention\n\nThe current solution is the **Attention Mechanism**, which captures these data dependencies. Instead of forcing the translator to rely on a single, fading memory of the whole sentence, the **Bahdanau Attention** mechanism allows the translator to \"re-read\" specific parts of the source text as they generate each word.\n\nWhen the decoder is about to produce a word, it looks back at all the encoder's hidden states and assigns a \"weight\" to them. If the translator is currently translating a noun, the attention mechanism tells them to \"focus\" more on the original noun and its modifiers in the source sentence, rather than the distant verbs.\n\nMathematically, the context vector  for each decoding step  is a weighted sum of all encoder hidden states :\n$$c_t = \\sum_{i=1}^{T} \\alpha_{ti} h_i$$\nWhere $\\alpha_{ti}$ represents the \"attention score,\" or how much focus the translator is giving to word  while producing word . This ensures that no matter how long the sequence is, the important details are never truly lost.\n\n---\n\n**Let's now jump on how the Attention mechanism works**\nAttention mechanism is like the engine that drives the entire transformer block. Once we get a hang of the attention mechanism, the rest of the parts are relatively easy to fill in.\n\n# Attending Different Parts  of the Input with Self Attention\n**Simplified Self-Attention:** In this section we aim to create create context vectors.\nContext Vectors are just a glorified embeddings vector or in other words an enriched embedding vector.\n\nA simplified self attention would have the following three steps:\n\n> The first three steps are being implemented for each token.\n\n1. Calculate the intermediate weight W which is the dot product on the embedding vector for each token.\n2. Normalize this resultant weights matrix across all the tokens\n3. Finally we do a weighted sum of the embeddings vector based on the weight.\n4. Scale to final all tokens\n\n\n*This looks awfully similar to the idea of relative position. Why do we need context vectors when we have relation position encodoing before creating the embeddings vector?*\n\n> There is a fundamental difference in their purposes:\n\n> Position Encodings: Tell the model where a word is in a sentence (e.g., \"This word is at index 2\"). It provides the \"coordinates.\"\n\n> Context Vectors (Self-Attention): Tell the model what the word means in relation to others. Even if the model knows a word is at index 2, it doesn't know if that word is a verb acting on the noun at index 5 until self-attention runs.\n\n> Crucial Distinction: Position encodings are additive data (spatial info); Context vectors are calculated relationships (semantic info).\n\n## Step 1\nGenerating intermediate weight W which is the dot product on the embedding vector for each token.\n\n::: {#1207f60e .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\n\n# Declaring an input token of size 6 with embedding size of 3\ninputs = torch.tensor(\n    [[0.43, 0.15, 0.89], # Your     (x^1)\n   [0.55, 0.87, 0.66], # journey  (x^2)\n   [0.57, 0.85, 0.64], # starts   (x^3)\n   [0.22, 0.58, 0.33], # with     (x^4)\n   [0.77, 0.25, 0.10], # one      (x^5)\n   [0.05, 0.80, 0.55]] # step     (x^6)\n)\n\n# For demonstration purposes, we select one input token\nquery = inputs[1]\n\n# \nattn_scores_2 = torch.empty(inputs.shape[0])\n# print(attn_scores_2)\n\n# Calculating step 1. This results in a vector of the same size as the context length\n# Context length here means the number of tokens\nfor i , x_i in enumerate(inputs):\n    attn_scores_2[i] = torch.dot(x_i, query)\n\n\nprint(f\"Attention score for second word i.e Journey {attn_scores_2}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAttention score for second word i.e Journey tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n```\n:::\n:::\n\n\nJust for more clarity, the dimension is equal to the number of tokens because the embeddding vector of the token 'journey' now has a dot product with all the tokens in the input text.\n\n## Step 2:\nNormalizing the intermediate weights now. The weight should sum up on a scale of 0 to 1. The easiest way to do this is using softmax layer.\n\n::: {#e57aa626 .cell execution_count=2}\n``` {.python .cell-code}\nattn_weights_2 = torch.softmax(attn_scores_2, dim =0)\nprint(f\"This is normalizing the vector of immediate weights {attn_weights_2}\")\nprint(f\"Sum should be 1: {attn_weights_2.sum()}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThis is normalizing the vector of immediate weights tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\nSum should be 1: 1.0\n```\n:::\n:::\n\n\n## Step 3:\nA weighted sum of the embeddings vector and the normalized attention weight.\n\n::: {#91056fdf .cell execution_count=3}\n``` {.python .cell-code}\nquery = inputs[1]\ncontext_vec_2 = torch.zeros(query.shape)\nfor i, x_i in enumerate(inputs):\n    context_vec_2 += attn_weights_2[i]*x_i\nprint(f\"The final attention weight: {context_vec_2}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe final attention weight: tensor([0.4419, 0.6515, 0.5683])\n```\n:::\n:::\n\n\nThe resultant vector for one token is the same as the input token embeddings size\nFrom a code perspective, this is a relatively poor. But for demonstration purposes, this will do. \n\nWhen reviewing the code, an obvious question is the difference between torch.zeros vs torch.empty. THe way are pretty much the same.\n\n> Torch.zero initializes matrix of all zeros. The memory allocation for this is done. Torch.empty allocates memory but does not initialize it. This means that we should not use it with a sum operation. This means that if we print torch.empty it will return whatever is present in the memory allocation from before. The reason we use it is because it is slightly faster.\n\n\n\n## Step 4:\nSo far, we have only done this for one token. Let's extend this to the whole list of input tokens.\n\n::: {#c97bb160 .cell execution_count=4}\n``` {.python .cell-code}\nattn_scores = torch.empty(inputs.shape)\nprint(attn_scores)\nfor i, x_i in enumerate(inputs):\n    for j , x_j in enumerate(inputs):\n        attn_scores[i] = torch.dot(x_i, x_j)\nprint(f\"Attention score for all words {attn_scores}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\nAttention score for all words tensor([[0.6310, 0.6310, 0.6310],\n        [1.0865, 1.0865, 1.0865],\n        [1.0605, 1.0605, 1.0605],\n        [0.6565, 0.6565, 0.6565],\n        [0.2935, 0.2935, 0.2935],\n        [0.9450, 0.9450, 0.9450]])\n```\n:::\n:::\n\n\nA slightly advanced approach leveraging linear algebra is:\n\n::: {#30494de5 .cell execution_count=5}\n``` {.python .cell-code}\n# Step 1\nattn_scores = inputs @ inputs.T \n\n# Step 2\nattn_weights = torch.softmax(attn_scores, dim =-1) # Do it on the last dimension of the vector, which in this case is equal to token size.\n\n# Step 3\n\nall_context_vectors = attn_weights @ inputs\nprint(all_context_vectors)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[0.4421, 0.5931, 0.5790],\n        [0.4419, 0.6515, 0.5683],\n        [0.4431, 0.6496, 0.5671],\n        [0.4304, 0.6298, 0.5510],\n        [0.4671, 0.5910, 0.5266],\n        [0.4177, 0.6503, 0.5645]])\n```\n:::\n:::\n\n\nTo summarize iteration 1, we did a watered down version of self attention. We took one token as an input, dot product across all tokens embeddings, normalize the resultant weights across token length and finally we do a sum proudct of this normalized weight with each embedding vector. We repeat the same exercise for all tokens.\nFew important things to consider for the scaled version.\nattn_scores has dimensions: token_length * token_length\nattn_weights: token_length * token_length\nall_context_vectors: token_length * embedding_size\n\n## Translator's Learnings from this step\n\nAt this stage, our translator has stopped using a word-for-word dictionary. They’ve realized that words are defined by their neighbors. When they see the word \"bank,\" they now look at the rest of the sentence to see if there are words like \"river\" or \"money.\" They are building a rough 'vibe' of the sentence, but they're still using a fixed set of rules and can't yet adapt their focus based on the specific task at hand.\n\n---\n\n\n# Implementing self attention with trainable weights.\nTrainable weights that form the basis of the mechanism used in LLMs.\nThe previous iteration uses matrix multiplications on its own embeddings. This iteration introduces a new class of trainable weights matrices $W_{q}$, $W_{k}$, $W_{v}$. These weights are randomly initialized and are used to project the input tokens into query, key, value matrices.\n\nWe follow the same idea of the previous step, start with one token and then generalize for all tokens.\n\n::: {#f32be53c .cell execution_count=6}\n``` {.python .cell-code}\n# Picking one token\nx_2 = inputs[1]\nd_in = inputs.shape[1] # Picking the embedding size. Here this value is 3\nd_out = 2 # This is output size of the matrix\n\n# We have initialized the random weights of the same size as the token embeddings and arbitary number 2.\n\ntorch.manual_seed(123)\n# All these 3 matrices are of the same size 3 * 2\nW_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\nW_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\nW_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n\n# Multiplying the input token of size 1 * 3 with 3 * 2\n# The result is a matrix of size 1 * 2\nquery_2 = x_2 @ W_query \nkey_2 = x_2 @ W_key\nvalue_2 = x_2 @ W_value\nprint(query_2)\n\n\n# These matrices are updated here but they could very well be used for one token\nkeys = inputs @ W_key\nvalues = inputs @ W_value\nprint(\"keys.shape:\", keys.shape)\nprint(\"values.shape:\", values.shape)\n\n\nkeys_2 = keys[1]\nattn_scores_22 = query_2.dot(keys_2)\nprint(attn_scores_22)\n\nattn_scores_2 = query_2 @ keys.T\nprint(attn_scores_2)\n\nd_k = keys.shape[-1]\nattn_weights_2 = torch.softmax(attn_scores_2 / d_k ** 0.5, dim =-1) # Why do we use d_k ** 0.5?\nprint(attn_weights_2)\n\ncontext_vec_2 = attn_weights_2 @ values\nprint(context_vec_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([0.4306, 1.4551])\nkeys.shape: torch.Size([6, 2])\nvalues.shape: torch.Size([6, 2])\ntensor(1.8524)\ntensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\ntensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\ntensor([0.3061, 0.8210])\n```\n:::\n:::\n\n\n> Why do we need $d_{k}$ ^ 0.5 ? The reason for this normalization is to improve the training performance by avoiding small gradients. This does not matter a lot now, because the embedding size is fairly small but when dealing with large GPT-like LLMs, large dot produts can result in very large gradients during backpropogration due to the softmax function applied to them. As dot products increase, the softmax function behaves like a step function resulting in very very small gradients. These large/small gradients can cause gradients to drastically slow down learning or cause training to stagnate.. \n\n> The scaling by the square root of the embedding dimension is the reason why this self attetnion mechanism is called the scaled-dot product attention.\n\nThis looks fairly complex. This would be way simpler if we introduce some linear algebra. \nThe central idea is, we do two successive matrix multiplications within the three initialized weights that we have introduced. There is one normalization layer in between.The output dimension needs to be the same as in the input i.e token_count * embedding_vector_size. The below code simplifies the above code\n\n::: {#9c0472c6 .cell execution_count=7}\n``` {.python .cell-code}\nimport torch.nn as nn\nclass SelfAttention_v1(nn.Module):\n    def __init__(self, d_in, d_out):\n        super().__init__()\n        self.W_query = torch.nn.Parameter(torch.rand(d_in, d_out)) # Got rid of requires gradient = False part\n        self.W_key = torch.nn.Parameter(torch.rand(d_in, d_out)) \n        self.W_value = torch.nn.Parameter(torch.rand(d_in, d_out)) \n        \n\n    def forward(self, x):\n        query = x @ self.W_query \n        key = x @ self.W_key\n        value = x @ self.W_value\n\n        attn_scores = query @ key.T\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim = -1)\n\n        context_vec = attn_weights @ values\n        return context_vec\n\ntorch.manual_seed(123)\nsa_v1 = SelfAttention_v1(d_in, d_out)\nprint(sa_v1(inputs))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[0.2996, 0.8053],\n        [0.3061, 0.8210],\n        [0.3058, 0.8203],\n        [0.2948, 0.7939],\n        [0.2927, 0.7891],\n        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n```\n:::\n:::\n\n\nThere is this interesting quote from the book. \n\n> We can improve the SelfAttention_v1 implementation further by utilizing PyTorch’s nn.Linear layers, which effectively perform matrix multiplication when the bias units are  disabled. Additionally, a significant advantage of using nn.Linear instead of manually  implementing nn.Parameter(torch.rand(...)) is that nn.Linear has an optimized weight initialization scheme, contributing to more stable and effective model training.\n\nThis absolutely did not make sense to me in the first go. I have tried to simplify this below.\n\nThe first sentence is pretty straightforward and does not require much explaination. It means that if we turn bias = False for nn.Linear layer then what we are left with a matrix of the same size as the nn.Parameter. The second bit is the interesting one. \"Optimized weight implementation scheme\" here is pointing to the weight initialization by torch.rand.\nWhen torch.rand is used the sample is pulled from a normal distribution between [0,1]. The challenge with sampling from a normal distribution is two fold\n\n- All weights are only positive. This restricts the model's ability to learn complex patterns early on and can shift the output mean significantly.\n- Weights are relatively high (~0.5). This does not seem too high but is considered high for deep learning algorithms. \n\n\nWhen we use nn.Linear the advantage is a distribution from [-1,1] with mean 0. This allows weights to be negative or positive, keeping the activations centered around zero and preventing the \"exploding internal variance\".\n\n\n> Here is a deep dive that I did with Gemini. This answers two questions: Weight of 0.5 seemed too low to me. This explains that weights need to be low because we are dealing with a large number of inputs (512). If we do a scaled dot product which is sum(multiply(A*B)), if B has mean of 0.5, that means the result might be 256. This is one step. There would be multiple steps that is going to keep exploding this. After we are done with this, we also need to do a gradient calculation. That would also not make sense because the gradients are going to be volatile. To stop the sums from getting huge, the weights must get smaller as the number of inputs get larger.\n\n\n> a further deep dive on the algorithms available for weight initialization of nn.Linear and when to use them is present in the link\n[Weight Initialization](https://gemini.google.com/app/d91673816e6cae90)\n\n\nWe add the linear layer now to improve on the first interation.\n\n::: {#7b29d6a4 .cell execution_count=8}\n``` {.python .cell-code}\nclass SelfAttention_v2(nn.Module):\n    def __init__(self, d_in, d_out, qkv_bias=False):\n        super().__init__()\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n\n    def forward(self, x):\n        keys = self.W_key(x)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n        attn_scores = queries @ keys.T\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n        context_vec = attn_weights @ values\n        return context_vec\n\ntorch.manual_seed(789)\nsa_v2 = SelfAttention_v2(d_in, d_out)\nprint(sa_v2(inputs))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[-0.0739,  0.0713],\n        [-0.0748,  0.0703],\n        [-0.0749,  0.0702],\n        [-0.0760,  0.0685],\n        [-0.0763,  0.0679],\n        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n```\n:::\n:::\n\n\n## Translator's Learnings from this step\nNow, the translator has learned that not all relationships are equal. By introducing $W_q$, $W_k$, and $W_v$, they’ve developed three distinct mental frameworks: What am I looking for? (Query), What do I offer to others? (Key), and What is the actual meaning? (Value). They are no longer just looking at neighbors; they are actively learning which connections are the most important for understanding the \"soul\" of the sentence. There is one hack that the translator is using right now. It has a look up to future words.\n\n---\n\n# Causal Attention\nCausal Attention is a type of self-attention that only considers previous and current inputs in the sequence, ensuring the temporal order of text generation.\n\n\nCausal Attention builds on the idea of self-attention. It imprves self attention by only considering only the previous and current inputs in a sequence when processing any given token when computing the attention scores. It masks the future tokens. This is fairly simple add on. We can just zero out all the values of the upper triangles of the attention weight matrix. \n\n## The steps are the following:\n- Calculate Attention scores\n- Apply SoftMax and get attention weights\n- Mask with 0 above the diagonal\n- Normalize the rows of the masked attention scores\n\nImplementing this pretty much the same way as before. We will do small codes with one go, then we will formalize with a class\n\n::: {#d2c1e157 .cell execution_count=9}\n``` {.python .cell-code}\n# First two steps combined\nqueries = sa_v2.W_query(inputs)\n\nkeys = sa_v2.W_key(inputs)\n\nattnn_scores = queries @ keys.T\nattn_weights = torch.softmax(attn_scores/ keys.shape[-1]** 0.5, dim = -1)\n\nprint(attn_weights)\n\n# Third step - this looks a whole lot complex but is rather simple. If you only think about the values that tril is generating.\ncontext_length = attn_scores.shape[0]\nmasked_simple = torch.tril(torch.ones(context_length,context_length))\nprint(masked_simple)\n\n# Fourth step\nrow_sums = masked_simple.sum(dim=-1, keepdim=True)\nmasked_simple_norm = masked_simple/row_sums\nprint(masked_simple_norm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[0.1972, 0.1910, 0.1894, 0.1361, 0.1344, 0.1520],\n        [0.1476, 0.2164, 0.2134, 0.1365, 0.1240, 0.1621],\n        [0.1479, 0.2157, 0.2129, 0.1366, 0.1260, 0.1608],\n        [0.1505, 0.1952, 0.1933, 0.1525, 0.1375, 0.1711],\n        [0.1571, 0.1874, 0.1885, 0.1453, 0.1819, 0.1399],\n        [0.1473, 0.2033, 0.1996, 0.1500, 0.1160, 0.1839]])\ntensor([[1., 0., 0., 0., 0., 0.],\n        [1., 1., 0., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 1., 0., 0.],\n        [1., 1., 1., 1., 1., 0.],\n        [1., 1., 1., 1., 1., 1.]])\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000],\n        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000],\n        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]])\n```\n:::\n:::\n\n\nThis is a pretty neat implementation which hits our objective of causal attention model right on the head. However, there is a room for a few improvements primarily from a mathematical standpoint. \n1. Use negative infinity to get rid of that extra normalization at step 4. The whole idea of step 4 was normalize all the values left after zeroing upper traingle of the matrix.When we use negative infinity with softmax the output is 0 and \n2. Dropouts addition to avoid overfitting - This is a technique where randomly selected hidden layer inputs are ignored during **training**. This avoids overfitting by making sure that the model does not rely heavily on a specific set of hidden layer inputs. Dropouts is only used during training and not inference.\n\nWe can combine all of this in one go and create our final class. We will also make it work with batches.\n\n::: {#33e251ee .cell execution_count=10}\n``` {.python .cell-code}\nbatch = torch.stack((inputs,inputs), dim = 0)\nprint(batch.shape) ## The shape now must be 2 * 6 * 3 which is 2 items in the batch. 6 tokens in each batch. Each token with an embedding size of 3\n\nclass CausalAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n        super().__init__()\n        self.d_out = d_out\n        self.W_query = nn.Linear(d_in,d_out,bias = qkv_bias)\n        self.W_key = nn.Linear(d_in,d_out,bias = qkv_bias)\n        self.W_value = nn.Linear(d_in,d_out,bias = qkv_bias)\n        self.dropout = nn.Dropout(dropout) # The input here indicates the percentage of values in the matrix that must be zero\n\n        # This tells pytorch that these weights are non trainable so no gradient-descent for them. The mask is saved in state_dict and is loaded/saced with the model\n        self.register_buffer(\n            'mask',\n            torch.triu(torch.ones(context_length,context_length),diagonal=1)\n        ) # Will take this in a bit\n\n    def forward(self, x):\n        b, num_tokens, d_in = x.shape # This is the reason that we do not need to initialize the d_in. d_in is derived from an object of the class\n\n        # Same step as the previous loop\n        # Shape of x = 2,6,3\n        # Shape of key, query, value is 2,6,2\n        keys = self.W_key(x) \n        queries = self.W_query(x)\n        values = self.W_value(x)\n\n        # Pretty much the same as the previous step. This is just doing transformations so that keys merge properly.\n        # Shape of attention scores is 2,6,6\n        # Keys.transpose essentially means switching the 1st dimenion with the second. so the dimesion is now 2,2,6\n        # queries (2,6,2) @ keys.transpose(1,2) (2,2,6) ==> (2,6,6)\n        # this is because torch considers everythin after the last two dimesnions as batch. A better way to say this would be anything the matrix multipllication here is 6,2 @ 2,6 --> 6,6 and then there are two batches in each.\n\n        attn_scores = queries @ keys.transpose(1,2)\n        \n        # This is making sure that all the values that are masked are now \n        # Fill elemts of the tensor with value where mask is true. The shape of the mask must be broadcastable\n        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens],-torch.inf)\n\n        # This is the same step as before.\n        attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim = -1)\n\n        # Droput we had discussed\n        # Zeroes the values of the matrix at random. the shape of the matrix is the same\n\n        attn_weights = self.dropout(attn_weights)\n\n        # This the same step as before.\n        # attn_weights dim = (2,6,6). values dim (2,6,2) --> 2,6,2\n        context_vec = attn_weights @ values\n\n        return context_vec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch.Size([2, 6, 3])\n```\n:::\n:::\n\n\nThe wraps up Causal Attention. Causal attention is the base on which the multi- headed attention is based on. Causal Attention also known as masked attention is a specialized form of self-attention. It restricts a model to only review previous and current inputs in a sequence when processing any given token when computing attention scores. \n\n## Translator's Learnings from this step\nOur translator is now writing a book. They’ve learned a vital rule: no spoilers. By masking the future, the translator forces themselves to generate the story one word at a time, using only what has been \"said\" so far. This discipline ensures the translation flows logically from start to finish, preventing the model from 'cheating' by looking at the end of the sentence before it has even started the beginning.\n\n---\n\n# Multi-Headed Attention\nThe term multi-headed attention refers to dividing the attention mechanism into multiple heads each operating indepedent. Here a single causal attention module can be considered single-head attention, where these is only one set of attention weights processing the input sequentially.\n\nThe main idea is to run the attention mechanism multiple times with different, learned linear projects.\n\n::: {#a254e66f .cell execution_count=11}\n``` {.python .cell-code}\nclass MultiHeadAttentionWrapper(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        # Defines the number of Causal Attention Modules\n        self.heads = nn.ModuleList(\n            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n            for _ in range(num_heads)]\n            )\n    # Concatenates the declaration from before\n    def forward(self,x):\n        return torch.cat([head(x) for head in self.heads], dim = -1)\n```\n:::\n\n\nThis is precisely what a multi-headed attention is. \n\n::: {#d605e9e5 .cell execution_count=12}\n``` {.python .cell-code}\ntorch.manual_seed(123)\ncontext_length = batch.shape[1] #This is the number of tokens\n\nd_in, d_out = 3,2\n\nmha = MultiHeadAttentionWrapper(\n    d_in, d_out, context_length, 0.0, num_heads=2 \n)\n\ncontext_vecs = mha(batch)\n\nprint(context_vecs)\nprint(\"contex_vecs.shape:\", context_vecs.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n         [-0.5874,  0.0058,  0.5891,  0.3257],\n         [-0.6300, -0.0632,  0.6202,  0.3860],\n         [-0.5675, -0.0843,  0.5478,  0.3589],\n         [-0.5526, -0.0981,  0.5321,  0.3428],\n         [-0.5299, -0.1081,  0.5077,  0.3493]],\n\n        [[-0.4519,  0.2216,  0.4772,  0.1063],\n         [-0.5874,  0.0058,  0.5891,  0.3257],\n         [-0.6300, -0.0632,  0.6202,  0.3860],\n         [-0.5675, -0.0843,  0.5478,  0.3589],\n         [-0.5526, -0.0981,  0.5321,  0.3428],\n         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\ncontex_vecs.shape: torch.Size([2, 6, 4])\n```\n:::\n:::\n\n\nThe shape of the output vector is now controlled by the number of Attention heads.\n\nCurrently, the implementation is sequential. We need to parallelize this. We can do this with Linear Algebra. We need to concatenate these attention weights and then do matrix multiplication at once. \n\n::: {#723b39d4 .cell execution_count=13}\n``` {.python .cell-code}\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n\n        assert(d_out %num_heads == 0), \"d_out be divisible by the num_heads\" # Why do we need to do this?\n\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads # Reduces the projection dimension to match the desired output dim\n\n        # In mulit-headed attention, we don't actually create several seperate linear layers for each head. Instead we perform one large matrix multiplication and then slice the result into smaller for each head. If d_out is not divisible by number of heads, we would end up with fractional dimensions or uneven heads. To mitigate this,we could use padding which can be totally avoided\n\n        self.W_query = nn.Linear(d_in,d_out,bias=qkv_bias) \n        self.W_key = nn.Linear(d_in,d_out,bias=qkv_bias)\n        self.W_value = nn.Linear(d_in,d_out,bias=qkv_bias)\n\n        # Uses a linear layer to combine individual head outputs\n        self.out_proj = nn.Linear(d_out, d_out)\n        self.dropout = nn.Dropout(dropout)\n\n        self.register_buffer(\n            \"mask\",\n            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n        )\n\n    def forward(self,x):\n        # We assume number of heads(num_heads) to 4\n        # Assuming output dimensions(d_out) to 8\n        # Dimensions assuming: 5,6,3 <==> (batch, token length, embedding vector)\n        b, num_tokens, d_in = x.shape\n\n        # Dimension output (5,6,3) * (3,8) --> (5,6,8)\n        keys = self.W_key(x)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n\n        # We implicitly split the matrix by adding a num_heads dimension, then we unroll the last dimension\n        # (batch, num_tokens, d_out) --> (b, num_tokens, num_heads, head_dim)\n\n        # (5,6,8) --> (5,6,4,2)        \n        keys = keys.view(b, num_tokens,self.num_heads, self.head_dim)\n        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n\n        # Transposes from shape (b, num_tokens, num_heads, head_dim) --> (b, num_heads, num_tokens, head_dim)\n        # (5,6,4,2) --> (5,4,6,2)\n        keys = keys.transpose(1,2)\n        queries = queries.transpose(1,2)\n        values = values.transpose(1,2)\n\n        # Can we not combine view and step to one step, view should be able to do it?\n        #  View changes how we interpret the memory. In this case it asking the interpretation of 8 vector of dim 1 to 4 rows and 1 column.\n        # Transpose changes the physical order. When we move the num_heads dimesnio, we are effectively asking to group all tokens for a single head together\n\n        # Computes dot product for each head\n        # (5,4,6,2) @ (5,4,2, 6) --> 5,4,6,6\n        attn_scores = queries @ keys.transpose(2,3)\n\n        # Masks truncated to the number of tokens\n        # (6,6)\n        mask_bool = self.mask[:num_tokens,:num_tokens].bool()\n\n        # uses masks to fill the attention scores\n        # This will broadcast 6 ,6 across \n        attn_scores.masked_fill_(mask_bool, -torch.inf)\n\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim = -1)\n        \n        attn_weights = self.dropout(attn_weights)\n\n\n        # (5,4,6,6) @ (5,4,6,2) --> (5,4,6,2).T --> (5,6,4,2)\n        context_vec = (attn_weights @ values).transpose(1,2)\n        # Combines heads where self.d_out = self.num_heads * self.head_dim\n        # (5,6,4,2) --> (5,6,8) We are doing the unroll here\n        # here contiguous().view() is asking to do the same thing as before but instead of the previous 8 --> 4,2, here it is asking to interpret this as 4,2 to 8\n        # After the transpose the data in memory is not longer lined up in a single row ergo not contiguous\n        # View has the requirement for the data to be in a contiguous block to work\n        # Calling .contiguis shuffles the physical memory to match the transposed shape so that it can flatten out.\n        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n\n        # Adding an optional linear projection. This is not strictly necessary but is commonly used in many LLM Architectures\n        context_vec = self.out_proj(context_vec)\n\n        return context_vec\n```\n:::\n\n\nThis completes the whole chapter of Coding Attention Mechanism\n\n---\n\n# Summarizing thoughts\nMulti-headed attention is the engine which powers the Transformers.\nMulti-Headed Attention is like a committtee of experts reading the same sentence. Each expert looks for different structures within the sentence simultaneously.\nSingle headed focusses on synactic relationship. \nBy projecting the input into different subspaces the model can capture a more rich \"context vector\" than a single attention pass could.\n\nWe start by taking **linear Projections** with our input over *Q, K , V* matrices. In MHA we split these into N distinct heads. Each head performs Scaled Dot Product Attention independently. This is where each expert learns in different subspaces in parallel. Because the each head's calculation is independent, it is highly efficient to compute on GPUs.\nWe finally stitch the results of all heads back together and pass them through the final linear layer $W_{o}$. This is re-integration the findings of our committee of experts into a single representation. The final linear layer is important as it glues different experts perscpectives together. The model now learns and weighs these experts into the final output\n\n## Going back to translator\n\nFinally, our single translator has become a full editorial team. One expert focuses on the tense of the verbs, another tracks the gender of the pronouns, and a third looks for poetic metaphors. By working in parallel, they catch nuances that a single mind would miss. They then sit down together (the final linear projection) to merge their individual notes into one perfect, polished masterpiece.\n\n",
    "supporting": [
      "Attention_Mechanisms_files"
    ],
    "filters": [],
    "includes": {}
  }
}